{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7792ee2b",
      "metadata": {
        "id": "7792ee2b"
      },
      "source": [
        "# 3D ResNet\n",
        "\n",
        "*Author: FAIR PyTorchVideo*\n",
        "\n",
        "**Resnet Style Video classification networks pretrained on the Kinetics 400 dataset**\n",
        "\n",
        "\n",
        "### Example Usage\n",
        "\n",
        "#### Imports\n",
        "\n",
        "Load the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e3af95",
      "metadata": {
        "id": "e4e3af95"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Choose the `slow_r50` model\n",
        "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f712be8d",
      "metadata": {
        "id": "f712be8d"
      },
      "source": [
        "Import remaining functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2ce15d4",
      "metadata": {
        "id": "a2ce15d4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import urllib\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "604c14a1",
      "metadata": {
        "id": "604c14a1"
      },
      "source": [
        "#### Setup\n",
        "\n",
        "Set the model to eval mode and move to desired device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e2aefa2",
      "metadata": {
        "attributes": {
          "classes": [
            "python "
          ],
          "id": ""
        },
        "id": "7e2aefa2"
      },
      "outputs": [],
      "source": [
        "# Set to GPU or CPU\n",
        "device = \"cpu\"\n",
        "model = model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cc097fe7",
      "metadata": {
        "id": "cc097fe7"
      },
      "source": [
        "Download the id to label mapping for the Kinetics 400 dataset on which the torch hub models were trained. This will be used to get the category label names from the predicted class ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35010e58",
      "metadata": {
        "id": "35010e58"
      },
      "outputs": [],
      "source": [
        "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
        "json_filename = \"kinetics_classnames.json\"\n",
        "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
        "except: urllib.request.urlretrieve(json_url, json_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "731698c9",
      "metadata": {
        "id": "731698c9"
      },
      "outputs": [],
      "source": [
        "with open(json_filename, \"r\") as f:\n",
        "    kinetics_classnames = json.load(f)\n",
        "\n",
        "# Create an id to label name mapping\n",
        "kinetics_id_to_classname = {}\n",
        "for k, v in kinetics_classnames.items():\n",
        "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c6f2e631",
      "metadata": {
        "id": "c6f2e631"
      },
      "source": [
        "#### Define input transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2008d5f",
      "metadata": {
        "id": "f2008d5f"
      },
      "outputs": [],
      "source": [
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 8 # unit [frames]\n",
        "segments = 1\n",
        "total_frames = segments * num_frames\n",
        "sampling_rate = 8 # unit [frames]\n",
        "frames_per_second = 30 # unit [fps]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "148f5022",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from typing import List, Union, Tuple, Any\n",
        "from striprtf.striprtf import rtf_to_text\n",
        "import albumentations\n",
        "# NOTA: nei video il fuoco, se c'è, permane fino alla fine\n",
        "\n",
        "class VideoRecord(object):\n",
        " # Sostanzilamente mantiene in memoria tutte le informazioni di annotazione di \n",
        " # un particolare video \n",
        "    \"\"\"\n",
        "    Helper class for class VideoFrameDataset. This class\n",
        "    represents a video sample's metadata.\n",
        "\n",
        "    Args:\n",
        "        root_datapath: the system path to the root folder of the videos.\n",
        "        row: A list with four or more elements where\n",
        "             1) The first element is the path to the video sample's frames excluding\n",
        "             the root_datapath prefix\n",
        "             2) The  second element is the starting frame id of the video\n",
        "             3) The third element is the inclusive ending frame id of the video\n",
        "             4) The fourth element is the label index.\n",
        "             5) any following elements are labels in the case of multi-label classification\n",
        "    \"\"\"\n",
        "    def __init__(self, row, root_datapath):\n",
        "        # row = lista di interi che contiene:\n",
        "        # 0: path del video\n",
        "        # 1: primo frame del video (e qui dobbiamo decidere da dove partire)\n",
        "        # 2: ultimo frame\n",
        "        # 3: label della classe\n",
        "        # 4: possono esserci altre annotazioni (es. fumo, fuoco)\n",
        "        self._data = row\n",
        "        self._path = os.path.join(root_datapath, row[0])\n",
        "\n",
        "    @property\n",
        "    def path(self) -> str:\n",
        "        return self._path\n",
        "\n",
        "    @property\n",
        "    def num_frames(self) -> int:\n",
        "        return self.end_frame - self.start_frame + 1  # +1 because end frame is inclusive\n",
        "\n",
        "    @property\n",
        "    def start_frame(self) -> int:\n",
        "        return int(self._data[1])\n",
        "\n",
        "    @property\n",
        "    def end_frame(self) -> int:\n",
        "        return int(self._data[2])\n",
        "\n",
        "    @property\n",
        "    def label(self) -> Union[int, List[int]]:\n",
        "        # just one label_id\n",
        "        if len(self._data) == 4:\n",
        "            return int(self._data[3])\n",
        "        # sample associated with multiple labels\n",
        "        else:\n",
        "            return [int(label_id) for label_id in self._data[3:]]\n",
        "\n",
        "\n",
        "class VideoFrameDataset(torch.utils.data.Dataset):\n",
        "    r\"\"\"\n",
        "    A highly efficient and adaptable dataset class for videos.\n",
        "    Instead of loading every frame of a video,\n",
        "    loads x RGB frames of a video (sparse temporal sampling) and evenly\n",
        "    chooses those frames from start to end of the video, returning\n",
        "    a list of x PIL images or ``FRAMES x CHANNELS x HEIGHT x WIDTH``\n",
        "    tensors.\n",
        "\n",
        "    More specifically, the frame range [START_FRAME, END_FRAME] is divided into NUM_SEGMENTS\n",
        "    segments and FRAMES_PER_SEGMENT consecutive frames are taken from each segment.\n",
        "\n",
        "    Note:\n",
        "        A demonstration of using this class can be seen\n",
        "        in ``demo.py``\n",
        "        https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch\n",
        "\n",
        "    Note:\n",
        "        This dataset broadly corresponds to the frame sampling technique\n",
        "        introduced in ``Temporal Segment Networks`` at ECCV2016\n",
        "        https://arxiv.org/abs/1608.00859.\n",
        "\n",
        "    Args:\n",
        "        root_path: The root path in which video folders lie.\n",
        "                   this is ROOT_DATA from the description above.\n",
        "        num_segments: The number of segments the video should\n",
        "                      be divided into to sample frames from.\n",
        "        frames_per_segment: The number of frames that should\n",
        "                            be loaded per segment. For each segment's\n",
        "                            frame-range, a random start index or the\n",
        "                            center is chosen, from which frames_per_segment\n",
        "                            consecutive frames are loaded.\n",
        "        imagefile_template: The image filename template that video frame files\n",
        "                            have inside of their video folders as described above.\n",
        "        transform: Transform pipeline that receives a list of numpy images/frames.\n",
        "        test_mode: If True, frames are taken from the center of each\n",
        "                   segment, instead of a random location in each segment.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 root_path: str, # dove sono contenuti i frame estratti         #? Questo non introdurrebbe dipendenza tra i dati???\n",
        "                 num_segments: int = 1, # divide il video da cui caricare i\n",
        "                 # frame in un certo numero di sezioni della stessa durata.\n",
        "                 frames_per_segment: int = 3, # frame estratti dal segmento in maniera (di default) a caso\n",
        "                 imagefile_template: str='{:05d}.jpg', # pattern nome dei frame\n",
        "                 transform=None, # pipeline di augmentation preprocessing\n",
        "                 totensor=True, # lasciarlo sempre a True\n",
        "                 test_mode: bool = False # quanto True, i frame vengono presi \n",
        "                 # sempre nelle stesse posizioni. È quello che vogliamo fare \n",
        "                 # quando costruiamo dataset per test o validation\n",
        "                 ):\n",
        "        super(VideoFrameDataset, self).__init__()\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.num_segments = num_segments\n",
        "        self.frames_per_segment = frames_per_segment\n",
        "        self.imagefile_template = imagefile_template\n",
        "        self.test_mode = test_mode\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = None\n",
        "        else:\n",
        "            additional_targets = {}\n",
        "            for i in range(self.num_segments * self.frames_per_segment - 1):\n",
        "                additional_targets[\"image%d\" % i] = \"image\"\n",
        "            self.transform = albumentations.Compose([transform],\n",
        "                                                    additional_targets=additional_targets,\n",
        "                                                    p=1)\n",
        "        self.totensor = totensor\n",
        "        self.totensor_transform = ImglistOrdictToTensor()\n",
        "\n",
        "        self._parse_annotationfile()\n",
        "        self._sanity_check_samples()\n",
        "\n",
        "    def _load_image(self, directory: str, idx: int) -> Image.Image:\n",
        "        return np.asarray(Image.open(os.path.join(directory, self.imagefile_template.format(idx))).convert('RGB'))\n",
        "\n",
        "    def _parse_annotationfile(self):\n",
        "      # Usando l'organizzazione delle cartelle suggerita l'annotazione viene \n",
        "      # gestita in automatico. Per ogni file video il codice ricava la posizione\n",
        "      # del rispettivo file rtf e fa il parsing del file per ricavare la GT\n",
        "      # In particolare quando vede un video di classe 1, segna il primo frame di\n",
        "      # avvistamento del fuoco e da lì in poi selezionarà i frame randomici, \n",
        "      # andando ad ignorare un'eventuale fase del video iniziale in cui il fuoco\n",
        "      # non c'è\n",
        "      # Fatto questo parse abbiamo costruito la label del video\n",
        "        self.video_list = []\n",
        "        for class_name in os.listdir(self.root_path):\n",
        "            for video_name in os.listdir(os.path.join(self.root_path, class_name)):\n",
        "                frames_dir = os.path.join(self.root_path, class_name, video_name)\n",
        "                if os.path.isdir(frames_dir):\n",
        "                    frame_path = os.path.join(class_name, video_name)\n",
        "                    end_frame = len(os.listdir(frames_dir))\n",
        "\n",
        "                    annotation_path = frames_dir\\\n",
        "                        .replace(\"\\\\\", \"/\") \\\n",
        "                        .replace(\"FRAMES/\", \"GT/\") \\\n",
        "                        .replace(\".mp4\", \".rtf\")\n",
        "\n",
        "                    with open(annotation_path, 'r') as file:\n",
        "                        text = rtf_to_text(file.read())\n",
        "                    if len(text):\n",
        "                        label = 1\n",
        "                        start_frame = int(text.split(\",\")[0])\n",
        "                        if start_frame == 0:\n",
        "                          start_frame = 1\n",
        "                    else:\n",
        "                        label = 0\n",
        "                        start_frame = 1\n",
        "\n",
        "                    self.video_list.append(VideoRecord(\n",
        "                        [frame_path, start_frame, end_frame, label],\n",
        "                        self.root_path))\n",
        "\n",
        "    def _sanity_check_samples(self):\n",
        "      # Controllo delle annotazioni ricavate per ogni video\n",
        "        for record in self.video_list:\n",
        "            if record.num_frames <= 0 or record.start_frame == record.end_frame:\n",
        "                print(f\"\\nDataset Warning: video {record.path} seems to have zero RGB frames on disk!\\n\")\n",
        "\n",
        "            elif record.num_frames < (self.num_segments * self.frames_per_segment):\n",
        "                print(f\"\\nDataset Warning: video {record.path} has {record.num_frames} frames \"\n",
        "                      f\"but the dataloader is set up to load \"\n",
        "                      f\"(num_segments={self.num_segments})*(frames_per_segment={self.frames_per_segment})\"\n",
        "                      f\"={self.num_segments * self.frames_per_segment} frames. Dataloader will throw an \"\n",
        "                      f\"error when trying to load this video.\\n\")\n",
        "\n",
        "    def _get_start_indices(self, record: VideoRecord) -> 'np.ndarray[int]':\n",
        "        \"\"\"\n",
        "        For each segment, choose a start index from where frames\n",
        "        are to be loaded from.\n",
        "\n",
        "        Args:\n",
        "            record: VideoRecord denoting a video sample.\n",
        "        Returns:\n",
        "            List of indices of where the frames of each\n",
        "            segment are to be loaded from.\n",
        "        \"\"\"\n",
        "        # choose start indices that are perfectly evenly spread across the video frames.\n",
        "        if self.test_mode:\n",
        "            distance_between_indices = (record.num_frames - self.frames_per_segment + 1) / float(self.num_segments)\n",
        "\n",
        "            start_indices = np.array([int(distance_between_indices / 2.0 + distance_between_indices * x)\n",
        "                                      for x in range(self.num_segments)])\n",
        "        # randomly sample start indices that are approximately evenly spread across the video frames.\n",
        "        else:\n",
        "            max_valid_start_index = (record.num_frames - self.frames_per_segment + 1) // self.num_segments\n",
        "\n",
        "            start_indices = np.multiply(list(range(self.num_segments)), max_valid_start_index) + \\\n",
        "                      np.random.randint(max_valid_start_index, size=self.num_segments)\n",
        "\n",
        "        return start_indices\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Union[\n",
        "        Tuple[List[Image.Image], Union[int, List[int]]],\n",
        "        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n",
        "        Tuple[Any, Union[int, List[int]]],\n",
        "        ]:\n",
        "        \"\"\"\n",
        "        For video with id idx, loads self.NUM_SEGMENTS * self.FRAMES_PER_SEGMENT\n",
        "        frames from evenly chosen locations across the video.\n",
        "\n",
        "        Args:\n",
        "            idx: Video sample index.\n",
        "        Returns:\n",
        "            A tuple of (video, label). Label is either a single\n",
        "            integer or a list of integers in the case of multiple labels.\n",
        "            Video is either 1) a list of PIL images if no transform is used\n",
        "            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n",
        "            if the transform \"ImglistToTensor\" is used\n",
        "            3) or anything else if a custom transform is used.\n",
        "        \"\"\"\n",
        "        record: VideoRecord = self.video_list[idx]\n",
        "\n",
        "        frame_start_indices: 'np.ndarray[int]' = self._get_start_indices(record)\n",
        "\n",
        "        return self._get(record, frame_start_indices)\n",
        "\n",
        "    def _get(self, record: VideoRecord, frame_start_indices: 'np.ndarray[int]') -> Union[\n",
        "        Tuple[List[Image.Image], Union[int, List[int]]],\n",
        "        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n",
        "        Tuple[Any, Union[int, List[int]]],\n",
        "        ]:\n",
        "        \"\"\"\n",
        "        Loads the frames of a video at the corresponding\n",
        "        indices.\n",
        "\n",
        "        Args:\n",
        "            record: VideoRecord denoting a video sample.\n",
        "            frame_start_indices: Indices from which to load consecutive frames from.\n",
        "        Returns:\n",
        "            A tuple of (video, label). Label is either a single\n",
        "            integer or a list of integers in the case of multiple labels.\n",
        "            Video is either 1) a list of PIL images if no transform is used\n",
        "            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n",
        "            if the transform \"ImglistToTensor\" is used\n",
        "            3) or anything else if a custom transform is used.\n",
        "        \"\"\"\n",
        "\n",
        "        frame_start_indices = frame_start_indices + record.start_frame\n",
        "        images = list()\n",
        "\n",
        "        # from each start_index, load self.frames_per_segment\n",
        "        # consecutive frames\n",
        "        for start_index in frame_start_indices:\n",
        "            frame_index = int(start_index)\n",
        "\n",
        "            # load self.frames_per_segment consecutive frames\n",
        "            for _ in range(self.frames_per_segment):\n",
        "                image = self._load_image(record.path, frame_index)\n",
        "                images.append(image)\n",
        "\n",
        "                if frame_index < record.end_frame:\n",
        "                    frame_index += 1\n",
        "\n",
        "        if self.transform is not None:\n",
        "            transform_input = {\"image\": images[0]}\n",
        "            for i, image in enumerate(images[1:]):\n",
        "                transform_input[\"image%d\" % i] = image\n",
        "            images = self.transform(**transform_input)\n",
        "\n",
        "        if self.totensor:\n",
        "            images = self.totensor_transform(images)\n",
        "        \n",
        "        return images, record.label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_list)\n",
        "\n",
        "\n",
        "class ImglistOrdictToTensor(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Converts a list or a dict of numpy images to a torch.FloatTensor\n",
        "    of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH).\n",
        "    Can be used as first transform for ``VideoFrameDataset``.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(img_list_or_dict):\n",
        "        \"\"\"\n",
        "        Converts each numpy image in a list or a dict to\n",
        "        a torch Tensor and stacks them into a single tensor.\n",
        "\n",
        "        Args:\n",
        "            img_list_or_dict: list or dict of numpy images.\n",
        "        Returns:\n",
        "            tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\n",
        "        \"\"\"\n",
        "        if isinstance(img_list_or_dict, list):\n",
        "            return torch.stack([transforms.functional.to_tensor(img)\n",
        "                                for img in img_list_or_dict])\n",
        "        else:\n",
        "            return torch.stack([transforms.functional.to_tensor(img_list_or_dict[k])\n",
        "                                for k in img_list_or_dict.keys()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837da2a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "augmentation = albumentations.OneOf([\n",
        "    albumentations.HorizontalFlip(p=1.),\n",
        "    ], p=.5)\n",
        "\n",
        "# Dataset for training\n",
        "dataset_train = VideoFrameDataset(root_path=\"FRAMES/TRAINING_SET/\",\n",
        "                            num_segments=segments,\n",
        "                            frames_per_segment=num_frames,\n",
        "                            transform=albumentations.Compose([\n",
        "                                augmentation],\n",
        "                                p=1.,\n",
        "                            )\n",
        "                            )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "752b1c5f",
      "metadata": {
        "id": "752b1c5f"
      },
      "source": [
        "#### Run Inference\n",
        "\n",
        "Download an example video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a586c88",
      "metadata": {
        "id": "6a586c88"
      },
      "outputs": [],
      "source": [
        "url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
        "video_path = 'archery.mp4'\n",
        "try: urllib.URLopener().retrieve(url_link, video_path)\n",
        "except: urllib.request.urlretrieve(url_link, video_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8bccde43",
      "metadata": {
        "id": "8bccde43"
      },
      "source": [
        "Load the video and transform it to the input format required by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402b45a9",
      "metadata": {
        "id": "402b45a9"
      },
      "outputs": [],
      "source": [
        "transform=Compose(\n",
        "        [\n",
        "            #UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
        "        ]\n",
        "    )\n",
        "\n",
        "inputs = transform(dataset_train[0][0].permute(1, 0, 2, 3).to(device)) # Permutazione necessaria eprchè le trasformazioni sono state fatte per C * T * H * W"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ca96dd24",
      "metadata": {
        "id": "ca96dd24"
      },
      "source": [
        "#### Get Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d796f971",
      "metadata": {
        "id": "d796f971"
      },
      "outputs": [],
      "source": [
        "# preds = model(inputs[None, ...])\n",
        "preds = model(torch.stack([inputs]*2, dim=0))\n",
        "print(preds.shape)\n",
        "# Get the predicted classes\n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "preds = post_act(preds)\n",
        "pred_classes = preds.topk(k=5).indices[0]\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2072e410",
      "metadata": {},
      "source": [
        "# Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38697a04",
      "metadata": {},
      "outputs": [],
      "source": [
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 3 # unit [frames]\n",
        "segments = 1\n",
        "total_frames = segments * num_frames\n",
        "sampling_rate = 8 # unit [frames]\n",
        "frames_per_second = 30 # unit [fps]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\n",
        "# model.blocks[5] = nn.Sequential(\n",
        "#     nn.AvgPool3d(kernel_size=(1, 7, 7), stride=(1, 1, 1), padding=0),\n",
        "#     nn.Flatten(),\n",
        "#     nn.Dropout(p=0.5),\n",
        "#     nn.Linear(in_features=total_frames*8192, out_features=1, bias=True),\n",
        "#     nn.AdaptiveAvgPool2d(output_size=1)\n",
        "#     # nn.Sigmoid() # Applied in training code\n",
        "# )\n",
        "\n",
        "model.blocks[5].pool = nn.AvgPool3d(kernel_size=(1, 7, 7), stride=(1, 1, 1), padding=0)\n",
        "model.blocks[5].dropout = nn.Dropout(p=0.5)\n",
        "model.blocks[5].proj = nn.Linear(in_features= 2048, out_features=1, bias=True)\n",
        "model.blocks[5].output_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "model = model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9413ffed",
      "metadata": {},
      "outputs": [],
      "source": [
        "import albumentations\n",
        "\n",
        "augmentation = albumentations.OneOf([\n",
        "    albumentations.HorizontalFlip(p=1.),\n",
        "    ], p=.5)\n",
        "\n",
        "transform=Compose(\n",
        "        [\n",
        "            #UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
        "        ]\n",
        "    )\n",
        "\n",
        "preprocessing = albumentations.Sequential([\n",
        "    albumentations.SmallestMaxSize(side_size, always_apply=True),\n",
        "    albumentations.CenterCrop(crop_size, crop_size, always_apply=True),\n",
        "    albumentations.Normalize(mean=mean,\n",
        "                                std=std,\n",
        "                                max_pixel_value=255.,\n",
        "                                always_apply=True), \n",
        "])\n",
        "\n",
        "# inputs = transform(dataset_train[0][0].permute(1, 0, 2, 3).to(device))\n",
        "\n",
        "# model(inputs[None, ...]).squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ef26a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Dataset for training\n",
        "train_dataset = VideoFrameDataset(root_path=\"FRAMES/TRAINING_SET/\",\n",
        "                            num_segments=segments,\n",
        "                            frames_per_segment=num_frames,\n",
        "                            transform=albumentations.Compose([\n",
        "                                preprocessing,\n",
        "                                augmentation],\n",
        "                                p=1.,\n",
        "                            )\n",
        "                            )\n",
        "\n",
        "\n",
        "# Dataset for training\n",
        "val_dataset = VideoFrameDataset(root_path=\"FRAMES/VALIDATION_SET/\",\n",
        "                            num_segments=segments,\n",
        "                            frames_per_segment=num_frames,\n",
        "                            transform=albumentations.Compose([\n",
        "                                preprocessing,\n",
        "                                augmentation],\n",
        "                                p=1.,\n",
        "                            ),\n",
        "                            test_mode=True\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6bd1395",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pred = model(torch.stack([train_dataset[0][0]]*1, dim=0))\n",
        "# pred = model(train_dataset[0][0][None, ...])\n",
        "# pred.shape\n",
        "# nn.Sigmoid()(pred).shape\n",
        "\n",
        "res = model.blocks[0](train_dataset[0][0][None, ...].permute(0, 2, 1, 3, 4))\n",
        "res = model.blocks[1](res)\n",
        "res = model.blocks[2](res)\n",
        "res = model.blocks[3](res)\n",
        "res = model.blocks[4](res)\n",
        "print(res.shape)\n",
        "res = model.blocks[5](res)\n",
        "print(res.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd6bc066",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tensorboard import notebook\n",
        "\n",
        "def start_tensorboard(log_dir):\n",
        "  writer = SummaryWriter(os.path.join(\"runs\", log_dir))\n",
        "\n",
        "  # run tensorboard in background\n",
        "  ! killall tensorboard\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir ./runs\n",
        "\n",
        "  notebook.list() # View open TensorBoard instances\n",
        "\n",
        "  return writer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fb5eefa",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm\n",
        "\n",
        "def one_epoch(model, lossFunction, output_activation, optimizer, train_loader, val_loader, writer, epoch_num, transform):\n",
        "  model = model.to(device)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  i_start = epoch_num * len(train_loader)\n",
        "  for i, (X, y) in tqdm(enumerate(train_loader), desc=\"epoch {} - train\".format(epoch_num)):\n",
        "\n",
        "    # Merge the first two dimensions of the input (spatial and temporal) to perform the transformation\n",
        "    X = X.to(device).permute(0, 2, 1, 3, 4)\n",
        "    y = y.to(device).float()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    o = model(X)\n",
        "    o = output_activation(o).squeeze()\n",
        "\n",
        "    print(\"Output\", o)\n",
        "    print(\"Label\", y.shape)\n",
        "\n",
        "    # make shape of o and y the same\n",
        "    # if o.shape != y.shape:\n",
        "    #   o = o.reshape(y.shape)\n",
        "    \n",
        "\n",
        "    l = lossFunction(o, y)\n",
        "\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    acc = ((o.detach() > .5) == y.detach()).float().mean()\n",
        "    \n",
        "    # print(\"- batch loss and accuracy : {:.7f}\\t{:.4f}\".format(l.detach().item(), acc))\n",
        "    writer.add_scalar('train/loss', l.detach().item(), i_start+i)\n",
        "    writer.add_scalar('train/acc', acc, i_start+i)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    val_loss = []\n",
        "    val_corr_pred = []\n",
        "    for X, y in tqdm(val_loader, desc=\"epoch {} - validation\".format(epoch_num)):\n",
        "      \n",
        "      X.to(device).permute(0, 2, 1, 3, 4)\n",
        "      y = y.to(device).float()\n",
        "\n",
        "      o = model(X)\n",
        "      o = output_activation(o).squeeze()\n",
        "      val_loss.append(lossFunction(o, y))\n",
        "      val_corr_pred.append((o > .5) == y)\n",
        "\n",
        "    val_loss = torch.stack(val_loss).mean().item()\n",
        "    val_accuracy = torch.concatenate(val_corr_pred).float().mean().item()\n",
        "\n",
        "    # print(\"Validation loss and accuracy : {:.7f}\\t{:.4f}\".format(val_loss, val_accuracy))\n",
        "    writer.add_scalar('val/loss', val_loss, i_start+i)\n",
        "    writer.add_scalar('val/acc', val_accuracy, i_start+i)\n",
        "  return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa27fc40",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn import BCELoss, CrossEntropyLoss, Sigmoid, Softmax\n",
        "\n",
        "# learning hyperparameters\n",
        "lossFunction, output_activation = BCELoss(), Sigmoid()\n",
        "batch_size = 10\n",
        "lr = .01\n",
        "momentum = .7\n",
        "lambda_reg = 0\n",
        "\n",
        "epochs = 600\n",
        "early_stopping_patience = 15\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "trainable = list(model.blocks[4:].parameters())\n",
        "for param in trainable:\n",
        "  param.requires_grad = True\n",
        "\n",
        "# create optimizer\n",
        "optimizer = torch.optim.SGD(trainable,\n",
        "                          lr=lr,\n",
        "                          weight_decay=lambda_reg,\n",
        "                          momentum=momentum)\n",
        "\n",
        "# create output directory and logger\n",
        "experiment_name = \"3DResNet_finetuning\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "394ae355",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(experiment_name)\n",
        "writer = start_tensorboard(experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa094152",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b1b0df5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dataloader_params = {\"batch_size\": batch_size, \"num_workers\": 0, \"pin_memory\": True}\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, **dataloader_params)\n",
        "val_loader = DataLoader(val_dataset, shuffle=False, **dataloader_params)\n",
        "\n",
        "# early stopping and best model saving\n",
        "early_stopping_counter = early_stopping_patience\n",
        "min_val_loss = 1e10\n",
        "\n",
        "# training and validation\n",
        "val_losses = torch.zeros(epochs)\n",
        "val_accuracies = torch.zeros(epochs)\n",
        "for e in range(epochs):\n",
        "  print(\"EPOCH {}\".format(e))\n",
        "  val_loss, val_accuracy = one_epoch(model, lossFunction, output_activation, optimizer, train_loader, val_loader, writer, e, transform)\n",
        "\n",
        "  # store the validation metrics\n",
        "  val_losses[e] = val_loss\n",
        "  val_accuracies[e] = val_accuracy\n",
        "  torch.save(val_losses, os.path.join(experiment_name,'val_losses.pth'))\n",
        "  torch.save(val_accuracies, os.path.join(experiment_name,'val_accuracies.pth'))\n",
        "\n",
        "  # save the best model and check the early stopping criteria\n",
        "  if val_loss < min_val_loss: # save the best model\n",
        "    min_val_loss = val_loss\n",
        "    early_stopping_counter = early_stopping_patience # reset early stopping counter\n",
        "    torch.save(model.state_dict(), os.path.join(experiment_name,'best_model.pth'))\n",
        "    print(\"- saved best model: val_loss =\", val_loss, \"val_accuracy =\", val_accuracy)\n",
        "\n",
        "  if e>0: # early stopping counter update\n",
        "    if val_losses[e] > val_losses[e-1]:\n",
        "        early_stopping_counter -= 1 # update early stopping counter\n",
        "    else:\n",
        "        early_stopping_counter = early_stopping_patience # reset early stopping counter\n",
        "  if early_stopping_counter == 0: # early stopping\n",
        "      break\n",
        "  \n",
        "  ### PERCHE NON FUNZIONA????\n",
        "  # AGGIUNGERE [None, ...]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "43973268aa23c7cf4b4cafe0e819fac9b2412fcc991a21988492c3271a4e3f9b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
