{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform  \n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Function to determine the device type based on the node name.\n",
    "    It uses a dictionary to map node names to device types.\n",
    "\n",
    "    :return: device type as a string\n",
    "    :raises Exception: if node name is not found in the device_map dictionary\n",
    "    \"\"\"\n",
    "    device_map = {\n",
    "        \"PC-Cristian\": \"cuda\",\n",
    "        \"Dell-G5-15-Alexios\": \"cuda\",\n",
    "        \"MacBook-Pro-di-Cristian.local\": \"mps\",\n",
    "        \"MacBookProDiGrazia\": \"cpu\",\n",
    "        \"DESKTOP-RQVK8SI\":\"cuda\",\n",
    "        \"MacBook-Pro.station\":\"mps\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        return device_map[platform.uname().node]\n",
    "    except KeyError:\n",
    "        raise Exception(\"Node name not found. Please add your node name and its corresponding device to the dictionary.\")\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from project_paths import *\n",
    "\n",
    "def delete_ds_store_files(folder):\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file == \".DS_Store\":\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted {file_path}\")\n",
    "\n",
    "# Specifica la cartella principale in cui cercare i file .DS_Store\n",
    "folder_path = data_folder_path\n",
    "\n",
    "delete_ds_store_files(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create YOLO dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, random\n",
    "\n",
    "def create_yolo_datset(splitted_frames_path, splitted_gt_path, yolo_dataset_path, num_frames_per_video=10):\n",
    "\n",
    "    # folders\n",
    "    # if yolo dataset folder, doesn't exist, create it\n",
    "    if not os.path.exists(yolo_dataset_path):\n",
    "        os.makedirs(yolo_dataset_path)  \n",
    "\n",
    "    # create the yolo dataset structure, we create a folder for train and a folder for val\n",
    "    if not os.path.exists(os.path.join(yolo_dataset_path, 'train')):\n",
    "        os.makedirs(os.path.join(yolo_dataset_path, 'train'))\n",
    "    if not os.path.exists(os.path.join(yolo_dataset_path, 'val')):\n",
    "        os.makedirs(os.path.join(yolo_dataset_path, 'val'))\n",
    "\n",
    "    # create the two inner folders: one for fire and one for no fire\n",
    "    if not os.path.exists(os.path.join(yolo_dataset_path, 'train', 'fire')):\n",
    "        os.makedirs(os.path.join(yolo_dataset_path, 'train', 'fire'))\n",
    "    if not os.path.exists(os.path.join(yolo_dataset_path, 'train', 'no_fire')):\n",
    "        os.makedirs(os.path.join(yolo_dataset_path, 'train', 'no_fire'))\n",
    "    if not os.path.exists(os.path.join(yolo_dataset_path, 'val', 'fire')):\n",
    "        os.makedirs(os.path.join(yolo_dataset_path, 'val', 'fire'))\n",
    "    if not os.path.exists(os.path.join(yolo_dataset_path, 'val', 'no_fire')):\n",
    "        os.makedirs(os.path.join(yolo_dataset_path, 'val', 'no_fire'))\n",
    "\n",
    "    # now we copy the images in the right folder\n",
    "    # we have to copy the images in the train folder and in the val folder\n",
    "    # for each folder in SPLITTED_FRAMES_DATASET/TRAINING_SET/0 we randomly sample num_frames_per_video frames and put them in dataset/train/no_fire\n",
    "    # for each folder in SPLITTED_FRAMES_DATASET/VALIDATION_SET/0 we randomly sample num_frames_per_video frames and put them in dataset/val/no_fire\n",
    "\n",
    "    count = 0\n",
    "    for folder in os.listdir(os.path.join(splitted_frames_path, 'TRAINING_SET','0')):\n",
    "        tot_frames = len(os.listdir(os.path.join(splitted_frames_path, 'TRAINING_SET','0')))\n",
    "        if tot_frames < num_frames_per_video:\n",
    "            num_frames_per_video = tot_frames\n",
    "        chosen_frames = []\n",
    "        for _ in range (num_frames_per_video):\n",
    "            # randomly sample a frame\n",
    "            frame = random.choice(os.listdir(os.path.join(splitted_frames_path, 'TRAINING_SET','0', folder)))\n",
    "            while frame in chosen_frames:\n",
    "                frame = random.choice(os.listdir(os.path.join(splitted_frames_path, 'TRAINING_SET','0', folder)))\n",
    "            chosen_frames.append(frame)\n",
    "            # copy the frame in the right folder\n",
    "            shutil.copy(os.path.join(splitted_frames_path, 'TRAINING_SET','0', folder, frame), os.path.join(yolo_dataset_path, 'train', 'no_fire', frame))\n",
    "            # rename the frame\n",
    "            os.rename(os.path.join(yolo_dataset_path, 'train', 'no_fire', frame), os.path.join(yolo_dataset_path, 'train', 'no_fire', str(count)+'.jpg'))\n",
    "            count += 1\n",
    "        \n",
    "    count = 0\n",
    "    for folder in os.listdir(os.path.join(splitted_frames_path, 'VALIDATION_SET','0')):\n",
    "        tot_frames = len(os.listdir(os.path.join(splitted_frames_path, 'VALIDATION_SET','0')))\n",
    "        if tot_frames < num_frames_per_video:\n",
    "            num_frames_per_video = tot_frames\n",
    "        chosen_frames = []\n",
    "        for _ in range (num_frames_per_video):\n",
    "            # randomly sample a frame\n",
    "            frame = random.choice(os.listdir(os.path.join(splitted_frames_path, 'VALIDATION_SET','0', folder)))\n",
    "            while frame in chosen_frames:\n",
    "                frame = random.choice(os.listdir(os.path.join(splitted_frames_path, 'VALIDATION_SET','0', folder)))\n",
    "            chosen_frames.append(frame)\n",
    "            # copy the frame in the right folder\n",
    "            shutil.copy(os.path.join(splitted_frames_path, 'VALIDATION_SET','0', folder, frame), os.path.join(yolo_dataset_path, 'val', 'no_fire', frame))\n",
    "            # rename the frame\n",
    "            os.rename(os.path.join(yolo_dataset_path, 'val', 'no_fire', frame), os.path.join(yolo_dataset_path, 'val', 'no_fire', str(count)+'.jpg'))\n",
    "            count += 1\n",
    "\n",
    "    # # for each folder in SPLITTED_FRAMES_DATASET/TRAINING_SET/1 we randomly sample num_frames_per_video frames starting from the frame indicated in the annotation file\n",
    "    # # and put them in dataset/train/fire\n",
    "    # # for each folder in SPLITTED_FRAMES_DATASET/VALIDATION_SET/1 we randomly sample num_frames_per_video frames starting from the frame indicated in the annotation file\n",
    "    # # and put them in dataset/val/fire\n",
    "\n",
    "    count = 0    \n",
    "    num_frames = 0\n",
    "    num_folders = 0\n",
    "    for folder in os.listdir(os.path.join(splitted_frames_path, 'TRAINING_SET','1')):\n",
    "        num_folders += 1\n",
    "        tot_frames = len(os.listdir(os.path.join(splitted_frames_path, 'TRAINING_SET','1')))\n",
    "        if tot_frames < num_frames_per_video:\n",
    "            num_frames_per_video = tot_frames\n",
    "        chosen_frames = []\n",
    "        # get the annotation file corresponding to the folder\n",
    "        annotation_file = folder.replace(\"mp4\", \"rtf\")\n",
    "        # open the annotation file\n",
    "        with open(os.path.join(splitted_gt_path, 'TRAINING_SET', '1', annotation_file), 'r') as f:\n",
    "            # get the frame in which the fire starts\n",
    "            start_frame = int(f.readline().split(',')[0])\n",
    "            # randomly sample a frame whose number starts from start_frame\n",
    "            available_frames = []\n",
    "            for _ in range (num_frames_per_video):\n",
    "                for frame in os.listdir(os.path.join(splitted_frames_path, 'TRAINING_SET','1', folder)):\n",
    "                    if int(frame.split('.')[0])>=start_frame:\n",
    "                        available_frames.append(frame)\n",
    "                frame = random.choice(available_frames)\n",
    "                while frame in chosen_frames:\n",
    "                    frame = random.choice(available_frames)\n",
    "                chosen_frames.append(frame)\n",
    "                num_frames += 1\n",
    "                # copy the frame in the right folder\n",
    "                shutil.copy(os.path.join(splitted_frames_path, 'TRAINING_SET','1', folder, frame), os.path.join(yolo_dataset_path, 'train', 'fire', frame))\n",
    "                # rename the frame\n",
    "                os.rename(os.path.join(yolo_dataset_path, 'train', 'fire', frame), os.path.join(yolo_dataset_path, 'train', 'fire', str(count)+'.jpg'))\n",
    "                count += 1\n",
    "\n",
    "    count = 0\n",
    "    for folder in os.listdir(os.path.join(splitted_frames_path, 'VALIDATION_SET','1')):\n",
    "        tot_frames = len(os.listdir(os.path.join(splitted_frames_path, 'VALIDATION_SET','1')))\n",
    "        if tot_frames < num_frames_per_video:\n",
    "            num_frames_per_video = tot_frames\n",
    "        chosen_frames = []\n",
    "        # get the annotation file corresponding to the folder\n",
    "        annotation_file = folder.replace(\"mp4\", \"rtf\")\n",
    "        # open the annotation file\n",
    "        with open(os.path.join(splitted_gt_path, 'VALIDATION_SET', '1', annotation_file), 'r') as f:\n",
    "            # get the frame in which the fire starts\n",
    "            start_frame = int(f.readline().split(',')[0])\n",
    "            # randomly sample a frame whose number starts from start_frame\n",
    "            available_frames = []\n",
    "            for _ in range (num_frames_per_video):\n",
    "                for frame in os.listdir(os.path.join(splitted_frames_path, 'VALIDATION_SET','1', folder)):\n",
    "                    if int(frame.split('.')[0])>=start_frame:\n",
    "                        available_frames.append(frame)\n",
    "                frame = random.choice(available_frames)\n",
    "                while frame in chosen_frames:\n",
    "                    frame = random.choice(available_frames)\n",
    "                chosen_frames.append(frame)\n",
    "                # copy the frame in the right folder\n",
    "                shutil.copy(os.path.join(splitted_frames_path, 'VALIDATION_SET','1', folder, frame), os.path.join(yolo_dataset_path, 'val', 'fire', frame))\n",
    "                # rename the frame\n",
    "                os.rename(os.path.join(yolo_dataset_path, 'val', 'fire', frame), os.path.join(yolo_dataset_path, 'val', 'fire', str(count)+'.jpg'))\n",
    "                count += 1\n",
    "\n",
    "create_yolo_datset(splitted_frames_path, splitted_annotations_path, yolo_folder_path, num_frames_per_video=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:01<00:00, 154.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:00<00:00, 181.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 214.35it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 199.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# open all the images in yolo_dataset and resize them to 224x224\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def resize_images(dataset_path, img_size=224):\n",
    "\n",
    "    folders = ['train', 'val']\n",
    "    subfolders = ['fire', 'no_fire']\n",
    "\n",
    "    for folder in folders:\n",
    "        for subfolder in subfolders:\n",
    "            for image in tqdm(os.listdir(os.path.join(dataset_path, folder, subfolder))):\n",
    "                img = Image.open(os.path.join(dataset_path, folder, subfolder, image))\n",
    "                img = img.resize((img_size, img_size))\n",
    "                img.save(os.path.join(dataset_path, folder, subfolder, image))\n",
    "\n",
    "resize_images(yolo_folder_path, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-cls.pt to yolov8m-cls.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32.7M/32.7M [00:19<00:00, 1.72MB/s]\n",
      "Ultralytics YOLOv8.0.119 ðŸš€ Python-3.10.10 torch-2.0.0 MPS\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8m-cls.pt, data=/Users/cristian/Desktop/MachineLearning/yolo_dataset, epochs=50, patience=50, batch=128, imgsz=256, save=True, save_period=1, cache=False, device=mps, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/classify/train\n",
      "Overriding model.yaml nc=1000 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   2655744  ultralytics.nn.modules.conv.Conv             [384, 768, 3, 2]              \n",
      "  8                  -1  2   7084032  ultralytics.nn.modules.block.C2f             [768, 768, 2, True]           \n",
      "  9                  -1  1    988162  ultralytics.nn.modules.head.Classify         [768, 2]                      \n",
      "YOLOv8m-cls summary: 141 layers, 15774898 parameters, 15774898 gradients\n",
      "Transferred 228/230 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=256, width=256, scale=(0.5, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.30000000000000004, 1.7], hue=[-0.015, 0.015]), Normalize(p=1.0, mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 38 weight(decay=0.0), 39 weight(decay=0.001), 39 bias(decay=0.0)\n",
      "Image sizes 256 train, 256 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       1/50         0G      1.323        117        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:39<00:00, 19.83s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.19s/it]\n",
      "                   all      0.717          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       2/50         0G      1.282        117        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:25<00:00, 102.66s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.63s/it]\n",
      "                   all      0.674          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       3/50         0G      1.249        117        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:05<00:00, 92.98s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.44s/it]\n",
      "                   all      0.739          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       4/50         0G      1.159        117        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:22<00:00, 101.39s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.00s/it]\n",
      "                   all      0.674          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       5/50         0G      1.066        117        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [04:07<00:00, 123.95s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.07s/it]\n",
      "                   all      0.652          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       6/50         0G      0.925        117        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:40<00:00, 110.38s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.38s/it]\n",
      "                   all      0.674          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       7/50         0G     0.8613        117        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:56<00:00, 118.26s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.97s/it]\n",
      "                   all      0.696          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       8/50         0G     0.7402        117        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:46<00:00, 113.36s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.81s/it]\n",
      "                   all      0.804          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       9/50         0G     0.6762        117        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:07<00:00, 93.91s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.41s/it]\n",
      "                   all       0.87          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      10/50         0G     0.5852        117        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:58<00:00, 119.31s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.34s/it]\n",
      "                   all      0.848          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      11/50         0G     0.4967        128        256:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [01:52<01:52, 112.86s/it]"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8m-cls.pt\")  # load a pretained model\n",
    "\n",
    "model.train(data=yolo_folder_path, imgsz=IMAGE_SIZE, epochs=50, batch = 128, device=device, save_period=1)  # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate a trained model\n",
    "yolo_weights_path = \"../weights/yolo.pt\"\n",
    "test_videos_folder = \"../test_data/TEST_SET\"\n",
    "test_frames_folder = \"../yolo_dataset/test\"\n",
    "\n",
    "# create the test frames folder if it doesn't exist\n",
    "if not os.path.exists(test_frames_folder):\n",
    "    os.mkdir(test_frames_folder)\n",
    "\n",
    "model = YOLO(yolo_weights_path)  # load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profile(contextlib.ContextDecorator):\n",
    "    \"\"\"\n",
    "    Profile class for profiling execution time. \n",
    "    Can be used as a decorator with @Profile() or as a context manager with 'with Profile():'.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    t : float\n",
    "        Accumulated time.\n",
    "    cuda : bool\n",
    "        Indicates whether CUDA is available.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __enter__()\n",
    "        Starts timing.\n",
    "    __exit__(type, value, traceback)\n",
    "        Stops timing and updates accumulated time.\n",
    "    time()\n",
    "        Returns the current time, synchronizing with CUDA if available.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, t=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the Profile class.\n",
    "\n",
    "        Parameters:\n",
    "        t : float\n",
    "            Initial accumulated time. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        self.t = t  # Accumulated time\n",
    "        self.cuda = torch.cuda.is_available()  # Checks if CUDA is available\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"\n",
    "        Starts timing.\n",
    "        \n",
    "        Returns:\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.start = self.time()  # Start time\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        \"\"\"\n",
    "        Stops timing and updates accumulated time.\n",
    "        \n",
    "        Parameters:\n",
    "        type, value, traceback : \n",
    "            Standard parameters for an exit method in a context manager.\n",
    "        \"\"\"\n",
    "        self.dt = self.time() - self.start  # Delta-time\n",
    "        self.t += self.dt  # Accumulates delta-time\n",
    "\n",
    "    def time(self):\n",
    "        \"\"\"\n",
    "        Returns the current time, synchronizing with CUDA if available.\n",
    "        \n",
    "        Returns:\n",
    "        float\n",
    "            The current time.\n",
    "        \"\"\"\n",
    "        if self.cuda:  # If CUDA is available\n",
    "            torch.cuda.synchronize()  # Synchronizes with CUDA\n",
    "        return time.time()  # Returns current time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, glob, tqdm, torch\n",
    "import pandas as pd\n",
    "\n",
    "# create dataframe to which save if a clip is fire or not and the discovering frame\n",
    "df = pd.DataFrame(columns=['video', 'fire', 'frame'])\n",
    "processed_frames = 0\n",
    "computation_time = 0.0\n",
    "dt = Profile()\n",
    "if torch.cuda.is_available():\n",
    "        memory = []\n",
    "\n",
    "for video in tqdm.tqdm(glob.glob(os.path.join(test_videos_folder, \"**\"), recursive=True)):\n",
    "        \n",
    "    if os.path.isdir(video):\n",
    "        continue\n",
    "    # Process the video\n",
    "    ret = True\n",
    "    cap = cv2.VideoCapture(video) # Decodifica lo streaming\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) # Ottiene il frame rate\n",
    "    f = 0\n",
    "    fire = 0\n",
    "    while ret:\n",
    "        ret, img = cap.read() #Â Chiamando read leggiamo il frame successivo dallo stream\n",
    "        if ret: # ret Ã¨ false quando non ci sono piÃ¹ frame da leggere\n",
    "            f += 1\n",
    "            # Il tensore img letto viene trasformato tramite la classe PIL e lo salviamo\n",
    "            frame_name = os.path.join(test_frames_folder, \"{:05d}.jpg\".format(f))\n",
    "            \n",
    "            cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            cv2.imwrite(frame_name, img)\n",
    "            # evaluate YOLO model on the frame\n",
    "            with dt:\n",
    "                results = model.predict(frame_name, verbose=False)\n",
    "            computation_time += dt.dt\n",
    "            processed_frames += 1\n",
    "            # if it is a fire tell the video is fire and continue with the next video\n",
    "            for result in results:\n",
    "                if result.probs.data.tolist()[0] >= 0.5:\n",
    "                    fire = 1\n",
    "                    ret = 0\n",
    "                    break\n",
    "    # save the result in the dataframe\n",
    "    df.loc[len(df)] = [video, fire, round(f/fps)]\n",
    "    #empty the test frames folder\n",
    "    # for frame in os.listdir(test_frames_folder):\n",
    "    #     os.remove(os.path.join(test_frames_folder, frame))\n",
    "\n",
    "    # Calcolo delle dimensioni della memoria occupata\n",
    "    if torch.cuda.is_available():\n",
    "        memory_bytes = torch.cuda.memory_allocated()\n",
    "        memory.append( memory_bytes / (1024 ** 2))\n",
    "    \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = '../test_data/GT_TEST_SET'\n",
    "# modify the first column of the dataframe to remove the path and keep only the video name\n",
    "df['video'] = df['video'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "delays = []\n",
    "\n",
    "video_counter = 0\n",
    "GUARD_TIME = 5 # seconds\n",
    "MEM_TARGET = 4000 # MB\n",
    "PFR_TARGET = 10 \n",
    "\n",
    "for i in range(len(df)):\n",
    "\n",
    "    annotation_file = df['video'][i].replace(\"mp4\", \"rtf\")\n",
    "\n",
    "    with open(os.path.join(annotations_path, annotation_file), 'r') as f:\n",
    "\n",
    "        line = f.readline()\n",
    "\n",
    "        if line and df['fire'][i] == 1:\n",
    "            # There is a fire in the video\n",
    "            g_frame = int(line.split(',')[0])\n",
    "            p_frame = df['frame'][i]\n",
    "            \n",
    "            if p_frame >= max(0, g_frame - GUARD_TIME):\n",
    "                # Detection is fast enough\n",
    "                delays.append(abs(p_frame - g_frame))\n",
    "                tp += 1\n",
    "            else:\n",
    "                # Detection is not fast enough\n",
    "                fp += 1\n",
    "        elif not line and df['fire'][i]==1:\n",
    "            fp += 1\n",
    "        elif line and df['fire'][i]==0:\n",
    "            fn += 1\n",
    "        elif not line and df['fire'][i]==0:\n",
    "            tn += 1\n",
    "        else:\n",
    "            print(\"Something went wrong\")\n",
    "\n",
    "# Compute precision, recall and f1 score\n",
    "# Count the number of true positives, false positives and false negatives\n",
    "\n",
    "\n",
    "try:\n",
    "    precision = tp/(tp+fp)\n",
    "except ZeroDivisionError:\n",
    "    precision = 0\n",
    "try:\n",
    "    recall = tp/(tp+fn)\n",
    "except ZeroDivisionError:\n",
    "    recall = 0\n",
    "\n",
    "try:\n",
    "    D = sum(delays)/len(delays) \n",
    "    Dn = max(0, 60-D)/60\n",
    "except:\n",
    "    print(\"Can't calculate D because no fire detected in the test set\")\n",
    "    D = float(\"inf\")\n",
    "    Dn = 0\n",
    "\n",
    "f_score = 2 * precision * recall / (1e-10 + precision + recall)\n",
    "pfr = 1 /(computation_time / processed_frames)\n",
    "mem = memory_per_video_occupancy.mean().item()\n",
    "\n",
    "pfr_delta = max(0, PFR_TARGET/pfr - 1)\n",
    "mem_delta = max(0, mem/MEM_TARGET - 1)\n",
    "fds = (precision * recall * Dn) / ((1 + pfr_delta) * (1 + mem_delta))\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "# Print results\n",
    "print(\"..:: RESULTS ::..\")\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F-score: {:.4f}\".format(f_score))\n",
    "print(\"Average notification delay: {:.4f}\".format(D))\n",
    "print(\"Normalized average detection delay: {:.4f}\".format(Dn))\n",
    "print(\"Processing frame rate: {:.4f}\".format(pfr))\n",
    "print(\"Memory usage: {:.4f}\".format(mem))\n",
    "print(\"Final detection score: {:.4f}\".format(fds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
