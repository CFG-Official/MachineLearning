{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration setup\n",
    "All global notebook variables will be placed here for readability and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import gdown\n",
    "import random\n",
    "from project_paths import *\n",
    "\n",
    "# Definition of global variables for the notebook\n",
    "raw_dataset_links = {\n",
    "    \"mivia\": (\n",
    "        \"https://drive.google.com/file/d/1tEz2wVQjPp1MjVHZLa-Z3uyVBnwljgGF/view?usp=sharing\",\n",
    "        \"https://drive.google.com/file/d/123AcAQCldRNE6iKpXuCaVtsaR3uHIOeN/view?usp=sharing\"\n",
    "    ),\n",
    "    \"custom\": (\n",
    "        \"https://drive.google.com/file/d/1eTDG_SbHkCo0OeVwRKugQ2vDV2csDx6q/view?usp=sharing\",\n",
    "        \"https://drive.google.com/file/d/1UjWkvzzezXNOkncas4Q-kP9X9VU2D0OE/view?usp=sharing\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "SELECTED_DATASET = \"custom\"\n",
    "RELOAD_DATASET = False # If True, the dataset is reloaded from the links above, otherwise it is loaded from the local folder\n",
    "MODES = {\"classic\", \"k-fold\"}\n",
    "SELECTED_MODE = \"k-fold\"\n",
    "\n",
    "\n",
    "\n",
    "video_link, labels_link = raw_dataset_links[SELECTED_DATASET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_google_file(shader_url, output_name):\n",
    "  id_url = \"https://drive.google.com/uc?id=\" + shader_url.split(\"/\")[5]\n",
    "  gdown.download(id_url, output_name)\n",
    "\n",
    "\n",
    "def reload_data_folder():\n",
    "    videos_name_file = \"VIDEOS.zip\"\n",
    "    labels_name_file = \"GT.zip\"\n",
    "\n",
    "    if data_folder_path.exists():\n",
    "        shutil.rmtree(data_folder_path)  # delete the folder\n",
    "    \n",
    "    os.makedirs(videos_path)  # create a new one with the same name\n",
    "    \n",
    "    download_google_file(video_link, videos_name_file)\n",
    "    zip_videos_path = videos_path / videos_name_file\n",
    "    shutil.move(videos_name_file, zip_videos_path)\n",
    "    shutil.unpack_archive(zip_videos_path, videos_path)\n",
    "    os.remove(zip_videos_path)\n",
    "    \n",
    "    download_google_file(labels_link, labels_name_file)\n",
    "    zip_labels_path = data_folder_path / labels_name_file\n",
    "    shutil.move(labels_name_file, zip_labels_path)\n",
    "    shutil.unpack_archive(zip_labels_path, data_folder_path)\n",
    "    os.remove(zip_labels_path)\n",
    "    \n",
    "    os.makedirs(train_labels_path)\n",
    "    old_no_fire_labels_folder_path = data_folder_path /\"GT_TRAINING_SET_CL0\"\n",
    "    old_fire_labels_folder_path = data_folder_path / \"GT_TRAINING_SET_CL1\"\n",
    "    shutil.move(old_no_fire_labels_folder_path, train_labels_path)\n",
    "    shutil.move(old_fire_labels_folder_path, train_labels_path)\n",
    "    os.rename(train_labels_path / \"GT_TRAINING_SET_CL0\", train_labels_path / \"0\")\n",
    "    os.rename(train_labels_path / \"GT_TRAINING_SET_CL1\", train_labels_path / \"1\")\n",
    "\n",
    "    if (data_folder_path / \"__MACOSX\").exists():\n",
    "        shutil.rmtree(data_folder_path / \"__MACOSX\")\n",
    "    \n",
    "\n",
    "def check_data_folder(reload = False, size_limit=10):\n",
    "    \"\"\"\n",
    "    Checks if a folder exists and if its size (including subfolders) is less than a given limit.\n",
    "    If both conditions are met, the folder is deleted and recreated.\n",
    "\n",
    "    :param folder_path: path of the folder to check\n",
    "    :param size_limit: size limit in MB\n",
    "    \"\"\"\n",
    "\n",
    "    if reload:\n",
    "        reload_data_folder()\n",
    "        return\n",
    "\n",
    "    if data_folder_path.exists():\n",
    "        total_size = sum(f.stat().st_size for f in data_folder_path.glob('**/*') if f.is_file()) / (1024 * 1024)\n",
    "        if total_size < size_limit:\n",
    "            reload_data_folder()\n",
    "    else:\n",
    "        reload_data_folder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform  \n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Function to determine the device type based on the node name.\n",
    "    It uses a dictionary to map node names to device types.\n",
    "\n",
    "    :return: device type as a string\n",
    "    :raises Exception: if node name is not found in the device_map dictionary\n",
    "    \"\"\"\n",
    "    device_map = {\n",
    "        \"PC-Cristian\": \"cuda\",\n",
    "        \"Dell-G5-15-Alexios\": \"cuda\",\n",
    "        \"MacBook-Pro-di-Cristian.local\": \"mps\",\n",
    "        \"MacBookProDiGrazia\": \"cpu\",\n",
    "        \"DESKTOP-RQVK8SI\":\"cuda\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        return device_map[platform.uname().node]\n",
    "    except KeyError:\n",
    "        raise Exception(\"Node name not found. Please add your node name and its corresponding device to the dictionary.\")\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_folder(RELOAD_DATASET)\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(\"ONFIRE2023_Example_Code\")\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "download_google_file(\"https://drive.google.com/file/d/1rXMCtpus2i2UDdSBD9RwWAxnT0wrrXOk/view?usp=sharing\", \"test_code.zip\")\n",
    "shutil.unpack_archive(\"test_code.zip\", \".\")\n",
    "os.remove(\"test_code.zip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract frames from video files\n",
    "\n",
    "Riconduciamo il problema da un dominio di video ad un dominio di immagini andando a selezionare i frame di ogni video e disponendoli in una sottocartella. È importante salvare i frame con qualità alta, altrimenti distorciamo l'informazione data al classificatore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(p = [0.8, 0.1, 0.1]):\n",
    "\n",
    "\n",
    "    for folder in os.listdir(train_videos_path):\n",
    "        if folder == \".DS_Store\": continue\n",
    "        # Crea le nuove cartelle\n",
    "\n",
    "        os.makedirs(os.path.join(val_videos_path,folder), exist_ok=True)\n",
    "        os.makedirs(os.path.join(test_videos_path,folder), exist_ok=True)\n",
    "        os.makedirs(os.path.join(val_labels_path,folder), exist_ok=True)\n",
    "        os.makedirs(os.path.join(test_labels_path,folder), exist_ok=True)\n",
    "        # Leggi la lista dei video nella cartella originaria\n",
    "        video_list = os.listdir(os.path.join(train_videos_path, folder))\n",
    "\n",
    "        # Calcola il numero di video da assegnare a ciascuna cartella\n",
    "        total_videos = len(video_list)\n",
    "        training_set_size = int(total_videos * p[0])\n",
    "        validation_set_size = int(total_videos * p[1])\n",
    "        test_set_size = int(total_videos * p[2])\n",
    "\n",
    "        # Genera l'indice casuale per mescolare la lista dei video\n",
    "        random.shuffle(video_list)\n",
    "\n",
    "        # QUELLI IN TRAINING RESTANO IN TRAINING\n",
    "        \n",
    "        # Copia i video nella cartella VALIDATION SET\n",
    "        for video in video_list[training_set_size:training_set_size+validation_set_size]:\n",
    "            src = os.path.join(train_videos_path,folder, video)\n",
    "            dst = os.path.join(val_videos_path, folder, video)\n",
    "            shutil.move(src, dst)\n",
    "            video_ext = video.split(\".\")[-1]\n",
    "            label_file = video.replace(video_ext, \"rtf\")\n",
    "            src = os.path.join(train_labels_path, folder, label_file)\n",
    "            dst = os.path.join(val_labels_path, folder, label_file)\n",
    "            shutil.move(src, dst)\n",
    "        \n",
    "        # Copia i video nella cartella TEST SET\n",
    "        for video in video_list[training_set_size+validation_set_size:]:\n",
    "            src = os.path.join(train_videos_path, folder, video)\n",
    "            dst = os.path.join(test_videos_path, folder, video)\n",
    "            shutil.move(src, dst)\n",
    "            video_ext = video.split(\".\")[-1]\n",
    "            label_file = video.replace(video_ext, \"rtf\")\n",
    "            src = os.path.join(train_labels_path, folder, label_file)\n",
    "            dst = os.path.join(test_labels_path, folder, label_file)\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "\n",
    "split_dataset()          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ffmpeg to faster the frame extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, argparse, glob, PIL, tqdm\n",
    "\n",
    "os.makedirs(frames_path, exist_ok=True)\n",
    "\n",
    "def extract_frames(video):\n",
    "    # Process the video\n",
    "    ret = True\n",
    "    cap = cv2.VideoCapture(video) # Decodifica lo streaming\n",
    "    f = 0\n",
    "    while ret:\n",
    "        ret, img = cap.read() # Chiamando read leggiamo il frame successivo dallo stream\n",
    "        if ret: # ret è false quando non ci sono più frame da leggere\n",
    "            f += 1\n",
    "            # Il tensore img letto viene trasformato tramite la classe PIL e lo salviamo\n",
    "            PIL.Image.fromarray(img).save(os.path.join(frames_path, video, \"{:05d}.jpg\".format(f)))\n",
    "    cap.release()\n",
    "\n",
    "# For all the videos\n",
    "for videos_path in [train_videos_path, val_videos_path, test_videos_path]:\n",
    "  file_list = [path for path in Path(videos_path).rglob(\"*\") if path.is_file()]\n",
    "  for video in tqdm.tqdm(file_list):\n",
    "    output_video_frames_folder = Path(os.path.join(frames_path, video.relative_to(videos_path.parent)))\n",
    "    if output_video_frames_folder.is_dir():\n",
    "      continue\n",
    "    \n",
    "    os.makedirs(output_video_frames_folder, exist_ok=True)\n",
    "    # Versione lenta che utilizza la funzione definita prima\n",
    "    #extract_frames(video) \n",
    "    # Versione veloce che fa uso della libreria ffmpeg\n",
    "    os.system(\"ffmpeg -i {} -r 1/1 {}/{}.jpg\".format(video, output_video_frames_folder, \"%05d\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch dataset\n",
    "\n",
    "![](https://albumentations.ai/assets/img/custom/albumentations_logo.png)\n",
    "\n",
    "We use [Albumentations](https://albumentations.ai/) to augment a set of frames with the same augmentation parameters.\n",
    "![](https://albumentations.ai/assets/img/custom/top_image.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use strprtf to parse RTF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install striprtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from typing import List, Union, Tuple, Any\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "import albumentations\n",
    "\n",
    "# Ci serve un codice che durante il training, una volta selezionato un video\n",
    "# da cui caricare i frame, non va a prenderli tutti (potrebbero essere migliaia)\n",
    "# ed il batch si riempirebbe di informazioni polarizzate ad uno stesso video\n",
    "\n",
    "# Per ora mettiamoci nell'ipotesi semplice di caricare un solo frame nel video,\n",
    "# ma credo che se si vuole usare una RNN non è questo il caso\n",
    "\n",
    "# Se selezioniamo sempre il frame centrale, ad esempio, perdiamo la diversità (?) 38:33\n",
    "# In fase di training conviene perché io voglio fornire alla mia rete campioni \n",
    "# differenti. Facendo molte epoche di training, andrò a coprire quasi tutti i frame\n",
    "# di un certo video ed è una strategia che può funzionare in training\n",
    "# In validation, devo capire se l'addestramento deve o non deve ancora andare \n",
    "# avanti, quindi scegliere elementi casuali e quindi avere un validation set che\n",
    "# cambia ad ogni epoca andrebbe a sballare la loss rendendola poco confrontabile\n",
    "# In validation (e quindi anche in test) per confrontare modelli diversi può avere\n",
    "# senso fissarlo in maniera tale da averlo identico ad ogni epoca per avere un\n",
    "# confronto basato sempre sugli stessi video\n",
    "\n",
    "# Passo in avanti, vogliamo effettuare la classificazione sulla base di 3 frame\n",
    "# consecutivi (o consecutivi e intervallati) per catturare la dinamicità (es.\n",
    "# vedere qualcosa di rosso in movimento)\n",
    "# Si noti che un po di augmentation la facciamo sempre, almeno un flip orizzontale\n",
    "# che non ci introduce nessun problema, per cui se la facciamo randomicamente la devo \n",
    "# fare su tutti i 3 frame selezionati altrimenti potrebbe condurci a risultati errati\n",
    "# Le augmentation, quindi, si applicano all'insieme dei frame selezionati per un dato video\n",
    "# Alcune augmentation però hanno ancora senso nel singolo, ad esempio l'introduzione\n",
    "# di un po di rumore applicato (simula un rumore di acquisizione che è \n",
    "# potenzialmente diverso ad ogni frame) a tale scopo si introduce ALMBUMENTATIONS\n",
    "# un pacchetto di augmentation avanzata che consente di fare quello che si fa\n",
    "# con torchvision ma anche di più.\n",
    "# In particolare prende in input qualcosa di più strutturato che un semplice\n",
    "# campione, ma magari un set di frame come serve a noi. Ma gestisce anche cose\n",
    "# più complesse come la gestioen dei bounding box nell'object detection etc...\n",
    "\n",
    "\n",
    "# NOTA: nei video il fuoco, se c'è, permane fino alla fine\n",
    "\n",
    "class VideoRecord(object):\n",
    " # Sostanzilamente mantiene in memoria tutte le informazioni di annotazione di \n",
    " # un particolare video \n",
    "    \"\"\"\n",
    "    Helper class for class VideoFrameDataset. This class\n",
    "    represents a video sample's metadata.\n",
    "\n",
    "    Args:\n",
    "        root_datapath: the system path to the root folder of the videos.\n",
    "        row: A list with four or more elements where\n",
    "             1) The first element is the path to the video sample's frames excluding\n",
    "             the root_datapath prefix\n",
    "             2) The  second element is the starting frame id of the video\n",
    "             3) The third element is the inclusive ending frame id of the video\n",
    "             4) The fourth element is the label index.\n",
    "             5) any following elements are labels in the case of multi-label classification\n",
    "    \"\"\"\n",
    "    def __init__(self, row, root_datapath):\n",
    "        # row = lista di interi che contiene:\n",
    "        # 0: path del video\n",
    "        # 1: primo frame del video (e qui dobbiamo decidere da dove partire)\n",
    "        # 2: ultimo frame\n",
    "        # 3: label della classe\n",
    "        # 4: possono esserci altre annotazioni (es. fumo, fuoco)\n",
    "        self._data = row\n",
    "        self._path = os.path.join(root_datapath, row[0])\n",
    "\n",
    "    @property\n",
    "    def path(self) -> str:\n",
    "        return self._path\n",
    "\n",
    "    @property\n",
    "    def num_frames(self) -> int:\n",
    "        return self.end_frame - self.start_frame + 1  # +1 because end frame is inclusive\n",
    "\n",
    "    @property\n",
    "    def start_frame(self) -> int:\n",
    "        return int(self._data[1])\n",
    "\n",
    "    @property\n",
    "    def end_frame(self) -> int:\n",
    "        return int(self._data[2])\n",
    "\n",
    "    @property\n",
    "    def label(self) -> Union[int, List[int]]:\n",
    "        # just one label_id\n",
    "        if len(self._data) == 4:\n",
    "            return int(self._data[3])\n",
    "        # sample associated with multiple labels\n",
    "        else:\n",
    "            return [int(label_id) for label_id in self._data[3:]]\n",
    "\n",
    "\n",
    "class VideoFrameDataset(torch.utils.data.Dataset):\n",
    "    r\"\"\"\n",
    "    A highly efficient and adaptable dataset class for videos.\n",
    "    Instead of loading every frame of a video,\n",
    "    loads x RGB frames of a video (sparse temporal sampling) and evenly\n",
    "    chooses those frames from start to end of the video, returning\n",
    "    a list of x PIL images or ``FRAMES x CHANNELS x HEIGHT x WIDTH``\n",
    "    tensors.\n",
    "\n",
    "    More specifically, the frame range [START_FRAME, END_FRAME] is divided into NUM_SEGMENTS\n",
    "    segments and FRAMES_PER_SEGMENT consecutive frames are taken from each segment.\n",
    "\n",
    "    Note:\n",
    "        A demonstration of using this class can be seen\n",
    "        in ``demo.py``\n",
    "        https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch\n",
    "\n",
    "    Note:\n",
    "        This dataset broadly corresponds to the frame sampling technique\n",
    "        introduced in ``Temporal Segment Networks`` at ECCV2016\n",
    "        https://arxiv.org/abs/1608.00859.\n",
    "\n",
    "    Args:\n",
    "        root_path: The root path in which video folders lie.\n",
    "                   this is ROOT_DATA from the description above.\n",
    "        num_segments: The number of segments the video should\n",
    "                      be divided into to sample frames from.\n",
    "        frames_per_segment: The number of frames that should\n",
    "                            be loaded per segment. For each segment's\n",
    "                            frame-range, a random start index or the\n",
    "                            center is chosen, from which frames_per_segment\n",
    "                            consecutive frames are loaded.\n",
    "        imagefile_template: The image filename template that video frame files\n",
    "                            have inside of their video folders as described above.\n",
    "        transform: Transform pipeline that receives a list of numpy images/frames.\n",
    "        test_mode: If True, frames are taken from the center of each\n",
    "                   segment, instead of a random location in each segment.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root_path: str, # dove sono contenuti i frame estratti         #? Questo non introdurrebbe dipendenza tra i dati???\n",
    "                 num_segments: int = 1, # divide il video da cui caricare i\n",
    "                 # frame in un certo numero di sezioni della stessa durata.\n",
    "                 frames_per_segment: int = 3, # frame estratti dal segmento in maniera (di default) a caso\n",
    "                 imagefile_template: str='{:05d}.jpg', # pattern nome dei frame\n",
    "                 transform=None, # pipeline di augmentation preprocessing\n",
    "                 totensor=True, # lasciarlo sempre a True\n",
    "                 test_mode: bool = False): # quanto True, i frame vengono presi \n",
    "                 # sempre nelle stesse posizioni. È quello che vogliamo fare \n",
    "                 # quando costruiamo dataset per test o validation\n",
    "        super(VideoFrameDataset, self).__init__()\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.num_segments = num_segments\n",
    "        self.frames_per_segment = frames_per_segment\n",
    "        self.imagefile_template = imagefile_template\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = None\n",
    "        else:\n",
    "            additional_targets = {}\n",
    "            for i in range(self.num_segments * self.frames_per_segment - 1):\n",
    "                additional_targets[\"image%d\" % i] = \"image\"\n",
    "            self.transform = albumentations.Compose([transform],\n",
    "                                                    additional_targets=additional_targets,\n",
    "                                                    p=1)\n",
    "        self.totensor = totensor\n",
    "        self.totensor_transform = ImglistOrdictToTensor()\n",
    "\n",
    "        self._parse_annotationfile()\n",
    "        self._sanity_check_samples()\n",
    "\n",
    "    def _load_image(self, directory: str, idx: int) -> Image.Image:\n",
    "        return np.asarray(Image.open(os.path.join(directory, self.imagefile_template.format(idx))).convert('RGB'))\n",
    "\n",
    "    def _parse_annotationfile(self):\n",
    "      # Usando l'organizzazione delle cartelle suggerita l'annotazione viene \n",
    "      # gestita in automatico. Per ogni file video il codice ricava la posizione\n",
    "      # del rispettivo file rtf e fa il parsing del file per ricavare la GT\n",
    "      # In particolare quando vede un video di classe 1, segna il primo frame di\n",
    "      # avvistamento del fuoco e da lì in poi selezionarà i frame randomici, \n",
    "      # andando ad ignorare un'eventuale fase del video iniziale in cui il fuoco\n",
    "      # non c'è\n",
    "      # Fatto questo parse abbiamo costruito la label del video\n",
    "        self.video_list = []\n",
    "        for class_name in os.listdir(self.root_path):\n",
    "            for video_name in os.listdir(os.path.join(self.root_path, class_name)):\n",
    "                frames_dir = os.path.join(self.root_path, class_name, video_name)\n",
    "                if os.path.isdir(frames_dir):\n",
    "                    frame_path = os.path.join(class_name, video_name)\n",
    "                    end_frame = len(os.listdir(frames_dir))\n",
    "                    print(\"Frame Path: \", frame_path)\n",
    "                    video_ext = frames_dir.split(\".\")[-1]\n",
    "                    annotation_path = frames_dir\\\n",
    "                        .replace(\"\\\\\", \"/\") \\\n",
    "                        .replace(\"FRAMES/\", \"GT/\") \\\n",
    "                        .replace(video_ext, \"rtf\")\n",
    "\n",
    "                    with open(annotation_path, 'r') as file:\n",
    "                        text = rtf_to_text(file.read())\n",
    "                    if len(text):\n",
    "                        label = 1\n",
    "                        start_frame = int(text.split(\",\")[0])\n",
    "                        if start_frame == 0:\n",
    "                          start_frame = 1\n",
    "                    else:\n",
    "                        label = 0\n",
    "                        start_frame = 1\n",
    "\n",
    "                    self.video_list.append(VideoRecord(\n",
    "                        [frame_path, start_frame, end_frame, label],\n",
    "                        self.root_path))\n",
    "\n",
    "    def _sanity_check_samples(self):\n",
    "      # Controllo delle annotazioni ricavate per ogni video\n",
    "        for record in self.video_list:\n",
    "            if record.num_frames <= 0 or record.start_frame == record.end_frame:\n",
    "                print(f\"\\nDataset Warning: video {record.path} seems to have zero RGB frames on disk!\\n\")\n",
    "\n",
    "            elif record.num_frames < (self.num_segments * self.frames_per_segment):\n",
    "                print(f\"\\nDataset Warning: video {record.path} has {record.num_frames} frames \"\n",
    "                      f\"but the dataloader is set up to load \"\n",
    "                      f\"(num_segments={self.num_segments})*(frames_per_segment={self.frames_per_segment})\"\n",
    "                      f\"={self.num_segments * self.frames_per_segment} frames. Dataloader will throw an \"\n",
    "                      f\"error when trying to load this video.\\n\")\n",
    "\n",
    "    def _get_start_indices(self, record: VideoRecord) -> 'np.ndarray[int]':\n",
    "        \"\"\"\n",
    "        For each segment, choose a start index from where frames\n",
    "        are to be loaded from.\n",
    "\n",
    "        Args:\n",
    "            record: VideoRecord denoting a video sample.\n",
    "        Returns:\n",
    "            List of indices of where the frames of each\n",
    "            segment are to be loaded from.\n",
    "        \"\"\"\n",
    "        # choose start indices that are perfectly evenly spread across the video frames.\n",
    "        if self.test_mode:\n",
    "            distance_between_indices = (record.num_frames - self.frames_per_segment + 1) / float(self.num_segments)\n",
    "\n",
    "            start_indices = np.array([int(distance_between_indices / 2.0 + distance_between_indices * x)\n",
    "                                      for x in range(self.num_segments)])\n",
    "        # randomly sample start indices that are approximately evenly spread across the video frames.\n",
    "        else:\n",
    "            max_valid_start_index = (record.num_frames - self.frames_per_segment + 1) // self.num_segments\n",
    "\n",
    "            start_indices = np.multiply(list(range(self.num_segments)), max_valid_start_index) + \\\n",
    "                      np.random.randint(max_valid_start_index, size=self.num_segments)\n",
    "\n",
    "        return start_indices\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Union[\n",
    "        Tuple[List[Image.Image], Union[int, List[int]]],\n",
    "        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n",
    "        Tuple[Any, Union[int, List[int]]],\n",
    "        ]:\n",
    "        \"\"\"\n",
    "        For video with id idx, loads self.NUM_SEGMENTS * self.FRAMES_PER_SEGMENT\n",
    "        frames from evenly chosen locations across the video.\n",
    "\n",
    "        Args:\n",
    "            idx: Video sample index.\n",
    "        Returns:\n",
    "            A tuple of (video, label). Label is either a single\n",
    "            integer or a list of integers in the case of multiple labels.\n",
    "            Video is either 1) a list of PIL images if no transform is used\n",
    "            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n",
    "            if the transform \"ImglistToTensor\" is used\n",
    "            3) or anything else if a custom transform is used.\n",
    "        \"\"\"\n",
    "        record: VideoRecord = self.video_list[idx]\n",
    "\n",
    "        frame_start_indices: 'np.ndarray[int]' = self._get_start_indices(record)\n",
    "\n",
    "        return self._get(record, frame_start_indices)\n",
    "\n",
    "    def _get(self, record: VideoRecord, frame_start_indices: 'np.ndarray[int]') -> Union[\n",
    "        Tuple[List[Image.Image], Union[int, List[int]]],\n",
    "        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n",
    "        Tuple[Any, Union[int, List[int]]],\n",
    "        ]:\n",
    "        \"\"\"\n",
    "        Loads the frames of a video at the corresponding\n",
    "        indices.\n",
    "\n",
    "        Args:\n",
    "            record: VideoRecord denoting a video sample.\n",
    "            frame_start_indices: Indices from which to load consecutive frames from.\n",
    "        Returns:\n",
    "            A tuple of (video, label). Label is either a single\n",
    "            integer or a list of integers in the case of multiple labels.\n",
    "            Video is either 1) a list of PIL images if no transform is used\n",
    "            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n",
    "            if the transform \"ImglistToTensor\" is used\n",
    "            3) or anything else if a custom transform is used.\n",
    "        \"\"\"\n",
    "\n",
    "        frame_start_indices = frame_start_indices + record.start_frame\n",
    "        images = list()\n",
    "\n",
    "        # from each start_index, load self.frames_per_segment\n",
    "        # consecutive frames\n",
    "        for start_index in frame_start_indices:\n",
    "            frame_index = int(start_index)\n",
    "\n",
    "            # load self.frames_per_segment consecutive frames\n",
    "            for _ in range(self.frames_per_segment):\n",
    "                image = self._load_image(record.path, frame_index)\n",
    "                images.append(image)\n",
    "\n",
    "                if frame_index < record.end_frame:\n",
    "                    frame_index += 1\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transform_input = {\"image\": images[0]}\n",
    "            for i, image in enumerate(images[1:]):\n",
    "                transform_input[\"image%d\" % i] = image\n",
    "            images = self.transform(**transform_input)\n",
    "\n",
    "        if self.totensor:\n",
    "            images = self.totensor_transform(images)\n",
    "        return images, record.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)\n",
    "\n",
    "\n",
    "class ImglistOrdictToTensor(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Converts a list or a dict of numpy images to a torch.FloatTensor\n",
    "    of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH).\n",
    "    Can be used as first transform for ``VideoFrameDataset``.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(img_list_or_dict):\n",
    "        \"\"\"\n",
    "        Converts each numpy image in a list or a dict to\n",
    "        a torch Tensor and stacks them into a single tensor.\n",
    "\n",
    "        Args:\n",
    "            img_list_or_dict: list or dict of numpy images.\n",
    "        Returns:\n",
    "            tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\n",
    "        \"\"\"\n",
    "        if isinstance(img_list_or_dict, list):\n",
    "            return torch.stack([transforms.functional.to_tensor(img)\n",
    "                                for img in img_list_or_dict])\n",
    "        else:\n",
    "            return torch.stack([transforms.functional.to_tensor(img_list_or_dict[k])\n",
    "                                for k in img_list_or_dict.keys()])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non viene fatto alcun controllo durante la selezione dei frame sul fatto che in fase di training un frame potrebbe essere scelto più volte. Non è un grande problema in quanto la probabilità che accada è molto bassa, e se succede la rete è molto diversa tra le due circostanze in cui uno stesso frame viene valutato. Inoltre, le augmentation potrebbero agire diversamente su quel frame.\n",
    "\n",
    "Questo controllo potrebbe essere fatto ma complica di molto l'implementazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_visualization():\n",
    "    from torchvision.utils import make_grid\n",
    "    import matplotlib.pyplot as plt\n",
    "    import  albumentations\n",
    "    # Augmentation\n",
    "    # OneOf prende in ingresso una lista di augmentation ed applica con una\n",
    "    # certa probabilità solo una augmentation di questa lista (presa casualmente)\n",
    "    # augmentation = albumentations.OneOf(\n",
    "    #     [\n",
    "    #     # albumentations.HorizontalFlip(p=1.),\n",
    "    #     albumentations.ShiftScaleRotate(shift_limit=0., scale_limit=0.5, rotate_limit=0, p=1.),\n",
    "    #     ],\n",
    "    #     p=1)\n",
    "\n",
    "\n",
    "        \n",
    "    augmentation = albumentations.Compose([\n",
    "        # albumentations.HorizontalFlip(p=0.3), # Flip orizzontale\n",
    "        # albumentations.Rotate(p=1., limit=15), # Rotazione\n",
    "        # albumentations.Blur(p=1), # Sfoca l'immagine\n",
    "        # albumentations.Sharpen(p=1),  # Accentua i bordi\n",
    "        # albumentations.RandomBrightnessContrast(p=1), # Cambia luminosità e contrasto\n",
    "        # albumentations.ChannelShuffle(p=0.2), # Scambia i canali RGB\n",
    "        # albumentations.ColorJitter(p=0.2), # Cambia colore\n",
    "        # albumentations.Downscale(p=0.2), # Riduce la risoluzione\n",
    "        # albumentations.Emboss(p=0.2), # Rilievo\n",
    "        # albumentations.HueSaturationValue(p=1), # Cambia tonalità, saturazione e valore\n",
    "        # albumentations.RandomFog(p=0.7), # Nebbia\n",
    "        # albumentations.RandomRain(p=0.3), # Pioggia\n",
    "        # albumentations.RandomShadow(p=0.5), # Ombra\n",
    "        # albumentations.Solarize(p=1), # Solarizzazione\n",
    "        albumentations.UnsharpMask(p=1), # Maschera di contrasto\n",
    "    ], p=1)\n",
    "    # Dataset with augmentation for visualization\n",
    "    dataset = VideoFrameDataset(root_path=test_frames_path,\n",
    "                                num_segments=2,\n",
    "                                frames_per_segment=2,\n",
    "                                transform=augmentation,\n",
    "                                )\n",
    "    counter = 0\n",
    "    for X, y in dataset:\n",
    "        if counter > 10:\n",
    "            break\n",
    "        plt.figure()\n",
    "        plt.imshow(make_grid(X).numpy().transpose((1, 2, 0)))\n",
    "        plt.title(y)\n",
    "        plt.show()\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "\n",
    "demo_visualization()\n",
    "\n",
    "# Questa demo è utile per testare l'augumentation prima di far iniziare il\n",
    "# training per un certo numero di ore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
