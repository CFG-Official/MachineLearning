{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-01T09:13:17.029674Z","iopub.execute_input":"2023-07-01T09:13:17.030215Z","iopub.status.idle":"2023-07-01T09:13:17.036395Z","shell.execute_reply.started":"2023-07-01T09:13:17.030173Z","shell.execute_reply":"2023-07-01T09:13:17.035505Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:13:17.038619Z","iopub.execute_input":"2023-07-01T09:13:17.039259Z","iopub.status.idle":"2023-07-01T09:13:17.050096Z","shell.execute_reply.started":"2023-07-01T09:13:17.039225Z","shell.execute_reply":"2023-07-01T09:13:17.048945Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import gdown\n\n# Converte i link forniti in link scaricabili e li scarica nella cartella specificata.\ndef download_google_file(shader_url, output_name):\n  id_url = \"https://drive.google.com/uc?id=\" + shader_url.split(\"/\")[5]\n  gdown.download(id_url, output_name)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:13:17.051700Z","iopub.execute_input":"2023-07-01T09:13:17.052339Z","iopub.status.idle":"2023-07-01T09:13:17.060563Z","shell.execute_reply.started":"2023-07-01T09:13:17.052296Z","shell.execute_reply":"2023-07-01T09:13:17.059617Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Download MIVIA dataset","metadata":{}},{"cell_type":"code","source":"# Scarichiamo i video\n\ndownload_google_file(\"https://drive.google.com/file/d/1tEz2wVQjPp1MjVHZLa-Z3uyVBnwljgGF/view?usp=sharing\", \"VIDEOS.zip\")\n!unzip VIDEOS.zip","metadata":{"_kg_hide-output":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scarichiamo le annotazioni\n# Cambiamo i nomi delle directory per semplicità\n# GT = Ground Truth\n\n# Per i video classificati 0 il file di annotazione è vuoto \n# Per i video classificati 1 il file di annotazione non è vuoto e in una riga csv\n# contiene l'indice del frame per cui per la prima volta l'operatore umano ha visto\n# il fumo/fuoco\n\ndownload_google_file(\"https://drive.google.com/file/d/123AcAQCldRNE6iKpXuCaVtsaR3uHIOeN/view?usp=sharing\", \"GT.zip\")\n!unzip GT.zip\n!mkdir -p GT/TRAINING_SET\n!mv GT_TRAINING_SET_CL0 GT/TRAINING_SET/0\n!mv GT_TRAINING_SET_CL1 GT/TRAINING_SET/1","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scarichiamo il codice con i file di test\n# Il modello deve essere tale da poter essere eseguibile con questo codice di\n# test\n\ndownload_google_file(\"https://drive.google.com/file/d/1rXMCtpus2i2UDdSBD9RwWAxnT0wrrXOk/view?usp=sharing\", \"test_code.zip\")\n!unzip test_code.zip","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"videos_path = \"TRAINING_SET\"\nframes_path = \"FRAMES\"","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:15:57.038540Z","iopub.execute_input":"2023-07-01T09:15:57.039354Z","iopub.status.idle":"2023-07-01T09:15:57.044124Z","shell.execute_reply.started":"2023-07-01T09:15:57.039316Z","shell.execute_reply":"2023-07-01T09:15:57.043031Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"!rm -R FRAMES/TRAINING_SET/","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:15:58.823315Z","iopub.execute_input":"2023-07-01T09:15:58.824187Z","iopub.status.idle":"2023-07-01T09:15:59.776008Z","shell.execute_reply.started":"2023-07-01T09:15:58.824142Z","shell.execute_reply":"2023-07-01T09:15:59.774823Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"rm: cannot remove 'FRAMES/TRAINING_SET/': No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2, os, argparse, glob, PIL, tqdm\n\ndef extract_frames(video):\n    # Process the video\n    ret = True\n    cap = cv2.VideoCapture(video) # Decodifica lo streaming\n    f = 0\n    while ret:\n        ret, img = cap.read() # Chiamando read leggiamo il frame successivo dallo stream\n        if ret: # ret è false quando non ci sono più frame da leggere\n            f += 1\n            # Il tensore img letto viene trasformato tramite la classe PIL e lo salviamo\n            PIL.Image.fromarray(img).save(os.path.join(frames_path, video, \"{:05d}.jpg\".format(f)))\n    cap.release()\n\n# For all the videos\nfile_list = [path for path in glob.glob(os.path.join(videos_path,\"**\"), recursive=True)\n            if os.path.isfile(path)]\nprint(file_list)\nfor video in tqdm.tqdm(file_list):\n  if os.path.isdir(os.path.join(frames_path, video)):\n    continue\n  \n  os.makedirs(os.path.join(frames_path, video))\n  # Versione lenta che utilizza la funzione definita prima\n  #extract_frames(video) \n  # Versione veloce che fa uso della libreria ffmpeg\n  os.system(\"ffmpeg -i {} -r 1/1 {}/{}/$Frame{}.jpg\".format(video, frames_path, video, \"%05d\")) \n\n","metadata":{"_kg_hide-output":true,"scrolled":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm *.zip\n!rm -r TRAINING_SET","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:34:57.268050Z","iopub.execute_input":"2023-07-01T09:34:57.268470Z","iopub.status.idle":"2023-07-01T09:34:59.374405Z","shell.execute_reply.started":"2023-07-01T09:34:57.268434Z","shell.execute_reply":"2023-07-01T09:34:59.373152Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"rm: cannot remove '*.zip': No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir MIVIA\n!mv FRAMES MIVIA\n!mv GT MIVIA","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:37:45.684185Z","iopub.execute_input":"2023-07-01T09:37:45.684637Z","iopub.status.idle":"2023-07-01T09:37:48.539073Z","shell.execute_reply.started":"2023-07-01T09:37:45.684572Z","shell.execute_reply":"2023-07-01T09:37:48.537813Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Download custom dataset","metadata":{}},{"cell_type":"code","source":"# Scarichiamo i video\n\ndownload_google_file(\"https://drive.google.com/file/d/1eTDG_SbHkCo0OeVwRKugQ2vDV2csDx6q/view?usp=share_link\", \"VIDEOS.zip\")\n!unzip VIDEOS.zip","metadata":{"scrolled":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scarichiamo le annotazioni\n# Cambiamo i nomi delle directory per semplicità\n# GT = Ground Truth\n\n# Per i video classificati 0 il file di annotazione è vuoto \n# Per i video classificati 1 il file di annotazione non è vuoto e in una riga csv\n# contiene l'indice del frame per cui per la prima volta l'operatore umano ha visto\n# il fumo/fuoco\n\ndownload_google_file(\"https://drive.google.com/file/d/1UjWkvzzezXNOkncas4Q-kP9X9VU2D0OE/view?usp=share_link\", \"GT.zip\")\n!unzip GT.zip\n!mkdir -p GT/TRAINING_SET\n!mv GT_TRAINING_SET_CL0 GT/TRAINING_SET/0\n!mv GT_TRAINING_SET_CL1 GT/TRAINING_SET/1","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm *.zip","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:44:04.144091Z","iopub.execute_input":"2023-07-01T09:44:04.144493Z","iopub.status.idle":"2023-07-01T09:44:05.139901Z","shell.execute_reply.started":"2023-07-01T09:44:04.144454Z","shell.execute_reply":"2023-07-01T09:44:05.138648Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"videos_path = \"TRAINING_SET\"\nframes_path = \"FRAMES\"","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:44:32.916651Z","iopub.execute_input":"2023-07-01T09:44:32.917048Z","iopub.status.idle":"2023-07-01T09:44:32.922414Z","shell.execute_reply.started":"2023-07-01T09:44:32.917014Z","shell.execute_reply":"2023-07-01T09:44:32.921359Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# For all the videos\nfile_list = [path for path in glob.glob(os.path.join(videos_path,\"**\"), recursive=True)\n            if os.path.isfile(path)]\nprint(file_list)\nfor video in tqdm.tqdm(file_list):\n  if os.path.isdir(os.path.join(frames_path, video)):\n    continue\n  \n  os.makedirs(os.path.join(frames_path, video))\n  # Versione veloce che fa uso della libreria ffmpeg\n  os.system(\"ffmpeg -i {} -r 1/1 {}/{}/$Frame{}.jpg\".format(video, frames_path, video, \"%05d\")) ","metadata":{"scrolled":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r TRAINING_SET","metadata":{"execution":{"iopub.status.busy":"2023-07-01T10:31:27.036309Z","iopub.execute_input":"2023-07-01T10:31:27.036730Z","iopub.status.idle":"2023-07-01T10:31:28.584693Z","shell.execute_reply.started":"2023-07-01T10:31:27.036698Z","shell.execute_reply":"2023-07-01T10:31:28.583396Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"!mkdir CUSTOM\n!mv FRAMES CUSTOM\n!mv GT CUSTOM","metadata":{"execution":{"iopub.status.busy":"2023-07-01T10:31:48.321976Z","iopub.execute_input":"2023-07-01T10:31:48.322408Z","iopub.status.idle":"2023-07-01T10:31:51.217666Z","shell.execute_reply.started":"2023-07-01T10:31:48.322370Z","shell.execute_reply":"2023-07-01T10:31:51.216184Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"!mkdir data\n!mv MIVIA data\n!mv CUSTOM data","metadata":{"execution":{"iopub.status.busy":"2023-07-01T10:32:38.558877Z","iopub.execute_input":"2023-07-01T10:32:38.559273Z","iopub.status.idle":"2023-07-01T10:32:41.372255Z","shell.execute_reply.started":"2023-07-01T10:32:38.559239Z","shell.execute_reply":"2023-07-01T10:32:41.370928Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"!pip install striprtf","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport os.path\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\nimport torch\nfrom typing import List, Union, Tuple, Any\nfrom striprtf.striprtf import rtf_to_text\nimport albumentations\n\nclass VideoRecord(object):\n # Sostanzilamente mantiene in memoria tutte le informazioni di annotazione di\n # un particolare video\n    \"\"\"\n    Helper class for class VideoFrameDataset. This class\n    represents a video sample's metadata.\n\n    Args:\n        root_datapath: the system path to the root folder of the videos.\n        row: A list with four or more elements where\n             1) The first element is the path to the video sample's frames excluding\n             the root_datapath prefix\n             2) The  second element is the starting frame id of the video\n             3) The third element is the inclusive ending frame id of the video\n             4) The fourth element is the label index.\n             5) any following elements are labels in the case of multi-label classification\n    \"\"\"\n    def __init__(self, row, root_datapath):\n        # row = lista di interi che contiene:\n        # 0: path del video\n        # 1: primo frame del video (e qui dobbiamo decidere da dove partire)\n        # 2: ultimo frame\n        # 3: label della classe\n        # 4: possono esserci altre annotazioni (es. fumo, fuoco)\n        self._data = row\n        self._path = os.path.join(root_datapath, row[0])\n\n    @property\n    def path(self) -> str:\n        return self._path\n\n    @property\n    def num_frames(self) -> int:\n        return self.end_frame - self.start_frame + 1  # +1 because end frame is inclusive\n\n    @property\n    def start_frame(self) -> int:\n        return int(self._data[1])\n\n    @property\n    def end_frame(self) -> int:\n        return int(self._data[2])\n\n    @property\n    def label(self) -> Union[int, List[int]]:\n        # just one label_id\n        if len(self._data) == 4:\n            return int(self._data[3])\n        # sample associated with multiple labels\n        else:\n            return [int(label_id) for label_id in self._data[3:]]\n\n\nclass VideoFrameDataset(torch.utils.data.Dataset):\n    r\"\"\"\n    A highly efficient and adaptable dataset class for videos.\n    Instead of loading every frame of a video,\n    loads x RGB frames of a video (sparse temporal sampling) and evenly\n    chooses those frames from start to end of the video, returning\n    a list of x PIL images or ``FRAMES x CHANNELS x HEIGHT x WIDTH``\n    tensors.\n\n    More specifically, the frame range [START_FRAME, END_FRAME] is divided into NUM_SEGMENTS\n    segments and FRAMES_PER_SEGMENT consecutive frames are taken from each segment.\n\n    Note:\n        A demonstration of using this class can be seen\n        in ``demo.py``\n        https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch\n\n    Note:\n        This dataset broadly corresponds to the frame sampling technique\n        introduced in ``Temporal Segment Networks`` at ECCV2016\n        https://arxiv.org/abs/1608.00859.\n\n    Args:\n        root_path: The root path in which video folders lie.\n                   this is ROOT_DATA from the description above.\n        num_segments: The number of segments the video should\n                      be divided into to sample frames from.\n        frames_per_segment: The number of frames that should\n                            be loaded per segment. For each segment's\n                            frame-range, a random start index or the\n                            center is chosen, from which frames_per_segment\n                            consecutive frames are loaded.\n        imagefile_template: The image filename template that video frame files\n                            have inside of their video folders as described above.\n        transform: Transform pipeline that receives a list of numpy images/frames.\n        test_mode: If True, frames are taken from the center of each\n                   segment, instead of a random location in each segment.\n\n    \"\"\"\n    def __init__(self,\n                 root_path: str, # dove sono contenuti i frame estratti         #? Questo non introdurrebbe dipendenza tra i dati???\n                 num_segments: int = 1, # divide il video da cui caricare i\n                 # frame in un certo numero di sezioni della stessa durata.\n                 frames_per_segment: int = 3, # frame estratti dal segmento in maniera (di default) a caso\n                 imagefile_template: str='{:05d}.jpg', # pattern nome dei frame\n                 transform=None, # pipeline di augmentation preprocessing\n                 totensor=True, # lasciarlo sempre a True\n                 test_mode: bool = False): # quanto True, i frame vengono presi\n                 # sempre nelle stesse posizioni. È quello che vogliamo fare\n                 # quando costruiamo dataset per test o validation\n        super(VideoFrameDataset, self).__init__()\n\n        self.root_path = root_path\n        self.num_segments = num_segments\n        self.frames_per_segment = frames_per_segment\n        self.imagefile_template = imagefile_template\n        self.test_mode = test_mode\n\n        if transform is None:\n            self.transform = None\n        else:\n            additional_targets = {}\n            for i in range(self.num_segments * self.frames_per_segment - 1):\n                additional_targets[\"image%d\" % i] = \"image\"\n            self.transform = albumentations.Compose([transform],\n                                                    additional_targets=additional_targets,\n                                                    p=1)\n        self.totensor = totensor\n        self.totensor_transform = ImglistOrdictToTensor()\n\n        self._parse_annotationfile()\n        self._sanity_check_samples()\n\n    def _load_image(self, directory: str, idx: int) -> Image.Image:\n        return np.asarray(Image.open(os.path.join(directory, self.imagefile_template.format(idx))).convert('RGB'))\n\n    def _parse_annotationfile(self):\n      # Usando l'organizzazione delle cartelle suggerita l'annotazione viene\n      # gestita in automatico. Per ogni file video il codice ricava la posizione\n      # del rispettivo file rtf e fa il parsing del file per ricavare la GT\n      # In particolare quando vede un video di classe 1, segna il primo frame di\n      # avvistamento del fuoco e da lì in poi selezionarà i frame randomici,\n      # andando ad ignorare un'eventuale fase del video iniziale in cui il fuoco\n      # non c'è\n      # Fatto questo parse abbiamo costruito la label del video\n        self.video_list = []\n        for class_name in os.listdir(self.root_path):\n            for video_name in os.listdir(os.path.join(self.root_path, class_name)):\n                frames_dir = os.path.join(self.root_path, class_name, video_name)\n                if os.path.isdir(frames_dir):\n                    frame_path = os.path.join(class_name, video_name)\n                    end_frame = len(os.listdir(frames_dir))\n\n                    video_ext = frames_dir.split(\".\")[-1]\n                    annotation_path = frames_dir\\\n                        .replace(\"\\\\\", \"/\") \\\n                        .replace(\"FRAMES/\", \"GT/\") \\\n                        .replace(video_ext, \"rtf\")\n\n                    with open(annotation_path, 'r') as file:\n                        text = rtf_to_text(file.read())\n                    if len(text):\n                        label = 1\n                        start_frame = int(text.split(\",\")[0])\n                        if start_frame == 0:\n                          start_frame = 1\n                    else:\n                        label = 0\n                        start_frame = 1\n\n                    self.video_list.append(VideoRecord(\n                        [frame_path, start_frame, end_frame, label],\n                        self.root_path))\n\n    def _sanity_check_samples(self):\n      # Controllo delle annotazioni ricavate per ogni video\n        for record in self.video_list:\n            if record.num_frames <= 0 or record.start_frame == record.end_frame:\n                print(f\"\\nDataset Warning: video {record.path} seems to have zero RGB frames on disk!\\n\")\n\n            elif record.num_frames < (self.num_segments * self.frames_per_segment):\n                print(f\"\\nDataset Warning: video {record.path} has {record.num_frames} frames \"\n                      f\"but the dataloader is set up to load \"\n                      f\"(num_segments={self.num_segments})*(frames_per_segment={self.frames_per_segment})\"\n                      f\"={self.num_segments * self.frames_per_segment} frames. Dataloader will throw an \"\n                      f\"error when trying to load this video.\\n\")\n\n    def _get_start_indices(self, record: VideoRecord) -> 'np.ndarray[int]':\n        \"\"\"\n        For each segment, choose a start index from where frames\n        are to be loaded from.\n\n        Args:\n            record: VideoRecord denoting a video sample.\n        Returns:\n            List of indices of where the frames of each\n            segment are to be loaded from.\n        \"\"\"\n        # choose start indices that are perfectly evenly spread across the video frames.\n        if self.test_mode:\n            distance_between_indices = (record.num_frames - self.frames_per_segment + 1) / float(self.num_segments)\n\n            start_indices = np.array([int(distance_between_indices / 2.0 + distance_between_indices * x)\n                                      for x in range(self.num_segments)])\n        # randomly sample start indices that are approximately evenly spread across the video frames.\n        else:\n            max_valid_start_index = (record.num_frames - self.frames_per_segment + 1) // self.num_segments\n\n            start_indices = np.multiply(list(range(self.num_segments)), max_valid_start_index) + \\\n                      np.random.randint(max_valid_start_index, size=self.num_segments)\n\n        return start_indices\n\n    def __getitem__(self, idx: int) -> Union[\n        Tuple[List[Image.Image], Union[int, List[int]]],\n        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n        Tuple[Any, Union[int, List[int]]],\n        ]:\n        \"\"\"\n        For video with id idx, loads self.NUM_SEGMENTS * self.FRAMES_PER_SEGMENT\n        frames from evenly chosen locations across the video.\n\n        Args:\n            idx: Video sample index.\n        Returns:\n            A tuple of (video, label). Label is either a single\n            integer or a list of integers in the case of multiple labels.\n            Video is either 1) a list of PIL images if no transform is used\n            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n            if the transform \"ImglistToTensor\" is used\n            3) or anything else if a custom transform is used.\n        \"\"\"\n        record: VideoRecord = self.video_list[idx]\n\n        frame_start_indices: 'np.ndarray[int]' = self._get_start_indices(record)\n\n        return self._get(record, frame_start_indices)\n\n    def _get(self, record: VideoRecord, frame_start_indices: 'np.ndarray[int]') -> Union[\n        Tuple[List[Image.Image], Union[int, List[int]]],\n        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n        Tuple[Any, Union[int, List[int]]],\n        ]:\n        \"\"\"\n        Loads the frames of a video at the corresponding\n        indices.\n\n        Args:\n            record: VideoRecord denoting a video sample.\n            frame_start_indices: Indices from which to load consecutive frames from.\n        Returns:\n            A tuple of (video, label). Label is either a single\n            integer or a list of integers in the case of multiple labels.\n            Video is either 1) a list of PIL images if no transform is used\n            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n            if the transform \"ImglistToTensor\" is used\n            3) or anything else if a custom transform is used.\n        \"\"\"\n\n        frame_start_indices = frame_start_indices + record.start_frame\n        images = list()\n\n        # from each start_index, load self.frames_per_segment\n        # consecutive frames\n        for start_index in frame_start_indices:\n            frame_index = int(start_index)\n\n            # load self.frames_per_segment consecutive frames\n            for _ in range(self.frames_per_segment):\n                image = self._load_image(record.path, frame_index)\n                images.append(image)\n\n                if frame_index < record.end_frame:\n                    frame_index += 1\n\n        if self.transform is not None:\n            transform_input = {\"image\": images[0]}\n            for i, image in enumerate(images[1:]):\n                transform_input[\"image%d\" % i] = image\n            images = self.transform(**transform_input)\n\n        if self.totensor:\n            images = self.totensor_transform(images)\n        return images, record.label\n\n    def __len__(self):\n        return len(self.video_list)\n\n\nclass ImglistOrdictToTensor(torch.nn.Module):\n    \"\"\"\n    Converts a list or a dict of numpy images to a torch.FloatTensor\n    of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH).\n    Can be used as first transform for ``VideoFrameDataset``.\n    \"\"\"\n    @staticmethod\n    def forward(img_list_or_dict):\n        \"\"\"\n        Converts each numpy image in a list or a dict to\n        a torch Tensor and stacks them into a single tensor.\n\n        Args:\n            img_list_or_dict: list or dict of numpy images.\n        Returns:\n            tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\n        \"\"\"\n        if isinstance(img_list_or_dict, list):\n            return torch.stack([transforms.functional.to_tensor(img)\n                                for img in img_list_or_dict])\n        else:\n            return torch.stack([transforms.functional.to_tensor(img_list_or_dict[k])\n                                for k in img_list_or_dict.keys()])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-01T11:04:25.525883Z","iopub.execute_input":"2023-07-01T11:04:25.526251Z","iopub.status.idle":"2023-07-01T11:04:25.565957Z","shell.execute_reply.started":"2023-07-01T11:04:25.526220Z","shell.execute_reply":"2023-07-01T11:04:25.564792Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass Model(nn.Module):\n    \n    def __init__(self, num_classes=1, to_train=0):\n        \n        super(Model, self).__init__()\n        self.num_classes = num_classes\n        self.to_train = to_train\n\n    def forward(self, x): \n        pass\n    \n    def get_trainable_parameters(self):\n        return [p for p in self.parameters() if p.requires_grad]\n    \n    def get_preprocessing(self):\n        pass","metadata":{"execution":{"iopub.status.busy":"2023-07-01T11:05:30.419622Z","iopub.execute_input":"2023-07-01T11:05:30.420720Z","iopub.status.idle":"2023-07-01T11:05:30.428336Z","shell.execute_reply.started":"2023-07-01T11:05:30.420674Z","shell.execute_reply":"2023-07-01T11:05:30.427192Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"!pip install pytorchvideo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport albumentations\nfrom pytorchvideo.models import create_res_basic_head\n\nclass X3D_xs(Model):\n    \n    def __init__(self, num_classes=1, to_train=0):\n        \n        super().__init__(num_classes=num_classes, to_train=to_train)\n\n        # Preprocessing parameters\n        side_size = 182\n        crop_size = 182\n        mean = [0.45, 0.45, 0.45]\n        std = [0.225, 0.225, 0.225]\n\n        self.preprocessing = albumentations.Sequential([\n            albumentations.SmallestMaxSize(side_size, always_apply=True),\n            albumentations.CenterCrop(crop_size, crop_size, always_apply=True),\n            albumentations.Normalize(mean=mean,\n                                        std=std,\n                                        max_pixel_value=255.,\n                                        always_apply=True), \n        ])\n\n        # Model parameters\n        self.model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_xs', pretrained=True)\n\n        # Replace the last layer for finetuning\n        self.model.blocks[:-to_train].requires_grad_(False)\n        self.model.blocks[-1] = create_res_basic_head(in_features=192, out_features=num_classes, pool_kernel_size=(1, 6, 6))\n\n    def forward(self, x):\n        # x è un batch di frame di video: B x T x C x H x W oppure T x C x H x W\n        # B = batch size, T = numero di frame, C = numero di canali, H = altezza, W = larghezza\n        if len(x.size()) == 4:\n            x = x.unsqueeze(0)\n\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T11:09:56.571118Z","iopub.execute_input":"2023-07-01T11:09:56.571631Z","iopub.status.idle":"2023-07-01T11:09:56.696215Z","shell.execute_reply.started":"2023-07-01T11:09:56.571564Z","shell.execute_reply":"2023-07-01T11:09:56.695300Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"num_classes = 1\nto_train = 2\n\nmodel = X3D_xs(num_classes, to_train)\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Augumentations","metadata":{}},{"cell_type":"code","source":"import cv2\n\n# Preprocessing and augmentation\n# applica sempre, ha senso per il preprocessing\npreprocessing = albumentations.Sequential([\n    albumentations.Resize(height=256, width=256, always_apply=True),\n    albumentations.CenterCrop(height=224, width=224, always_apply=True),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n                                std=[0.229, 0.224, 0.225],\n                                max_pixel_value=255.,\n                                always_apply=True),\n])\n\naugmentation = albumentations.Compose([\n    albumentations.HorizontalFlip(p=0.2), # Flip orizzontale\n    albumentations.Rotate(p=1., limit=30), # Rotazione\n    albumentations.Blur(p=0.2), # Sfoca l'immagine\n    albumentations.Sharpen(p=0.2),  # Accentua i bordi\n], p=.5)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T11:11:08.207891Z","iopub.execute_input":"2023-07-01T11:11:08.208281Z","iopub.status.idle":"2023-07-01T11:11:08.216674Z","shell.execute_reply.started":"2023-07-01T11:11:08.208248Z","shell.execute_reply":"2023-07-01T11:11:08.215471Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Split","metadata":{}},{"cell_type":"code","source":"!rm -r data/split","metadata":{"execution":{"iopub.status.busy":"2023-07-01T12:14:20.411329Z","iopub.execute_input":"2023-07-01T12:14:20.411731Z","iopub.status.idle":"2023-07-01T12:14:24.425461Z","shell.execute_reply.started":"2023-07-01T12:14:20.411700Z","shell.execute_reply":"2023-07-01T12:14:24.424082Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport shutil\n\ndef split_dataset(source_folder, dest_folder, percentages):\n    \n    # cartella per le annotazioni + cartella per i frames\n    new_folders = ['FRAMES','GT']\n    \n    for new_folder in new_folders:\n        new_path = os.path.join(dest_folder,new_folder)\n        # crea le cartelle di destinazione\n        for folder in ['TRAINING_SET', 'VALIDATION_SET', 'TEST_SET']:\n            for video_type in ['0','1']:\n                os.makedirs(os.path.join(new_path, folder, video_type), exist_ok=True)\n\n    original_frames_path = os.path.join(source_folder, new_folders[0], 'TRAINING_SET')\n    original_annotations_path = os.path.join(source_folder, new_folders[1], 'TRAINING_SET')\n    \n        # Ottieni la lista di cartelle VideoX.mp4\n    video_folders_0 = [folder_name for folder_name in os.listdir(os.path.join(original_frames_path, '0')) if\n                     os.path.isdir(os.path.join(original_frames_path, '0', folder_name))]\n    video_folders_1 = [folder_name for folder_name in os.listdir(os.path.join(original_frames_path, '1')) if\n                     os.path.isdir(os.path.join(original_frames_path, '1', folder_name))]\n\n    random.shuffle(video_folders_0)  # Mischia l'ordine delle cartelle\n    random.shuffle(video_folders_1)  # Mischia l'ordine delle cartelle\n\n    # Calcola il numero di video per ciascuna categoria\n    total_videos_0 = len(video_folders_0)\n    total_videos_1 = len(video_folders_1)\n    train_count_0 = int(total_videos_0 * percentages[0])\n    val_count_0 = int(total_videos_0 * percentages[1])\n    test_count_0 = total_videos_0 - train_count_0 - val_count_0\n    train_count_1 = int(total_videos_1 * percentages[0])\n    val_count_1 = int(total_videos_1 * percentages[1])\n    test_count_1 = total_videos_1 - train_count_1 - val_count_1\n\n    # Copia i file nelle nuove cartelle per la categoria 0\n    for i, video_folder in enumerate(video_folders_0):\n        source_video_path = os.path.join(original_frames_path, '0', video_folder)\n        source_annot_path = os.path.join(original_annotations_path, '0', video_folder[:-4] + '.rtf')\n\n        if i < train_count_0:\n            dest_video_path = os.path.join(dest_folder, 'FRAMES', 'TRAINING_SET', '0', video_folder)\n            dest_annot_path = os.path.join(dest_folder, 'GT', 'TRAINING_SET', '0', video_folder[:-4] + '.rtf')\n        elif i < train_count_0 + val_count_0:\n            dest_video_path = os.path.join(dest_folder, 'FRAMES', 'VALIDATION_SET', '0', video_folder)\n            dest_annot_path = os.path.join(dest_folder, 'GT', 'VALIDATION_SET', '0', video_folder[:-4] + '.rtf')\n        else:\n            dest_video_path = os.path.join(dest_folder, 'FRAMES', 'TEST_SET', '0', video_folder)\n            dest_annot_path = os.path.join(dest_folder, 'GT', 'TEST_SET', '0', video_folder[:-4] + '.rtf')\n\n        shutil.copytree(source_video_path, dest_video_path)\n        shutil.copyfile(source_annot_path, dest_annot_path)\n\n    # Copia i file nelle nuove cartelle per la categoria 1\n    for i, video_folder in enumerate(video_folders_1):\n        source_video_path = os.path.join(original_frames_path, '1', video_folder)\n        source_annot_path = os.path.join(original_annotations_path, '1', video_folder[:-4] + '.rtf')\n\n        if i < train_count_1:\n            dest_video_path = os.path.join(dest_folder, 'FRAMES', 'TRAINING_SET', '1', video_folder)\n            dest_annot_path = os.path.join(dest_folder, 'GT', 'TRAINING_SET', '1', video_folder[:-4] + '.rtf')\n        elif i < train_count_1 + val_count_1:\n            dest_video_path = os.path.join(dest_folder, 'FRAMES', 'VALIDATION_SET', '1', video_folder)\n            dest_annot_path = os.path.join(dest_folder, 'GT', 'VALIDATION_SET', '1', video_folder[:-4] + '.rtf')\n        else:\n            dest_video_path = os.path.join(dest_folder, 'FRAMES', 'TEST_SET', '1', video_folder)\n            dest_annot_path = os.path.join(dest_folder, 'GT', 'TEST_SET', '1', video_folder[:-4] + '.rtf')\n\n        shutil.copytree(source_video_path, dest_video_path)\n        shutil.copyfile(source_annot_path, dest_annot_path)\n\n    print(\"Split completato.\")\n\nsplit_dataset('data/MIVIA', 'data/split', [0.6, 0.2, 0.2])","metadata":{"execution":{"iopub.status.busy":"2023-07-01T12:19:53.796842Z","iopub.execute_input":"2023-07-01T12:19:53.797224Z","iopub.status.idle":"2023-07-01T12:20:09.381050Z","shell.execute_reply.started":"2023-07-01T12:19:53.797193Z","shell.execute_reply":"2023-07-01T12:20:09.379702Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Split completato.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Dataset for training\ntrain_dataset = VideoFrameDataset(root_path=\"data/split/FRAMES/TRAINING_SET/\",\n                            num_segments=3,\n                            frames_per_segment=1,\n                            transform=albumentations.Compose([\n                                preprocessing,\n                                augmentation],\n                                p=1.,\n                            )\n                            )\n\nval_dataset = VideoFrameDataset(root_path=\"data/split/FRAMES/VALIDATION_SET/\",\n                            num_segments=3,\n                            frames_per_segment=1,\n                            transform=albumentations.Compose([\n                                preprocessing,\n                                augmentation],\n                                p=1.,\n                            ),\n                            test_mode=True\n                            )","metadata":{"execution":{"iopub.status.busy":"2023-07-01T12:28:47.273208Z","iopub.execute_input":"2023-07-01T12:28:47.273569Z","iopub.status.idle":"2023-07-01T12:28:47.369253Z","shell.execute_reply.started":"2023-07-01T12:28:47.273538Z","shell.execute_reply":"2023-07-01T12:28:47.368349Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\nfrom tensorboard import notebook\n\ndef start_tensorboard(log_dir):\n  writer = SummaryWriter(os.path.join(\"experiments\",\"runs\", log_dir))\n\n  # run tensorboard in background\n  ! killall tensorboard\n  %load_ext tensorboard\n  %tensorboard --logdir ./runs\n\n  notebook.list() # View open TensorBoard instances\n\n  return writer","metadata":{"execution":{"iopub.status.busy":"2023-07-01T12:33:00.274103Z","iopub.execute_input":"2023-07-01T12:33:00.274508Z","iopub.status.idle":"2023-07-01T12:33:08.389513Z","shell.execute_reply.started":"2023-07-01T12:33:00.274475Z","shell.execute_reply":"2023-07-01T12:33:08.388478Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.utils import make_grid\nfrom tqdm import tqdm\n\ndef one_epoch(model, lossFunction, output_activation, optimizer, train_loader, val_loader, writer, epoch_num):\n  model = model.to(device)\n\n  model.train()\n\n  i_start = epoch_num * len(train_loader)\n  for i, (X, y) in tqdm(enumerate(train_loader), desc=\"epoch {} - train\".format(epoch_num)):\n\n    X = X.to(device)\n    y = y.to(device).float()\n\n    optimizer.zero_grad()\n\n    o = model(X)\n    o = output_activation(o)\n\n    # make shape of o and y the same\n    if o.shape != y.shape:\n      y = y.reshape(o.shape)\n\n    l = lossFunction(o, y)\n\n    l.backward()\n    optimizer.step()\n\n    acc = ((o.detach() > .5) == y.detach()).float().mean()\n\n    # print(\"- batch loss and accuracy : {:.7f}\\t{:.4f}\".format(l.detach().item(), acc))\n    writer.add_scalar('train/loss', l.detach().item(), i_start+i)\n    writer.add_scalar('train/acc', acc, i_start+i)\n\n  model.eval()\n  with torch.no_grad():\n    val_loss = []\n    val_corr_pred = []\n    for X, y in tqdm(val_loader, desc=\"epoch {} - validation\".format(epoch_num)):\n\n      X = X.to(device)\n      y = y.to(device).float()\n\n      o = model(X)\n      o = output_activation(o).squeeze()\n      val_loss.append(lossFunction(o, y))\n      val_corr_pred.append((o > .5) == y)\n\n    val_loss = torch.stack(val_loss).mean().item()\n    val_accuracy = torch.concatenate(val_corr_pred).float().mean().item()\n\n    # print(\"Validation loss and accuracy : {:.7f}\\t{:.4f}\".format(val_loss, val_accuracy))\n    writer.add_scalar('val/loss', val_loss, i_start+i)\n    writer.add_scalar('val/acc', val_accuracy, i_start+i)\n  return val_loss, val_accuracy","metadata":{"execution":{"iopub.status.busy":"2023-07-01T12:29:13.134547Z","iopub.execute_input":"2023-07-01T12:29:13.135272Z","iopub.status.idle":"2023-07-01T12:29:13.148177Z","shell.execute_reply.started":"2023-07-01T12:29:13.135238Z","shell.execute_reply":"2023-07-01T12:29:13.147057Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"from torch.nn import BCELoss, CrossEntropyLoss, Sigmoid, Softmax\nfrom datetime import datetime\n\n# learning parameters\nlossFunction, output_activation = BCELoss(), Sigmoid()\nbatch_size = 64\nlr = .0001\nmomentum = .9\nlambda_reg = 0\n\nepochs = 200\nearly_stopping_patience = 10\n\noptimizer = torch.optim.Adam(model.get_trainable_parameters(),\n                          lr=lr,\n                          weight_decay=lambda_reg)\n\nmodel_name = \"x3d_xs\"\n# create output directory and logger\nname = model_name+\"_\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nexperiment_name = os.path.join(\"experiments\", name)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T12:31:36.926485Z","iopub.execute_input":"2023-07-01T12:31:36.927416Z","iopub.status.idle":"2023-07-01T12:31:36.937515Z","shell.execute_reply.started":"2023-07-01T12:31:36.927380Z","shell.execute_reply":"2023-07-01T12:31:36.936183Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"import os\nos.makedirs(experiment_name)\nwriter = start_tensorboard(name)\n!reload_ext tensorboard","metadata":{"execution":{"iopub.status.busy":"2023-07-01T12:49:57.646931Z","iopub.execute_input":"2023-07-01T12:49:57.647570Z","iopub.status.idle":"2023-07-01T12:50:06.600115Z","shell.execute_reply.started":"2023-07-01T12:49:57.647536Z","shell.execute_reply":"2023-07-01T12:50:06.598943Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"/bin/bash: line 1: killall: command not found\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-2f461de561338cbe\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-2f461de561338cbe\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}},{"name":"stdout","text":"Known TensorBoard instances:\n  - port 6006: logdir ./runs (started 0:00:00 ago; pid 7205)\n/bin/bash: line 1: reload_ext: command not found\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ndataloader_params = {\"batch_size\": batch_size, \"num_workers\": 0, \"pin_memory\": True}\ntrain_loader = DataLoader(train_dataset, shuffle=True, **dataloader_params)\nval_loader = DataLoader(val_dataset, shuffle=False, **dataloader_params)\n\n# early stopping and best model saving\nearly_stopping_counter = early_stopping_patience\nmin_val_loss = 1e10\n\n# training and validation\nval_losses = torch.zeros(epochs)\nval_accuracies = torch.zeros(epochs)\nfor e in range(epochs):\n  print(\"EPOCH {}\".format(e))\n  val_loss, val_accuracy = one_epoch(model, lossFunction, output_activation, optimizer, train_loader, val_loader, writer, e)\n\n  # store the validation metrics\n  val_losses[e] = val_loss\n  val_accuracies[e] = val_accuracy\n  torch.save(val_losses, os.path.join(experiment_name,'val_losses.pth'))\n  torch.save(val_accuracies, os.path.join(experiment_name,'val_accuracies.pth'))\n\n  # save the best model and check the early stopping criteria\n  if val_loss < min_val_loss: # save the best model\n    min_val_loss = val_loss\n    early_stopping_counter = early_stopping_patience # reset early stopping counter\n    torch.save(model.state_dict(), os.path.join(experiment_name,'best_model.pth'))\n    print(\"- saved best model: val_loss =\", val_loss, \"val_accuracy =\", val_accuracy)\n\n  if e>0: # early stopping counter update\n    if val_losses[e] > val_losses[e-1]:\n        early_stopping_counter -= 1 # update early stopping counter\n    else:\n        early_stopping_counter = early_stopping_patience # reset early stopping counter\n  if early_stopping_counter == 0: # early stopping\n      break","metadata":{"execution":{"iopub.status.busy":"2023-07-01T12:50:13.573843Z","iopub.execute_input":"2023-07-01T12:50:13.574298Z","iopub.status.idle":"2023-07-01T13:02:58.061068Z","shell.execute_reply.started":"2023-07-01T12:50:13.574259Z","shell.execute_reply":"2023-07-01T13:02:58.059885Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"EPOCH 0\n","output_type":"stream"},{"name":"stderr","text":"epoch 0 - train: 3it [00:08,  2.95s/it]\nepoch 0 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.5548129081726074 val_accuracy = 0.7049179673194885\nEPOCH 1\n","output_type":"stream"},{"name":"stderr","text":"epoch 1 - train: 3it [00:02,  1.11it/s]\nepoch 1 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.5287127494812012 val_accuracy = 0.737704873085022\nEPOCH 2\n","output_type":"stream"},{"name":"stderr","text":"epoch 2 - train: 3it [00:02,  1.08it/s]\nepoch 2 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.4908022880554199 val_accuracy = 0.7704917788505554\nEPOCH 3\n","output_type":"stream"},{"name":"stderr","text":"epoch 3 - train: 3it [00:02,  1.11it/s]\nepoch 3 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 4\n","output_type":"stream"},{"name":"stderr","text":"epoch 4 - train: 3it [00:02,  1.12it/s]\nepoch 4 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.45676174759864807 val_accuracy = 0.7868852019309998\nEPOCH 5\n","output_type":"stream"},{"name":"stderr","text":"epoch 5 - train: 3it [00:03,  1.08s/it]\nepoch 5 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.44654399156570435 val_accuracy = 0.7704917788505554\nEPOCH 6\n","output_type":"stream"},{"name":"stderr","text":"epoch 6 - train: 3it [00:02,  1.10it/s]\nepoch 6 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.4277288615703583 val_accuracy = 0.7704917788505554\nEPOCH 7\n","output_type":"stream"},{"name":"stderr","text":"epoch 7 - train: 3it [00:02,  1.09it/s]\nepoch 7 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.4159943461418152 val_accuracy = 0.8196721076965332\nEPOCH 8\n","output_type":"stream"},{"name":"stderr","text":"epoch 8 - train: 3it [00:02,  1.13it/s]\nepoch 8 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.4089595377445221 val_accuracy = 0.8032786250114441\nEPOCH 9\n","output_type":"stream"},{"name":"stderr","text":"epoch 9 - train: 3it [00:02,  1.10it/s]\nepoch 9 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.39802148938179016 val_accuracy = 0.8032786250114441\nEPOCH 10\n","output_type":"stream"},{"name":"stderr","text":"epoch 10 - train: 3it [00:02,  1.11it/s]\nepoch 10 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.3501308560371399 val_accuracy = 0.8196721076965332\nEPOCH 11\n","output_type":"stream"},{"name":"stderr","text":"epoch 11 - train: 3it [00:02,  1.10it/s]\nepoch 11 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 12\n","output_type":"stream"},{"name":"stderr","text":"epoch 12 - train: 3it [00:02,  1.12it/s]\nepoch 12 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 13\n","output_type":"stream"},{"name":"stderr","text":"epoch 13 - train: 3it [00:03,  1.06s/it]\nepoch 13 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 14\n","output_type":"stream"},{"name":"stderr","text":"epoch 14 - train: 3it [00:02,  1.13it/s]\nepoch 14 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.3169251084327698 val_accuracy = 0.9016392827033997\nEPOCH 15\n","output_type":"stream"},{"name":"stderr","text":"epoch 15 - train: 3it [00:02,  1.12it/s]\nepoch 15 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.3077356219291687 val_accuracy = 0.9016392827033997\nEPOCH 16\n","output_type":"stream"},{"name":"stderr","text":"epoch 16 - train: 3it [00:02,  1.11it/s]\nepoch 16 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.29107046127319336 val_accuracy = 0.8852458596229553\nEPOCH 17\n","output_type":"stream"},{"name":"stderr","text":"epoch 17 - train: 3it [00:02,  1.09it/s]\nepoch 17 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.28031599521636963 val_accuracy = 0.8852458596229553\nEPOCH 18\n","output_type":"stream"},{"name":"stderr","text":"epoch 18 - train: 3it [00:02,  1.07it/s]\nepoch 18 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 19\n","output_type":"stream"},{"name":"stderr","text":"epoch 19 - train: 3it [00:02,  1.11it/s]\nepoch 19 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.2549821138381958 val_accuracy = 0.9180327653884888\nEPOCH 20\n","output_type":"stream"},{"name":"stderr","text":"epoch 20 - train: 3it [00:02,  1.11it/s]\nepoch 20 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.2524360418319702 val_accuracy = 0.9016392827033997\nEPOCH 21\n","output_type":"stream"},{"name":"stderr","text":"epoch 21 - train: 3it [00:02,  1.09it/s]\nepoch 21 - validation: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.2313801348209381 val_accuracy = 0.9016392827033997\nEPOCH 22\n","output_type":"stream"},{"name":"stderr","text":"epoch 22 - train: 3it [00:02,  1.10it/s]\nepoch 22 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 23\n","output_type":"stream"},{"name":"stderr","text":"epoch 23 - train: 3it [00:02,  1.10it/s]\nepoch 23 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 24\n","output_type":"stream"},{"name":"stderr","text":"epoch 24 - train: 3it [00:02,  1.12it/s]\nepoch 24 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.20234844088554382 val_accuracy = 0.9344261884689331\nEPOCH 25\n","output_type":"stream"},{"name":"stderr","text":"epoch 25 - train: 3it [00:02,  1.12it/s]\nepoch 25 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 26\n","output_type":"stream"},{"name":"stderr","text":"epoch 26 - train: 3it [00:02,  1.10it/s]\nepoch 26 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 27\n","output_type":"stream"},{"name":"stderr","text":"epoch 27 - train: 3it [00:02,  1.10it/s]\nepoch 27 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.2002321034669876 val_accuracy = 0.9180327653884888\nEPOCH 28\n","output_type":"stream"},{"name":"stderr","text":"epoch 28 - train: 3it [00:02,  1.10it/s]\nepoch 28 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.19716039299964905 val_accuracy = 0.9180327653884888\nEPOCH 29\n","output_type":"stream"},{"name":"stderr","text":"epoch 29 - train: 3it [00:02,  1.06it/s]\nepoch 29 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.18105539679527283 val_accuracy = 0.9180327653884888\nEPOCH 30\n","output_type":"stream"},{"name":"stderr","text":"epoch 30 - train: 3it [00:03,  1.07s/it]\nepoch 30 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 31\n","output_type":"stream"},{"name":"stderr","text":"epoch 31 - train: 3it [00:02,  1.08it/s]\nepoch 31 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 32\n","output_type":"stream"},{"name":"stderr","text":"epoch 32 - train: 3it [00:02,  1.13it/s]\nepoch 32 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.16907760500907898 val_accuracy = 0.9016392827033997\nEPOCH 33\n","output_type":"stream"},{"name":"stderr","text":"epoch 33 - train: 3it [00:02,  1.12it/s]\nepoch 33 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.1634662002325058 val_accuracy = 0.9180327653884888\nEPOCH 34\n","output_type":"stream"},{"name":"stderr","text":"epoch 34 - train: 3it [00:02,  1.07it/s]\nepoch 34 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 35\n","output_type":"stream"},{"name":"stderr","text":"epoch 35 - train: 3it [00:02,  1.10it/s]\nepoch 35 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.14613617956638336 val_accuracy = 0.9180327653884888\nEPOCH 36\n","output_type":"stream"},{"name":"stderr","text":"epoch 36 - train: 3it [00:02,  1.07it/s]\nepoch 36 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 37\n","output_type":"stream"},{"name":"stderr","text":"epoch 37 - train: 3it [00:02,  1.11it/s]\nepoch 37 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 38\n","output_type":"stream"},{"name":"stderr","text":"epoch 38 - train: 3it [00:03,  1.03s/it]\nepoch 38 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 39\n","output_type":"stream"},{"name":"stderr","text":"epoch 39 - train: 3it [00:02,  1.08it/s]\nepoch 39 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 40\n","output_type":"stream"},{"name":"stderr","text":"epoch 40 - train: 3it [00:02,  1.12it/s]\nepoch 40 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 41\n","output_type":"stream"},{"name":"stderr","text":"epoch 41 - train: 3it [00:02,  1.12it/s]\nepoch 41 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 42\n","output_type":"stream"},{"name":"stderr","text":"epoch 42 - train: 3it [00:02,  1.10it/s]\nepoch 42 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 43\n","output_type":"stream"},{"name":"stderr","text":"epoch 43 - train: 3it [00:02,  1.13it/s]\nepoch 43 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 44\n","output_type":"stream"},{"name":"stderr","text":"epoch 44 - train: 3it [00:02,  1.11it/s]\nepoch 44 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.14164693653583527 val_accuracy = 0.9180327653884888\nEPOCH 45\n","output_type":"stream"},{"name":"stderr","text":"epoch 45 - train: 3it [00:02,  1.11it/s]\nepoch 45 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 46\n","output_type":"stream"},{"name":"stderr","text":"epoch 46 - train: 3it [00:02,  1.11it/s]\nepoch 46 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 47\n","output_type":"stream"},{"name":"stderr","text":"epoch 47 - train: 3it [00:03,  1.04s/it]\nepoch 47 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 48\n","output_type":"stream"},{"name":"stderr","text":"epoch 48 - train: 3it [00:02,  1.11it/s]\nepoch 48 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 49\n","output_type":"stream"},{"name":"stderr","text":"epoch 49 - train: 3it [00:02,  1.10it/s]\nepoch 49 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.13025988638401031 val_accuracy = 0.9344261884689331\nEPOCH 50\n","output_type":"stream"},{"name":"stderr","text":"epoch 50 - train: 3it [00:02,  1.09it/s]\nepoch 50 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 51\n","output_type":"stream"},{"name":"stderr","text":"epoch 51 - train: 3it [00:02,  1.11it/s]\nepoch 51 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.12675870954990387 val_accuracy = 0.9344261884689331\nEPOCH 52\n","output_type":"stream"},{"name":"stderr","text":"epoch 52 - train: 3it [00:02,  1.10it/s]\nepoch 52 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 53\n","output_type":"stream"},{"name":"stderr","text":"epoch 53 - train: 3it [00:02,  1.09it/s]\nepoch 53 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 54\n","output_type":"stream"},{"name":"stderr","text":"epoch 54 - train: 3it [00:02,  1.07it/s]\nepoch 54 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 55\n","output_type":"stream"},{"name":"stderr","text":"epoch 55 - train: 3it [00:03,  1.07s/it]\nepoch 55 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 56\n","output_type":"stream"},{"name":"stderr","text":"epoch 56 - train: 3it [00:02,  1.12it/s]\nepoch 56 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 57\n","output_type":"stream"},{"name":"stderr","text":"epoch 57 - train: 3it [00:02,  1.12it/s]\nepoch 57 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 58\n","output_type":"stream"},{"name":"stderr","text":"epoch 58 - train: 3it [00:02,  1.10it/s]\nepoch 58 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 59\n","output_type":"stream"},{"name":"stderr","text":"epoch 59 - train: 3it [00:02,  1.11it/s]\nepoch 59 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 60\n","output_type":"stream"},{"name":"stderr","text":"epoch 60 - train: 3it [00:02,  1.12it/s]\nepoch 60 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 61\n","output_type":"stream"},{"name":"stderr","text":"epoch 61 - train: 3it [00:02,  1.12it/s]\nepoch 61 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.1251959651708603 val_accuracy = 0.9672130346298218\nEPOCH 62\n","output_type":"stream"},{"name":"stderr","text":"epoch 62 - train: 3it [00:02,  1.11it/s]\nepoch 62 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 63\n","output_type":"stream"},{"name":"stderr","text":"epoch 63 - train: 3it [00:02,  1.10it/s]\nepoch 63 - validation: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.12192118167877197 val_accuracy = 0.9508196115493774\nEPOCH 64\n","output_type":"stream"},{"name":"stderr","text":"epoch 64 - train: 3it [00:02,  1.09it/s]\nepoch 64 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.1201467216014862 val_accuracy = 0.9344261884689331\nEPOCH 65\n","output_type":"stream"},{"name":"stderr","text":"epoch 65 - train: 3it [00:02,  1.10it/s]\nepoch 65 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.10677728056907654 val_accuracy = 0.9672130346298218\nEPOCH 66\n","output_type":"stream"},{"name":"stderr","text":"epoch 66 - train: 3it [00:02,  1.06it/s]\nepoch 66 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 67\n","output_type":"stream"},{"name":"stderr","text":"epoch 67 - train: 3it [00:02,  1.09it/s]\nepoch 67 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 68\n","output_type":"stream"},{"name":"stderr","text":"epoch 68 - train: 3it [00:02,  1.12it/s]\nepoch 68 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 69\n","output_type":"stream"},{"name":"stderr","text":"epoch 69 - train: 3it [00:02,  1.10it/s]\nepoch 69 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 70\n","output_type":"stream"},{"name":"stderr","text":"epoch 70 - train: 3it [00:02,  1.08it/s]\nepoch 70 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 71\n","output_type":"stream"},{"name":"stderr","text":"epoch 71 - train: 3it [00:02,  1.08it/s]\nepoch 71 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 72\n","output_type":"stream"},{"name":"stderr","text":"epoch 72 - train: 3it [00:03,  1.04s/it]\nepoch 72 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 73\n","output_type":"stream"},{"name":"stderr","text":"epoch 73 - train: 3it [00:02,  1.12it/s]\nepoch 73 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 74\n","output_type":"stream"},{"name":"stderr","text":"epoch 74 - train: 3it [00:02,  1.07it/s]\nepoch 74 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 75\n","output_type":"stream"},{"name":"stderr","text":"epoch 75 - train: 3it [00:02,  1.12it/s]\nepoch 75 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 76\n","output_type":"stream"},{"name":"stderr","text":"epoch 76 - train: 3it [00:02,  1.11it/s]\nepoch 76 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 77\n","output_type":"stream"},{"name":"stderr","text":"epoch 77 - train: 3it [00:02,  1.12it/s]\nepoch 77 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 78\n","output_type":"stream"},{"name":"stderr","text":"epoch 78 - train: 3it [00:02,  1.12it/s]\nepoch 78 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 79\n","output_type":"stream"},{"name":"stderr","text":"epoch 79 - train: 3it [00:02,  1.10it/s]\nepoch 79 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 80\n","output_type":"stream"},{"name":"stderr","text":"epoch 80 - train: 3it [00:03,  1.01s/it]\nepoch 80 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 81\n","output_type":"stream"},{"name":"stderr","text":"epoch 81 - train: 3it [00:02,  1.06it/s]\nepoch 81 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 82\n","output_type":"stream"},{"name":"stderr","text":"epoch 82 - train: 3it [00:02,  1.08it/s]\nepoch 82 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 83\n","output_type":"stream"},{"name":"stderr","text":"epoch 83 - train: 3it [00:02,  1.10it/s]\nepoch 83 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 84\n","output_type":"stream"},{"name":"stderr","text":"epoch 84 - train: 3it [00:02,  1.11it/s]\nepoch 84 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 85\n","output_type":"stream"},{"name":"stderr","text":"epoch 85 - train: 3it [00:02,  1.11it/s]\nepoch 85 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.10081709921360016 val_accuracy = 0.9344261884689331\nEPOCH 86\n","output_type":"stream"},{"name":"stderr","text":"epoch 86 - train: 3it [00:02,  1.10it/s]\nepoch 86 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 87\n","output_type":"stream"},{"name":"stderr","text":"epoch 87 - train: 3it [00:02,  1.09it/s]\nepoch 87 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 88\n","output_type":"stream"},{"name":"stderr","text":"epoch 88 - train: 3it [00:02,  1.10it/s]\nepoch 88 - validation: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 89\n","output_type":"stream"},{"name":"stderr","text":"epoch 89 - train: 3it [00:02,  1.01it/s]\nepoch 89 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 90\n","output_type":"stream"},{"name":"stderr","text":"epoch 90 - train: 3it [00:02,  1.08it/s]\nepoch 90 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 91\n","output_type":"stream"},{"name":"stderr","text":"epoch 91 - train: 3it [00:02,  1.13it/s]\nepoch 91 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 92\n","output_type":"stream"},{"name":"stderr","text":"epoch 92 - train: 3it [00:02,  1.12it/s]\nepoch 92 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 93\n","output_type":"stream"},{"name":"stderr","text":"epoch 93 - train: 3it [00:02,  1.11it/s]\nepoch 93 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 94\n","output_type":"stream"},{"name":"stderr","text":"epoch 94 - train: 3it [00:02,  1.11it/s]\nepoch 94 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 95\n","output_type":"stream"},{"name":"stderr","text":"epoch 95 - train: 3it [00:02,  1.10it/s]\nepoch 95 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 96\n","output_type":"stream"},{"name":"stderr","text":"epoch 96 - train: 3it [00:02,  1.11it/s]\nepoch 96 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 97\n","output_type":"stream"},{"name":"stderr","text":"epoch 97 - train: 3it [00:03,  1.04s/it]\nepoch 97 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 98\n","output_type":"stream"},{"name":"stderr","text":"epoch 98 - train: 3it [00:02,  1.07it/s]\nepoch 98 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 99\n","output_type":"stream"},{"name":"stderr","text":"epoch 99 - train: 3it [00:02,  1.12it/s]\nepoch 99 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 100\n","output_type":"stream"},{"name":"stderr","text":"epoch 100 - train: 3it [00:02,  1.11it/s]\nepoch 100 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 101\n","output_type":"stream"},{"name":"stderr","text":"epoch 101 - train: 3it [00:02,  1.11it/s]\nepoch 101 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 102\n","output_type":"stream"},{"name":"stderr","text":"epoch 102 - train: 3it [00:02,  1.10it/s]\nepoch 102 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 103\n","output_type":"stream"},{"name":"stderr","text":"epoch 103 - train: 3it [00:02,  1.07it/s]\nepoch 103 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 104\n","output_type":"stream"},{"name":"stderr","text":"epoch 104 - train: 3it [00:02,  1.09it/s]\nepoch 104 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 105\n","output_type":"stream"},{"name":"stderr","text":"epoch 105 - train: 3it [00:02,  1.13it/s]\nepoch 105 - validation: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 106\n","output_type":"stream"},{"name":"stderr","text":"epoch 106 - train: 3it [00:02,  1.10it/s]\nepoch 106 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 107\n","output_type":"stream"},{"name":"stderr","text":"epoch 107 - train: 3it [00:02,  1.14it/s]\nepoch 107 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.0926499217748642 val_accuracy = 0.9672130346298218\nEPOCH 108\n","output_type":"stream"},{"name":"stderr","text":"epoch 108 - train: 3it [00:02,  1.11it/s]\nepoch 108 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 109\n","output_type":"stream"},{"name":"stderr","text":"epoch 109 - train: 3it [00:02,  1.08it/s]\nepoch 109 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 110\n","output_type":"stream"},{"name":"stderr","text":"epoch 110 - train: 3it [00:02,  1.11it/s]\nepoch 110 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 111\n","output_type":"stream"},{"name":"stderr","text":"epoch 111 - train: 3it [00:02,  1.08it/s]\nepoch 111 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 112\n","output_type":"stream"},{"name":"stderr","text":"epoch 112 - train: 3it [00:02,  1.11it/s]\nepoch 112 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 113\n","output_type":"stream"},{"name":"stderr","text":"epoch 113 - train: 3it [00:02,  1.08it/s]\nepoch 113 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 114\n","output_type":"stream"},{"name":"stderr","text":"epoch 114 - train: 3it [00:03,  1.07s/it]\nepoch 114 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.08229915052652359 val_accuracy = 0.9508196115493774\nEPOCH 115\n","output_type":"stream"},{"name":"stderr","text":"epoch 115 - train: 3it [00:02,  1.11it/s]\nepoch 115 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 116\n","output_type":"stream"},{"name":"stderr","text":"epoch 116 - train: 3it [00:02,  1.10it/s]\nepoch 116 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"- saved best model: val_loss = 0.08166190981864929 val_accuracy = 0.9508196115493774\nEPOCH 117\n","output_type":"stream"},{"name":"stderr","text":"epoch 117 - train: 3it [00:02,  1.12it/s]\nepoch 117 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 118\n","output_type":"stream"},{"name":"stderr","text":"epoch 118 - train: 3it [00:02,  1.10it/s]\nepoch 118 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 119\n","output_type":"stream"},{"name":"stderr","text":"epoch 119 - train: 3it [00:02,  1.09it/s]\nepoch 119 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 120\n","output_type":"stream"},{"name":"stderr","text":"epoch 120 - train: 3it [00:02,  1.11it/s]\nepoch 120 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 121\n","output_type":"stream"},{"name":"stderr","text":"epoch 121 - train: 3it [00:02,  1.09it/s]\nepoch 121 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 122\n","output_type":"stream"},{"name":"stderr","text":"epoch 122 - train: 3it [00:03,  1.02s/it]\nepoch 122 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 123\n","output_type":"stream"},{"name":"stderr","text":"epoch 123 - train: 3it [00:02,  1.11it/s]\nepoch 123 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 124\n","output_type":"stream"},{"name":"stderr","text":"epoch 124 - train: 3it [00:02,  1.13it/s]\nepoch 124 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 125\n","output_type":"stream"},{"name":"stderr","text":"epoch 125 - train: 3it [00:02,  1.12it/s]\nepoch 125 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 126\n","output_type":"stream"},{"name":"stderr","text":"epoch 126 - train: 3it [00:02,  1.10it/s]\nepoch 126 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 127\n","output_type":"stream"},{"name":"stderr","text":"epoch 127 - train: 3it [00:02,  1.08it/s]\nepoch 127 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 128\n","output_type":"stream"},{"name":"stderr","text":"epoch 128 - train: 3it [00:02,  1.11it/s]\nepoch 128 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 129\n","output_type":"stream"},{"name":"stderr","text":"epoch 129 - train: 3it [00:02,  1.09it/s]\nepoch 129 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 130\n","output_type":"stream"},{"name":"stderr","text":"epoch 130 - train: 3it [00:02,  1.09it/s]\nepoch 130 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 131\n","output_type":"stream"},{"name":"stderr","text":"epoch 131 - train: 3it [00:03,  1.03s/it]\nepoch 131 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 132\n","output_type":"stream"},{"name":"stderr","text":"epoch 132 - train: 3it [00:02,  1.10it/s]\nepoch 132 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 133\n","output_type":"stream"},{"name":"stderr","text":"epoch 133 - train: 3it [00:02,  1.10it/s]\nepoch 133 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 134\n","output_type":"stream"},{"name":"stderr","text":"epoch 134 - train: 3it [00:02,  1.09it/s]\nepoch 134 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 135\n","output_type":"stream"},{"name":"stderr","text":"epoch 135 - train: 3it [00:02,  1.07it/s]\nepoch 135 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 136\n","output_type":"stream"},{"name":"stderr","text":"epoch 136 - train: 3it [00:02,  1.11it/s]\nepoch 136 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 137\n","output_type":"stream"},{"name":"stderr","text":"epoch 137 - train: 3it [00:02,  1.09it/s]\nepoch 137 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 138\n","output_type":"stream"},{"name":"stderr","text":"epoch 138 - train: 3it [00:02,  1.10it/s]\nepoch 138 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 139\n","output_type":"stream"},{"name":"stderr","text":"epoch 139 - train: 3it [00:03,  1.04s/it]\nepoch 139 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 140\n","output_type":"stream"},{"name":"stderr","text":"epoch 140 - train: 3it [00:02,  1.11it/s]\nepoch 140 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 141\n","output_type":"stream"},{"name":"stderr","text":"epoch 141 - train: 3it [00:02,  1.11it/s]\nepoch 141 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 142\n","output_type":"stream"},{"name":"stderr","text":"epoch 142 - train: 3it [00:02,  1.09it/s]\nepoch 142 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 143\n","output_type":"stream"},{"name":"stderr","text":"epoch 143 - train: 3it [00:02,  1.08it/s]\nepoch 143 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 144\n","output_type":"stream"},{"name":"stderr","text":"epoch 144 - train: 3it [00:02,  1.09it/s]\nepoch 144 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 145\n","output_type":"stream"},{"name":"stderr","text":"epoch 145 - train: 3it [00:02,  1.08it/s]\nepoch 145 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 146\n","output_type":"stream"},{"name":"stderr","text":"epoch 146 - train: 3it [00:02,  1.07it/s]\nepoch 146 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 147\n","output_type":"stream"},{"name":"stderr","text":"epoch 147 - train: 3it [00:02,  1.13it/s]\nepoch 147 - validation: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 148\n","output_type":"stream"},{"name":"stderr","text":"epoch 148 - train: 3it [00:02,  1.07it/s]\nepoch 148 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 149\n","output_type":"stream"},{"name":"stderr","text":"epoch 149 - train: 3it [00:02,  1.09it/s]\nepoch 149 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 150\n","output_type":"stream"},{"name":"stderr","text":"epoch 150 - train: 3it [00:02,  1.12it/s]\nepoch 150 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 151\n","output_type":"stream"},{"name":"stderr","text":"epoch 151 - train: 3it [00:02,  1.05it/s]\nepoch 151 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 152\n","output_type":"stream"},{"name":"stderr","text":"epoch 152 - train: 3it [00:02,  1.11it/s]\nepoch 152 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 153\n","output_type":"stream"},{"name":"stderr","text":"epoch 153 - train: 3it [00:02,  1.10it/s]\nepoch 153 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 154\n","output_type":"stream"},{"name":"stderr","text":"epoch 154 - train: 3it [00:02,  1.07it/s]\nepoch 154 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 155\n","output_type":"stream"},{"name":"stderr","text":"epoch 155 - train: 3it [00:02,  1.11it/s]\nepoch 155 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 156\n","output_type":"stream"},{"name":"stderr","text":"epoch 156 - train: 3it [00:03,  1.05s/it]\nepoch 156 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 157\n","output_type":"stream"},{"name":"stderr","text":"epoch 157 - train: 3it [00:02,  1.10it/s]\nepoch 157 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 158\n","output_type":"stream"},{"name":"stderr","text":"epoch 158 - train: 3it [00:02,  1.10it/s]\nepoch 158 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 159\n","output_type":"stream"},{"name":"stderr","text":"epoch 159 - train: 3it [00:02,  1.08it/s]\nepoch 159 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 160\n","output_type":"stream"},{"name":"stderr","text":"epoch 160 - train: 3it [00:02,  1.09it/s]\nepoch 160 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 161\n","output_type":"stream"},{"name":"stderr","text":"epoch 161 - train: 3it [00:02,  1.12it/s]\nepoch 161 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 162\n","output_type":"stream"},{"name":"stderr","text":"epoch 162 - train: 3it [00:02,  1.09it/s]\nepoch 162 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 163\n","output_type":"stream"},{"name":"stderr","text":"epoch 163 - train: 3it [00:02,  1.09it/s]\nepoch 163 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 164\n","output_type":"stream"},{"name":"stderr","text":"epoch 164 - train: 3it [00:03,  1.01s/it]\nepoch 164 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 165\n","output_type":"stream"},{"name":"stderr","text":"epoch 165 - train: 3it [00:02,  1.10it/s]\nepoch 165 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 166\n","output_type":"stream"},{"name":"stderr","text":"epoch 166 - train: 3it [00:02,  1.11it/s]\nepoch 166 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 167\n","output_type":"stream"},{"name":"stderr","text":"epoch 167 - train: 3it [00:02,  1.08it/s]\nepoch 167 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 168\n","output_type":"stream"},{"name":"stderr","text":"epoch 168 - train: 3it [00:02,  1.11it/s]\nepoch 168 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 169\n","output_type":"stream"},{"name":"stderr","text":"epoch 169 - train: 3it [00:02,  1.11it/s]\nepoch 169 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 170\n","output_type":"stream"},{"name":"stderr","text":"epoch 170 - train: 3it [00:02,  1.10it/s]\nepoch 170 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 171\n","output_type":"stream"},{"name":"stderr","text":"epoch 171 - train: 3it [00:02,  1.10it/s]\nepoch 171 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 172\n","output_type":"stream"},{"name":"stderr","text":"epoch 172 - train: 3it [00:02,  1.09it/s]\nepoch 172 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 173\n","output_type":"stream"},{"name":"stderr","text":"epoch 173 - train: 3it [00:03,  1.03s/it]\nepoch 173 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 174\n","output_type":"stream"},{"name":"stderr","text":"epoch 174 - train: 3it [00:02,  1.09it/s]\nepoch 174 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 175\n","output_type":"stream"},{"name":"stderr","text":"epoch 175 - train: 3it [00:02,  1.06it/s]\nepoch 175 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 176\n","output_type":"stream"},{"name":"stderr","text":"epoch 176 - train: 3it [00:02,  1.11it/s]\nepoch 176 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 177\n","output_type":"stream"},{"name":"stderr","text":"epoch 177 - train: 3it [00:02,  1.11it/s]\nepoch 177 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 178\n","output_type":"stream"},{"name":"stderr","text":"epoch 178 - train: 3it [00:02,  1.11it/s]\nepoch 178 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 179\n","output_type":"stream"},{"name":"stderr","text":"epoch 179 - train: 3it [00:02,  1.10it/s]\nepoch 179 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 180\n","output_type":"stream"},{"name":"stderr","text":"epoch 180 - train: 3it [00:02,  1.08it/s]\nepoch 180 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 181\n","output_type":"stream"},{"name":"stderr","text":"epoch 181 - train: 3it [00:03,  1.03s/it]\nepoch 181 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 182\n","output_type":"stream"},{"name":"stderr","text":"epoch 182 - train: 3it [00:02,  1.09it/s]\nepoch 182 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 183\n","output_type":"stream"},{"name":"stderr","text":"epoch 183 - train: 3it [00:02,  1.08it/s]\nepoch 183 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 184\n","output_type":"stream"},{"name":"stderr","text":"epoch 184 - train: 3it [00:02,  1.07it/s]\nepoch 184 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 185\n","output_type":"stream"},{"name":"stderr","text":"epoch 185 - train: 3it [00:02,  1.11it/s]\nepoch 185 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 186\n","output_type":"stream"},{"name":"stderr","text":"epoch 186 - train: 3it [00:02,  1.12it/s]\nepoch 186 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 187\n","output_type":"stream"},{"name":"stderr","text":"epoch 187 - train: 3it [00:02,  1.09it/s]\nepoch 187 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 188\n","output_type":"stream"},{"name":"stderr","text":"epoch 188 - train: 3it [00:02,  1.11it/s]\nepoch 188 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 189\n","output_type":"stream"},{"name":"stderr","text":"epoch 189 - train: 3it [00:02,  1.10it/s]\nepoch 189 - validation: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 190\n","output_type":"stream"},{"name":"stderr","text":"epoch 190 - train: 3it [00:02,  1.12it/s]\nepoch 190 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 191\n","output_type":"stream"},{"name":"stderr","text":"epoch 191 - train: 3it [00:02,  1.08it/s]\nepoch 191 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 192\n","output_type":"stream"},{"name":"stderr","text":"epoch 192 - train: 3it [00:02,  1.09it/s]\nepoch 192 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 193\n","output_type":"stream"},{"name":"stderr","text":"epoch 193 - train: 3it [00:02,  1.11it/s]\nepoch 193 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 194\n","output_type":"stream"},{"name":"stderr","text":"epoch 194 - train: 3it [00:02,  1.11it/s]\nepoch 194 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 195\n","output_type":"stream"},{"name":"stderr","text":"epoch 195 - train: 3it [00:02,  1.09it/s]\nepoch 195 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 196\n","output_type":"stream"},{"name":"stderr","text":"epoch 196 - train: 3it [00:02,  1.09it/s]\nepoch 196 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 197\n","output_type":"stream"},{"name":"stderr","text":"epoch 197 - train: 3it [00:02,  1.09it/s]\nepoch 197 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 198\n","output_type":"stream"},{"name":"stderr","text":"epoch 198 - train: 3it [00:03,  1.05s/it]\nepoch 198 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH 199\n","output_type":"stream"},{"name":"stderr","text":"epoch 199 - train: 3it [00:02,  1.11it/s]\nepoch 199 - validation: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Dataset with augmentation for visualization\ntest_dataset = VideoFrameDataset(root_path=\"data/split/FRAMES/TEST_SET/\",\n                            num_segments=3,\n                            frames_per_segment=1,\n                            ########transform=albumentations.Compose([preprocessing, augmentation], p=1.),\n                            transform=albumentations.Compose([preprocessing], p=1.),\n                            test_mode=True,\n                            )\n\ndataloader_params = {\"batch_size\": batch_size, \"num_workers\": 0, \"pin_memory\": True}\ntest_loader = DataLoader(test_dataset, shuffle=False, **dataloader_params)\n\n\n############## FOR DEBUGGING PURPOSES ##############\n# selected_model = project_root / \"src\" / \"slow_r50_trained\" / \"best_model.pth\"\n# model = FireDetectionModelFactory.create_model(model_name, num_classes, to_train).to(device)\n####################################################\n\nmodel = X3D_xs(num_classes, to_train).to(device)\nmodel.load_state_dict(torch.load(os.path.join(experiment_name,'best_model.pth')))\n\n# model.load_state_dict(torch.load('../weights/X3Dxs.pth', map_location=torch.device('cpu')))\n\nY, Y_hat = [], []\nwith torch.no_grad():\n  model.eval()\n  for X, y in test_loader:\n    Y.append(y)\n    out = (output_activation(model(X.to(device))) > .5).squeeze().cpu()\n    # ensure out has always one dimension\n    out = out.view(-1)  # or out = out.reshape(-1)\n    Y_hat.append(out)\n\nY = torch.cat(Y).float()\nY_hat = torch.cat(Y_hat).float()  # Concatenate along the batch dimension (dim=0)\naccuracy = (Y == Y_hat).float().mean().item()\nprint(\"Accuracy on the test set: {:.4f}\".format(accuracy))","metadata":{"execution":{"iopub.status.busy":"2023-07-01T13:05:23.824010Z","iopub.execute_input":"2023-07-01T13:05:23.824391Z","iopub.status.idle":"2023-07-01T13:05:25.061223Z","shell.execute_reply.started":"2023-07-01T13:05:23.824360Z","shell.execute_reply":"2023-07-01T13:05:25.060187Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on the test set: 0.8923\n","output_type":"stream"}]}]}