{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration setup\n",
    "All global notebook variables will be placed here for readability and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import gdown\n",
    "import random\n",
    "from project_paths import *\n",
    "import cv2, argparse, glob, PIL, tqdm, sys\n",
    "\n",
    "# Definition of global variables for the notebook\n",
    "raw_dataset_links = {\n",
    "    \"mivia\": (\n",
    "        \"https://drive.google.com/file/d/1tEz2wVQjPp1MjVHZLa-Z3uyVBnwljgGF/view?usp=sharing\",\n",
    "        \"https://drive.google.com/file/d/123AcAQCldRNE6iKpXuCaVtsaR3uHIOeN/view?usp=sharing\"\n",
    "    ),\n",
    "    \"custom\": (\n",
    "        \"https://drive.google.com/file/d/1eTDG_SbHkCo0OeVwRKugQ2vDV2csDx6q/view?usp=sharing\",\n",
    "        \"https://drive.google.com/file/d/1UjWkvzzezXNOkncas4Q-kP9X9VU2D0OE/view?usp=sharing\"\n",
    "    ),\n",
    "    # Dataset custom senza video duplicati e con numerazione corretta.\n",
    "    # Fino ad adesso ne è stato testato solo l'effettivo scaricamento.\n",
    "    # Si testerà l'estrazione dei frames a breve.\n",
    "    \"mivia_custom_04_07\": (\n",
    "        \"https://drive.google.com/file/d/13VJ-MGfrS_cvOwLvmiDlqmawGV0ebZGC/view?usp=sharing\",\n",
    "        \"https://drive.google.com/file/d/13TmHShb5tNa9doujghCNoFlB5pAJsJWz/view?usp=sharing\"\n",
    "    ),\n",
    "    \"mivia_custom_09_07\": (\n",
    "        \"https://drive.google.com/file/d/13n9lbNyJchDk5olfHzU6Oy_WQR7ZbPFX/view?usp=drive_link\",\n",
    "        \"https://drive.google.com/file/d/13lJY76imebgbovVc1asDm6GvuNkaF8u8/view?usp=drive_link\"\n",
    "   ),\n",
    "    \"mivia_custom_10_07\": (\n",
    "        \"https://drive.google.com/file/d/14-z_UsyDaPyGCTdXrXS2YDkGIfDfGQNJ/view?usp=sharing\",\n",
    "        \"https://drive.google.com/file/d/13xXyOqp2ET_ySTzd7mP-qk8hs5x3y96g/view?usp=sharing\"\n",
    "    ),\n",
    "    # Versione in cui il numero dei secondi indica il momento in cui entrambe le anomalie sono presenti nei video (Fire e Smoke)\n",
    "    \"mivia_custom_12_07\": ( \n",
    "        \"https://drive.google.com/file/d/14-z_UsyDaPyGCTdXrXS2YDkGIfDfGQNJ/view?usp=sharing\", # Same as mivia_custom_10_07\n",
    "        \"https://drive.google.com/file/d/1FuyS0f1nsEHREvekEAJWHpe2Zte434rQ/view?usp=sharing\" # GT updated\n",
    "    ),\n",
    "    \"mivia_custom_14_07\": (\n",
    "        \"https://drive.google.com/file/d/1d48J_kUpfUmuJjvLAlBwT4PoVFHFXeja/view?usp=sharing\",\n",
    "        \"https://drive.google.com/file/d/1FuyS0f1nsEHREvekEAJWHpe2Zte434rQ/view?usp=sharing\"\n",
    "    ),\n",
    "    \"mivia_only_15_07\": (\n",
    "        \"https://drive.google.com/file/d/1NXFvcJqjZ80l7YEl4k23YV0FveJs-avm/view?usp=sharing\", # Dataset composto soltanto dai video di MIVIA\n",
    "        \"https://drive.google.com/file/d/1NWy-pD6zYDg5t_84H3NkVswIJlD6OUIc/view?usp=sharing\"  # Le annotazioni sono già modificate con i frames assoluti.\n",
    "    )\n",
    "}\n",
    "\n",
    "SELECTED_DATASET = \"mivia_only_15_07\"\n",
    "RELOAD_DATASET = False # If True, the dataset is reloaded from the links above, otherwise it is loaded from the local folder\n",
    "\n",
    "\n",
    "\n",
    "video_link, labels_link = raw_dataset_links[SELECTED_DATASET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_google_file(shader_url, output_name):\n",
    "  id_url = \"https://drive.google.com/uc?id=\" + shader_url.split(\"/\")[5]\n",
    "  gdown.download(id_url, output_name)\n",
    "\n",
    "\n",
    "def reload_data_folder_videos():\n",
    "    videos_name_file = \"VIDEOS.zip\"\n",
    "    shutil.rmtree(videos_path, ignore_errors=True)  # delete the folder\n",
    "    os.makedirs(videos_path, exist_ok=True)  # create a new one with the same name\n",
    "    download_google_file(video_link, videos_name_file)\n",
    "    zip_videos_path = videos_path / videos_name_file\n",
    "    shutil.move(videos_name_file, zip_videos_path)\n",
    "    shutil.unpack_archive(zip_videos_path, videos_path)\n",
    "    os.remove(zip_videos_path)\n",
    "    if (data_folder_path / \"__MACOSX\").exists():\n",
    "        shutil.rmtree(data_folder_path / \"__MACOSX\")\n",
    "\n",
    "\n",
    "def reload_data_folder_annotations():\n",
    "    labels_name_file = \"GT.zip\"\n",
    "    shutil.rmtree(train_original_annotations_path, ignore_errors=True)  # delete the folder\n",
    "    download_google_file(labels_link, labels_name_file)\n",
    "    zip_labels_path = data_folder_path / labels_name_file\n",
    "    shutil.move(labels_name_file, zip_labels_path)\n",
    "    shutil.unpack_archive(zip_labels_path, data_folder_path)\n",
    "    os.remove(zip_labels_path)    \n",
    "    os.makedirs(train_original_annotations_path)\n",
    "    old_no_fire_labels_folder_path = data_folder_path /\"GT_TRAINING_SET_CL0\"\n",
    "    old_fire_labels_folder_path = data_folder_path / \"GT_TRAINING_SET_CL1\"\n",
    "    shutil.move(old_no_fire_labels_folder_path, train_original_annotations_path)\n",
    "    shutil.move(old_fire_labels_folder_path, train_original_annotations_path)\n",
    "    os.rename(train_original_annotations_path / \"GT_TRAINING_SET_CL0\", train_original_annotations_path / \"0\")\n",
    "    os.rename(train_original_annotations_path / \"GT_TRAINING_SET_CL1\", train_original_annotations_path / \"1\")\n",
    "    if (data_folder_path / \"__MACOSX\").exists():\n",
    "        shutil.rmtree(data_folder_path / \"__MACOSX\")\n",
    "\n",
    "def reload_data_folder():\n",
    "    \n",
    "    if data_folder_path.exists():\n",
    "        shutil.rmtree(data_folder_path)  # delete the folder\n",
    "    \n",
    "    reload_data_folder_videos()\n",
    "    reload_data_folder_annotations()\n",
    "    \n",
    "    \n",
    "\n",
    "def check_data_folder(reload = False, size_limit=10):\n",
    "    \"\"\"\n",
    "    Checks if a folder exists and if its size (including subfolders) is less than a given limit.\n",
    "    If both conditions are met, the folder is deleted and recreated.\n",
    "\n",
    "    :param folder_path: path of the folder to check\n",
    "    :param size_limit: size limit in MB\n",
    "    \"\"\"\n",
    "\n",
    "    if reload:\n",
    "        reload_data_folder()\n",
    "        return\n",
    "\n",
    "    if data_folder_path.exists():\n",
    "        total_size = sum(f.stat().st_size for f in data_folder_path.glob('**/*') if f.is_file()) / (1024 * 1024)\n",
    "        if total_size < size_limit:\n",
    "            reload_data_folder()\n",
    "    else:\n",
    "        reload_data_folder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform  \n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Function to determine the device type based on the node name.\n",
    "    It uses a dictionary to map node names to device types.\n",
    "\n",
    "    :return: device type as a string\n",
    "    :raises Exception: if node name is not found in the device_map dictionary\n",
    "    \"\"\"\n",
    "    device_map = {\n",
    "        \"PC-Cristian\": \"cuda\",\n",
    "        \"Dell-G5-15-Alexios\": \"cuda\",\n",
    "        \"MacBook-Pro-di-Cristian.local\": \"mps\",\n",
    "        \"MacBookProDiGrazia\": \"cpu\",\n",
    "        \"DESKTOP-RQVK8SI\":\"cuda\",\n",
    "        \"MacBook-Pro.station\":\"mps\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        return device_map[platform.uname().node]\n",
    "    except KeyError:\n",
    "        raise Exception(\"Node name not found. Please add your node name and its corresponding device to the dictionary.\")\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_folder(RELOAD_DATASET)\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(\"ONFIRE2023_Example_Code\")\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# download_google_file(\"https://drive.google.com/file/d/1rXMCtpus2i2UDdSBD9RwWAxnT0wrrXOk/view?usp=sharing\", \"test_code.zip\")\n",
    "# shutil.unpack_archive(\"test_code.zip\", \".\")\n",
    "# os.remove(\"test_code.zip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract frames from video files\n",
    "\n",
    "Riconduciamo il problema da un dominio di video ad un dominio di immagini andando a selezionare i frame di ogni video e disponendoli in una sottocartella. È importante salvare i frame con qualità alta, altrimenti distorciamo l'informazione data al classificatore."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ffmpeg to faster the frame extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(original_frames_path, exist_ok=True)\n",
    "\n",
    "file_list = [path for path in Path(videos_path).rglob(\"*\") if path.is_file()]\n",
    "for video in tqdm.tqdm(file_list):\n",
    "  output_video_frames_folder = Path(os.path.join(original_frames_path, video.relative_to(videos_path)))\n",
    "  if output_video_frames_folder.is_dir():\n",
    "    continue\n",
    "  \n",
    "  os.makedirs(output_video_frames_folder, exist_ok=True)\n",
    "  os.system(\"ffmpeg -i {} -r 1/1 {}/{}.jpg\".format(video, output_video_frames_folder, \"%05d\"))\n",
    "  shutil.rmtree(output_video_frames_folder / \"__MACOSX\", ignore_errors=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(frames_source_path, annotations_source_path, frames_destination_path, annotations_destination_path, mode=\"classic\", p=[0.8,0.2], mivia_validation_percentage=0.8):\n",
    "        \"\"\"\n",
    "        If mode is \"classic\", it splits the dataset into train, validation and test sets.\n",
    "        If mode is \"k-fold\", it splits only the test. In this case other videos remain in the training set folder.\n",
    "        \"\"\"\n",
    "        # set paths and the mivia percentage of videos which have to be put in the validation set\n",
    "        no_fire_original_frames_folder = frames_source_path /'0'\n",
    "        fire_original_frames_folder = frames_source_path / '1'\n",
    "        no_fire_original_annotations_folder = annotations_source_path / '0'\n",
    "        fire_original_annotations_folder = annotations_source_path / '1'\n",
    "        \n",
    "        # create the destination folders\n",
    "        os.makedirs(frames_destination_path / \"TRAINING_SET\" / \"0\", exist_ok=True)\n",
    "        os.makedirs(frames_destination_path / \"TRAINING_SET\" / \"1\", exist_ok=True)\n",
    "\n",
    "        os.makedirs(train_splitted_annotations_path / \"0\", exist_ok=True)\n",
    "        os.makedirs(train_splitted_annotations_path / \"1\", exist_ok=True)\n",
    "\n",
    "        no_fires_folders = [folder_name for folder_name in os.listdir(no_fire_original_frames_folder) if\n",
    "                            os.path.isdir(os.path.join(no_fire_original_frames_folder, folder_name))]\n",
    "        \n",
    "\n",
    "        fires_folders = [folder_name for folder_name in os.listdir(fire_original_frames_folder) if\n",
    "                            os.path.isdir(os.path.join(fire_original_frames_folder, folder_name))]\n",
    "\n",
    "        random.shuffle(no_fires_folders)\n",
    "        random.shuffle(fires_folders)\n",
    "\n",
    "        # count total no fires and fires\n",
    "        total_no_fires = len(no_fires_folders)\n",
    "        total_fires = len(fires_folders)\n",
    "        print(f\"Total no fires: {total_no_fires}\", f\"Total fires: {total_fires}\")\n",
    "\n",
    "        # create thea list of entries for each category\n",
    "        mivia_fire_entries = []\n",
    "        custom_fire_entries = []\n",
    "        mivia_no_fire_entries = []\n",
    "        custom_no_fire_entries = []\n",
    "            \n",
    "        for folder in no_fires_folders:\n",
    "            if \"custom\" in folder:\n",
    "                custom_no_fire_entries.append(folder)\n",
    "            else:\n",
    "                mivia_no_fire_entries.append(folder)\n",
    "                \n",
    "        for folder in fires_folders:\n",
    "            if \"custom\" in folder:\n",
    "                custom_fire_entries.append(folder)\n",
    "            else:\n",
    "                mivia_fire_entries.append(folder)\n",
    "\n",
    "        # if mode is \"classic\", it splits the dataset into train and validation\n",
    "        if mode == \"classic\":\n",
    "            # create the validation set folders\n",
    "            os.makedirs(frames_destination_path / \"VALIDATION_SET\" / \"0\", exist_ok=True)\n",
    "            os.makedirs(frames_destination_path / \"VALIDATION_SET\" / \"1\", exist_ok=True)\n",
    "            \n",
    "            os.makedirs(val_splitted_annotations_path / \"0\", exist_ok=True)\n",
    "            os.makedirs(val_splitted_annotations_path / \"1\", exist_ok=True)\n",
    "\n",
    "            # we have to compute the number of videos to put in the validation set from the total of the no fire videos\n",
    "            # and the total of the fire videos depending on the given percentage\n",
    "            no_fires_to_put_in_validation = int(total_no_fires * p[1])\n",
    "            # now on the total of videos for each catedory we have to compute how many of them have to come from the mivia dataset\n",
    "            mivia_no_fires_to_put_in_validation = int(no_fires_to_put_in_validation * mivia_validation_percentage)\n",
    "            # if this percentage is higher than the total number of mivia available videos, we put all of them in the validation set and the\n",
    "            # remaining ones will come from the custom dataset\n",
    "            if mivia_no_fires_to_put_in_validation > len(mivia_no_fire_entries):\n",
    "                mivia_no_fires_to_put_in_validation = len(mivia_no_fire_entries)\n",
    "            custom_no_fires_to_put_in_validation = no_fires_to_put_in_validation - mivia_no_fires_to_put_in_validation\n",
    "\n",
    "            # we do the same for the fire videos\n",
    "            fires_to_put_in_validation = int(total_fires * p[1])\n",
    "            mivia_fires_to_put_in_validation = int(fires_to_put_in_validation * mivia_validation_percentage)\n",
    "            if mivia_fires_to_put_in_validation > len(mivia_fire_entries):\n",
    "                mivia_fires_to_put_in_validation = len(mivia_fire_entries)\n",
    "            custom_fires_to_put_in_validation = fires_to_put_in_validation - mivia_fires_to_put_in_validation\n",
    "            \n",
    "            # create the lists of the validation set and the training set\n",
    "            no_fire_entries_validation = mivia_no_fire_entries[:mivia_no_fires_to_put_in_validation] + custom_no_fire_entries[:custom_no_fires_to_put_in_validation]\n",
    "            fire_entries_validation = mivia_fire_entries[:mivia_fires_to_put_in_validation] + custom_fire_entries[:custom_fires_to_put_in_validation]\n",
    "            no_fire_entries_training = mivia_no_fire_entries[mivia_no_fires_to_put_in_validation:] + custom_no_fire_entries[custom_no_fires_to_put_in_validation:]\n",
    "            fire_entries_training = mivia_fire_entries[mivia_fires_to_put_in_validation:] + custom_fire_entries[custom_fires_to_put_in_validation:]\n",
    "            \n",
    "            # iterate over the validation entries and copy them in the validation set folder\n",
    "            for folder in no_fire_entries_validation:\n",
    "                shutil.copytree(no_fire_original_frames_folder / folder, frames_destination_path / \"VALIDATION_SET\" / \"0\" / folder)\n",
    "                file_name = folder.replace(\"mp4\", \"rtf\")\n",
    "                shutil.copy(no_fire_original_annotations_folder / file_name, val_splitted_annotations_path / \"0\" / file_name)\n",
    "                \n",
    "            for folder in fire_entries_validation:\n",
    "                shutil.copytree(fire_original_frames_folder / folder, frames_destination_path / \"VALIDATION_SET\" / \"1\" / folder)\n",
    "                file_name = folder.replace(\"mp4\", \"rtf\")\n",
    "                shutil.copy(fire_original_annotations_folder / file_name, val_splitted_annotations_path / \"1\" / file_name)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            no_fire_entries_training = mivia_no_fire_entries + custom_no_fire_entries\n",
    "            fire_entries_training = mivia_fire_entries + custom_fire_entries\n",
    "            \n",
    "        # iterate over the training entries and copy them in the training set folder\n",
    "        for folder in no_fire_entries_training:\n",
    "            shutil.copytree(no_fire_original_frames_folder / folder, frames_destination_path / \"TRAINING_SET\" / \"0\" / folder)\n",
    "            file_name = folder.replace(\"mp4\", \"rtf\")\n",
    "            shutil.copy(no_fire_original_annotations_folder / file_name, train_splitted_annotations_path / \"0\" / file_name)\n",
    "            \n",
    "        for folder in fire_entries_training:\n",
    "            shutil.copytree(fire_original_frames_folder / folder, frames_destination_path / \"TRAINING_SET\" / \"1\" / folder)\n",
    "            file_name = folder.replace(\"mp4\", \"rtf\")\n",
    "            shutil.copy(fire_original_annotations_folder / file_name, train_splitted_annotations_path / \"1\" / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_management.dataset_management import *\n",
    "split_dataset(train_original_frames_path, train_original_annotations_path, splitted_frames_path, splitted_annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of elements in splitted_frames/training_set/0, splitted_frames/training_set/1, splitted_frames/validation_set/0, splitted_frames/validation_set/1\n",
    "train_0 = [folder for folder in os.listdir(splitted_frames_path / \"TRAINING_SET\" / \"0\") if os.path.isdir(splitted_frames_path / \"TRAINING_SET\" / \"0\" / folder)]\n",
    "train_1 = [folder for folder in os.listdir(splitted_frames_path / \"TRAINING_SET\" / \"1\") if os.path.isdir(splitted_frames_path / \"TRAINING_SET\" / \"1\" / folder)]\n",
    "val_0 = [folder for folder in os.listdir(splitted_frames_path / \"VALIDATION_SET\" / \"0\") if os.path.isdir(splitted_frames_path / \"VALIDATION_SET\" / \"0\" / folder)]\n",
    "val_1 = [folder for folder in os.listdir(splitted_frames_path / \"VALIDATION_SET\" / \"1\") if os.path.isdir(splitted_frames_path / \"VALIDATION_SET\" / \"1\" / folder)]\n",
    "\n",
    "mivia_train_0 = []\n",
    "custom_train_0 = []\n",
    "for folder in train_0:\n",
    "    if \"custom\" in folder:\n",
    "        custom_train_0.append(folder)\n",
    "    else:\n",
    "        mivia_train_0.append(folder)\n",
    "        \n",
    "mivia_train_1 = []\n",
    "custom_train_1 = []\n",
    "for folder in train_1:\n",
    "    if \"custom\" in folder:\n",
    "        custom_train_1.append(folder)\n",
    "    else:\n",
    "        mivia_train_1.append(folder)\n",
    "\n",
    "mivia_val_0 = []\n",
    "custom_val_0 = []\n",
    "for folder in val_0:\n",
    "    if \"custom\" in folder:\n",
    "        custom_val_0.append(folder)\n",
    "    else:\n",
    "        mivia_val_0.append(folder)\n",
    "\n",
    "mivia_val_1 = []\n",
    "custom_val_1 = []\n",
    "for folder in val_1:\n",
    "    if \"custom\" in folder:\n",
    "        custom_val_1.append(folder)\n",
    "    else:\n",
    "        mivia_val_1.append(folder)\n",
    "\n",
    "print(\"mivia_train_0: \", len(mivia_train_0))\n",
    "print(\"custom_train_0: \", len(custom_train_0))\n",
    "print(\"mivia_train_1: \", len(mivia_train_1))\n",
    "print(\"custom_train_1: \", len(custom_train_1))\n",
    "print(\"mivia_val_0: \", len(mivia_val_0))\n",
    "print(\"custom_val_0: \", len(custom_val_0))\n",
    "print(\"mivia_val_1: \", len(mivia_val_1))\n",
    "print(\"custom_val_1: \", len(custom_val_1))\n",
    "\n",
    "for folder in mivia_train_0:\n",
    "    if folder in custom_train_0:\n",
    "        print(\"Duplicate folder:\", folder)\n",
    "        \n",
    "for folder in mivia_train_1:\n",
    "    if folder in custom_train_1:\n",
    "        print(\"Duplicate folder:\", folder)\n",
    "        \n",
    "for folder in mivia_val_0:\n",
    "    if folder in custom_val_0:\n",
    "        print(\"Duplicate folder:\", folder)\n",
    "\n",
    "for folder in mivia_val_1:\n",
    "    if folder in custom_val_1:\n",
    "        print(\"Duplicate folder:\", folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('MachineLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a2c952ceb315243f34ba302acbe0fb5218585a422897d4fb07b81cc17398136"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
