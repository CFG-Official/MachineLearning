{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VYNYr9sJ5QOY"
      },
      "source": [
        "![](https://mivia.unisa.it/onfire2023/assets/img/iciap_logo.jpg)\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Dear Participant,\n",
        "\n",
        "Thanks for your participation to ONFIRE Contest 2023.\n",
        "\n",
        "Please download the following files:\n",
        "- Training videos: https://drive.google.com/file/d/1tEz2wVQjPp1MjVHZLa-Z3uyVBnwljgGF/view?usp=sharing\n",
        "- Training annotations: https://drive.google.com/file/d/123AcAQCldRNE6iKpXuCaVtsaR3uHIOeN/view?usp=sharing\n",
        "- Code example: https://drive.google.com/file/d/1rXMCtpus2i2UDdSBD9RwWAxnT0wrrXOk/view?usp=sharing\n",
        "\n",
        "Please remind that:\n",
        "- The deadline for the submission of the methods is 21st July, 2023. The submission must be done with an email in which the participants share (directly or with external links) the trained model, the code and the report. The participants can receive the training set and its annotations by sending an email to onfire2023@unisa.it, in which they also communicate the name of the team.\n",
        "- The participants can use these training samples and annotations, but also additional videos.\n",
        "- The participants must submit their trained model and their code by carefully following the detailed instructions reported in the website. \n",
        "- The participants are strongly encouraged to submit a contest paper to ICIAP 2023, whose deadline is 28th July, 2023. The contest paper must be also sent by email to the organizers. Otherwise, the participants must produce a brief PDF report of the proposed method.\n",
        "- The detailed instructions of the proposed method can be downloaded here: https://mivia.unisa.it/onfire2023/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FupE6O1CrQzu",
        "outputId": "c3a79b62-fe00-4023-dd6f-46ad50ff50e9"
      },
      "outputs": [],
      "source": [
        "# You can follow this tutorial for more information - https://www.tutorialspoint.com/google_colab/index.htm\n",
        "# You can also see this video - https://www.youtube.com/watch?v=inN8seMm7UI\n",
        "\n",
        "# Mount your Drive - After doing this step, your Google Drive folders are accessible from Google Colab.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "import os\n",
        "import platform  \n",
        "\n",
        "# Codice utile a Cristian per cambiare il path in base al sistema operativo e utilizzare GPU/MPS a seconda del caso\n",
        "if platform.uname().node == \"PC-Cristian\":\n",
        "    device = \"cuda\"\n",
        "elif platform.uname().node == \"Dell-G5-15-Alexios\":\n",
        "    print(\"Welcome Alexios!\")\n",
        "    device = \"cuda\"\n",
        "    main_folder = os.getcwd()\n",
        "elif platform.uname().node == \"MacBook-Pro-di-Cristian.local\":\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3tr1jyxY6PlS"
      },
      "source": [
        "# Download and unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BONhSjX4sMAF"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "# Converte i link forniti in link scaricabili e li scarica nella cartella specificata.\n",
        "def download_google_file(shader_url, output_name):\n",
        "  id_url = \"https://drive.google.com/uc?id=\" + shader_url.split(\"/\")[5]\n",
        "  gdown.download(id_url, output_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IYEqO-5svPd",
        "outputId": "d9f266b2-8b54-4fae-a8c1-091c6ac16664"
      },
      "outputs": [],
      "source": [
        "# Scarichiamo i video\n",
        "mivia_videos = \"https://drive.google.com/file/d/1tEz2wVQjPp1MjVHZLa-Z3uyVBnwljgGF/view?usp=sharing\"\n",
        "our_videos = \"https://drive.google.com/file/d/1crTM4H7IsKyMHLRm-x4xuzSt2I0K39uj/view?usp=sharing\"\n",
        "download_google_file(our_videos, \"VIDEOS.zip\")\n",
        "!mv VIDEOS.zip ../data\n",
        "!unzip ../data/VIDEOS.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39Avi9EMt1cD",
        "outputId": "38fc1165-2f70-4777-d02c-8b4e727dc4c1"
      },
      "outputs": [],
      "source": [
        "# Scarichiamo le annotazioni\n",
        "# Cambiamo i nomi delle directory per semplicità\n",
        "# GT = Ground Truth\n",
        "\n",
        "# Per i video classificati 0 il file di annotazione è vuoto \n",
        "# Per i video classificati 1 il file di annotazione non è vuoto e in una riga csv\n",
        "# contiene l'indice del frame per cui per la prima volta l'operatore umano ha visto\n",
        "# il fumo/fuoco\n",
        "\n",
        "mivia_annotations = \"https://drive.google.com/file/d/1tEz2wVQjPp1MjVHZLa-Z3uyVBnwljgGF/view?usp=sharing\"\n",
        "our_annotations = \"https://drive.google.com/file/d/1ydxWtCXoC1q2qCP98kUUVkmP5dFaXe4q/view?usp=sharing\"\n",
        "download_google_file(our_annotations, \"GT.zip\")\n",
        "!mv GT.zip ../data\n",
        "!unzip ../data/GT.zip\n",
        "!mkdir -p ../data/GT/TRAINING_SET\n",
        "!mv ../data/GT_TRAINING_SET_CL0 ../data/GT/TRAINING_SET/0\n",
        "!mv ../data/GT_TRAINING_SET_CL1 ../data/GT/TRAINING_SET/1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygjJrXJYqjOn",
        "outputId": "653f31ad-6566-44dc-94c5-dc67f2f48c48"
      },
      "outputs": [],
      "source": [
        "# Scarichiamo il codice con i file di test\n",
        "# Il modello deve essere tale da poter essere eseguibile con questo codice di\n",
        "# test\n",
        "\n",
        "download_google_file(\"https://drive.google.com/file/d/1rXMCtpus2i2UDdSBD9RwWAxnT0wrrXOk/view?usp=sharing\", \"test_code.zip\")\n",
        "!unzip test_code.zip"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JT9JzXPm6WLl"
      },
      "source": [
        "# Extract frames from video files\n",
        "\n",
        "Riconduciamo il problema da un dominio di video ad un dominio di immagini andando a selezionare i frame di ogni video e disponendoli in una sottocartella. È importante salvare i frame con qualità alta, altrimenti distorciamo l'informazione data al classificatore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwSrdj6ivMHz"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"../data/\")\n",
        "train_videos_path = \"TRAINING_SET\"\n",
        "test_videos_path = \"TEST_SET\"\n",
        "val_videos_path = \"VALIDATION_SET\"\n",
        "frames_path = \"FRAMES\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_dataset(main_path, p = [0.8, 0.1, 0.1]):\n",
        "    # Definisci il percorso della cartella originaria\n",
        "    folder_path = main_path\n",
        "\n",
        "    # Definisci il percorso delle nuove cartelle\n",
        "    test_set_path = 'TEST_SET'\n",
        "    validation_set_path = 'VALIDATION_SET'\n",
        "\n",
        "    for folder in os.listdir(folder_path):\n",
        "        if folder == \".DS_Store\": continue\n",
        "        # Crea le nuove cartelle\n",
        "        os.makedirs(os.path.join(validation_set_path, folder), exist_ok=True)\n",
        "        os.makedirs(os.path.join(test_set_path, folder), exist_ok=True)\n",
        "        os.makedirs(os.path.join(\"GT\", validation_set_path, folder), exist_ok=True)\n",
        "        os.makedirs(os.path.join(\"GT\", test_set_path, folder), exist_ok=True)\n",
        "        \n",
        "        # Leggi la lista dei video nella cartella originaria\n",
        "        video_list = os.listdir(os.path.join(folder_path, folder))\n",
        "\n",
        "        # Calcola il numero di video da assegnare a ciascuna cartella\n",
        "        total_videos = len(video_list)\n",
        "        training_set_size = int(total_videos * p[0])\n",
        "        validation_set_size = int(total_videos * p[1])\n",
        "        test_set_size = int(total_videos * p[2])\n",
        "\n",
        "        # Genera l'indice casuale per mescolare la lista dei video\n",
        "        random.shuffle(video_list)\n",
        "\n",
        "        # QUELLI IN TRAINING RESTANO IN TRAINING\n",
        "\n",
        "        # Copia i video nella cartella VALIDATION SET\n",
        "        for video in video_list[training_set_size:training_set_size+validation_set_size]:\n",
        "            src = os.path.join(folder_path, folder, video)\n",
        "            dst = os.path.join(validation_set_path, folder, video)\n",
        "            shutil.move(src, dst)\n",
        "            video_ext = video.split(\".\")[-1]\n",
        "            shutil.move(os.path.join(\"GT\", src).replace(video_ext, \"rtf\"), os.path.join(\"GT\", dst).replace(video_ext, \"rtf\"))\n",
        "\n",
        "        # Copia i video nella cartella TEST SET\n",
        "        for video in video_list[training_set_size+validation_set_size:]:\n",
        "            src = os.path.join(folder_path, folder, video)\n",
        "            dst = os.path.join(test_set_path, folder, video)\n",
        "            shutil.move(src, dst)\n",
        "            video_ext = video.split(\".\")[-1]\n",
        "            shutil.move(os.path.join(\"GT\", src).replace(video_ext, \"rtf\"), os.path.join(\"GT\", dst).replace(video_ext, \"rtf\"))\n",
        "            \n",
        "split_dataset(\"TRAINING_SET\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EjVzy0pEEzk",
        "outputId": "3fc39976-3ada-4c5a-c145-02397166a584"
      },
      "outputs": [],
      "source": [
        "!rm -R FRAMES/TRAINING_SET/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FpJD2gh2GaX-"
      },
      "source": [
        "We use ffmpeg to faster the frame extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSfjgF7SvE23",
        "outputId": "4bba8c72-dfcb-4977-9d11-833e7631a952"
      },
      "outputs": [],
      "source": [
        "import cv2, os, argparse, glob, PIL, tqdm\n",
        "\n",
        "def extract_frames(video):\n",
        "    # Process the video\n",
        "    ret = True\n",
        "    cap = cv2.VideoCapture(video) # Decodifica lo streaming\n",
        "    f = 0\n",
        "    while ret:\n",
        "        ret, img = cap.read() # Chiamando read leggiamo il frame successivo dallo stream\n",
        "        if ret: # ret è false quando non ci sono più frame da leggere\n",
        "            f += 1\n",
        "            # Il tensore img letto viene trasformato tramite la classe PIL e lo salviamo\n",
        "            PIL.Image.fromarray(img).save(os.path.join(frames_path, video, \"{:05d}.jpg\".format(f)))\n",
        "    cap.release()\n",
        "\n",
        "# For all the videos\n",
        "for videos_path in [\"TEST_SET\", \"VALIDATION_SET\", \"TRAINING_SET\"]:\n",
        "  file_list = [path for path in glob.glob(os.path.join(videos_path,\"**\"), recursive=True)\n",
        "              if os.path.isfile(path)]\n",
        "  print(file_list)\n",
        "  for video in tqdm.tqdm(file_list):\n",
        "    if os.path.isdir(os.path.join(frames_path, video)):\n",
        "      continue\n",
        "    \n",
        "    os.makedirs(os.path.join(frames_path, video))\n",
        "    # Versione lenta che utilizza la funzione definita prima\n",
        "    #extract_frames(video) \n",
        "    # Versione veloce che fa uso della libreria ffmpeg\n",
        "    os.system(\"ffmpeg -i {} -r 1/1 {}/{}/{}.jpg\".format(video, frames_path, video, \"%05d\")) \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CplIL8gd6a8j"
      },
      "source": [
        "# PyTorch dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zIzzsLisFye1"
      },
      "source": [
        "![](https://albumentations.ai/assets/img/custom/albumentations_logo.png)\n",
        "\n",
        "We use [Albumentations](https://albumentations.ai/) to augment a set of frames with the same augmentation parameters.\n",
        "![](https://albumentations.ai/assets/img/custom/top_image.jpg)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kKpc8TVGFuSn"
      },
      "source": [
        "We use strprtf to parse RTF files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxOLICtI6iY3",
        "outputId": "d4f5f9c3-265a-469b-cd0f-ace20b922643"
      },
      "outputs": [],
      "source": [
        "!pip install striprtf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD0VmuLtw2Dm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from typing import List, Union, Tuple, Any\n",
        "from striprtf.striprtf import rtf_to_text\n",
        "import albumentations\n",
        "\n",
        "# Ci serve un codice che durante il training, una volta selezionato un video\n",
        "# da cui caricare i frame, non va a prenderli tutti (potrebbero essere migliaia)\n",
        "# ed il batch si riempirebbe di informazioni polarizzate ad uno stesso video\n",
        "\n",
        "# Per ora mettiamoci nell'ipotesi semplice di caricare un solo frame nel video,\n",
        "# ma credo che se si vuole usare una RNN non è questo il caso\n",
        "\n",
        "# Se selezioniamo sempre il frame centrale, ad esempio, perdiamo la diversità (?) 38:33\n",
        "# In fase di training conviene perché io voglio fornire alla mia rete campioni \n",
        "# differenti. Facendo molte epoche di training, andrò a coprire quasi tutti i frame\n",
        "# di un certo video ed è una strategia che può funzionare in training\n",
        "# In validation, devo capire se l'addestramento deve o non deve ancora andare \n",
        "# avanti, quindi scegliere elementi casuali e quindi avere un validation set che\n",
        "# cambia ad ogni epoca andrebbe a sballare la loss rendendola poco confrontabile\n",
        "# In validation (e quindi anche in test) per confrontare modelli diversi può avere\n",
        "# senso fissarlo in maniera tale da averlo identico ad ogni epoca per avere un\n",
        "# confronto basato sempre sugli stessi video\n",
        "\n",
        "# Passo in avanti, vogliamo effettuare la classificazione sulla base di 3 frame\n",
        "# consecutivi (o consecutivi e intervallati) per catturare la dinamicità (es.\n",
        "# vedere qualcosa di rosso in movimento)\n",
        "# Si noti che un po di augmentation la facciamo sempre, almeno un flip orizzontale\n",
        "# che non ci introduce nessun problema, per cui se la facciamo randomicamente la devo \n",
        "# fare su tutti i 3 frame selezionati altrimenti potrebbe condurci a risultati errati\n",
        "# Le augmentation, quindi, si applicano all'insieme dei frame selezionati per un dato video\n",
        "# Alcune augmentation però hanno ancora senso nel singolo, ad esempio l'introduzione\n",
        "# di un po di rumore applicato (simula un rumore di acquisizione che è \n",
        "# potenzialmente diverso ad ogni frame) a tale scopo si introduce ALMBUMENTATIONS\n",
        "# un pacchetto di augmentation avanzata che consente di fare quello che si fa\n",
        "# con torchvision ma anche di più.\n",
        "# In particolare prende in input qualcosa di più strutturato che un semplice\n",
        "# campione, ma magari un set di frame come serve a noi. Ma gestisce anche cose\n",
        "# più complesse come la gestioen dei bounding box nell'object detection etc...\n",
        "\n",
        "\n",
        "# NOTA: nei video il fuoco, se c'è, permane fino alla fine\n",
        "\n",
        "class VideoRecord(object):\n",
        " # Sostanzilamente mantiene in memoria tutte le informazioni di annotazione di \n",
        " # un particolare video \n",
        "    \"\"\"\n",
        "    Helper class for class VideoFrameDataset. This class\n",
        "    represents a video sample's metadata.\n",
        "\n",
        "    Args:\n",
        "        root_datapath: the system path to the root folder of the videos.\n",
        "        row: A list with four or more elements where\n",
        "             1) The first element is the path to the video sample's frames excluding\n",
        "             the root_datapath prefix\n",
        "             2) The  second element is the starting frame id of the video\n",
        "             3) The third element is the inclusive ending frame id of the video\n",
        "             4) The fourth element is the label index.\n",
        "             5) any following elements are labels in the case of multi-label classification\n",
        "    \"\"\"\n",
        "    def __init__(self, row, root_datapath):\n",
        "        # row = lista di interi che contiene:\n",
        "        # 0: path del video\n",
        "        # 1: primo frame del video (e qui dobbiamo decidere da dove partire)\n",
        "        # 2: ultimo frame\n",
        "        # 3: label della classe\n",
        "        # 4: possono esserci altre annotazioni (es. fumo, fuoco)\n",
        "        self._data = row\n",
        "        self._path = os.path.join(root_datapath, row[0])\n",
        "\n",
        "    @property\n",
        "    def path(self) -> str:\n",
        "        return self._path\n",
        "\n",
        "    @property\n",
        "    def num_frames(self) -> int:\n",
        "        return self.end_frame - self.start_frame + 1  # +1 because end frame is inclusive\n",
        "\n",
        "    @property\n",
        "    def start_frame(self) -> int:\n",
        "        return int(self._data[1])\n",
        "\n",
        "    @property\n",
        "    def end_frame(self) -> int:\n",
        "        return int(self._data[2])\n",
        "\n",
        "    @property\n",
        "    def label(self) -> Union[int, List[int]]:\n",
        "        # just one label_id\n",
        "        if len(self._data) == 4:\n",
        "            return int(self._data[3])\n",
        "        # sample associated with multiple labels\n",
        "        else:\n",
        "            return [int(label_id) for label_id in self._data[3:]]\n",
        "\n",
        "\n",
        "class VideoFrameDataset(torch.utils.data.Dataset):\n",
        "    r\"\"\"\n",
        "    A highly efficient and adaptable dataset class for videos.\n",
        "    Instead of loading every frame of a video,\n",
        "    loads x RGB frames of a video (sparse temporal sampling) and evenly\n",
        "    chooses those frames from start to end of the video, returning\n",
        "    a list of x PIL images or ``FRAMES x CHANNELS x HEIGHT x WIDTH``\n",
        "    tensors.\n",
        "\n",
        "    More specifically, the frame range [START_FRAME, END_FRAME] is divided into NUM_SEGMENTS\n",
        "    segments and FRAMES_PER_SEGMENT consecutive frames are taken from each segment.\n",
        "\n",
        "    Note:\n",
        "        A demonstration of using this class can be seen\n",
        "        in ``demo.py``\n",
        "        https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch\n",
        "\n",
        "    Note:\n",
        "        This dataset broadly corresponds to the frame sampling technique\n",
        "        introduced in ``Temporal Segment Networks`` at ECCV2016\n",
        "        https://arxiv.org/abs/1608.00859.\n",
        "\n",
        "    Args:\n",
        "        root_path: The root path in which video folders lie.\n",
        "                   this is ROOT_DATA from the description above.\n",
        "        num_segments: The number of segments the video should\n",
        "                      be divided into to sample frames from.\n",
        "        frames_per_segment: The number of frames that should\n",
        "                            be loaded per segment. For each segment's\n",
        "                            frame-range, a random start index or the\n",
        "                            center is chosen, from which frames_per_segment\n",
        "                            consecutive frames are loaded.\n",
        "        imagefile_template: The image filename template that video frame files\n",
        "                            have inside of their video folders as described above.\n",
        "        transform: Transform pipeline that receives a list of numpy images/frames.\n",
        "        test_mode: If True, frames are taken from the center of each\n",
        "                   segment, instead of a random location in each segment.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 root_path: str, # dove sono contenuti i frame estratti         #? Questo non introdurrebbe dipendenza tra i dati???\n",
        "                 num_segments: int = 1, # divide il video da cui caricare i\n",
        "                 # frame in un certo numero di sezioni della stessa durata.\n",
        "                 frames_per_segment: int = 3, # frame estratti dal segmento in maniera (di default) a caso\n",
        "                 imagefile_template: str='{:05d}.jpg', # pattern nome dei frame\n",
        "                 transform=None, # pipeline di augmentation preprocessing\n",
        "                 totensor=True, # lasciarlo sempre a True\n",
        "                 test_mode: bool = False): # quanto True, i frame vengono presi \n",
        "                 # sempre nelle stesse posizioni. È quello che vogliamo fare \n",
        "                 # quando costruiamo dataset per test o validation\n",
        "        super(VideoFrameDataset, self).__init__()\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.num_segments = num_segments\n",
        "        self.frames_per_segment = frames_per_segment\n",
        "        self.imagefile_template = imagefile_template\n",
        "        self.test_mode = test_mode\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = None\n",
        "        else:\n",
        "            additional_targets = {}\n",
        "            for i in range(self.num_segments * self.frames_per_segment - 1):\n",
        "                additional_targets[\"image%d\" % i] = \"image\"\n",
        "            self.transform = albumentations.Compose([transform],\n",
        "                                                    additional_targets=additional_targets,\n",
        "                                                    p=1)\n",
        "        self.totensor = totensor\n",
        "        self.totensor_transform = ImglistOrdictToTensor()\n",
        "\n",
        "        self._parse_annotationfile()\n",
        "        self._sanity_check_samples()\n",
        "\n",
        "    def _load_image(self, directory: str, idx: int) -> Image.Image:\n",
        "        return np.asarray(Image.open(os.path.join(directory, self.imagefile_template.format(idx))).convert('RGB'))\n",
        "\n",
        "    def _parse_annotationfile(self):\n",
        "      # Usando l'organizzazione delle cartelle suggerita l'annotazione viene \n",
        "      # gestita in automatico. Per ogni file video il codice ricava la posizione\n",
        "      # del rispettivo file rtf e fa il parsing del file per ricavare la GT\n",
        "      # In particolare quando vede un video di classe 1, segna il primo frame di\n",
        "      # avvistamento del fuoco e da lì in poi selezionarà i frame randomici, \n",
        "      # andando ad ignorare un'eventuale fase del video iniziale in cui il fuoco\n",
        "      # non c'è\n",
        "      # Fatto questo parse abbiamo costruito la label del video\n",
        "        self.video_list = []\n",
        "        for class_name in os.listdir(self.root_path):\n",
        "            for video_name in os.listdir(os.path.join(self.root_path, class_name)):\n",
        "                frames_dir = os.path.join(self.root_path, class_name, video_name)\n",
        "                if os.path.isdir(frames_dir):\n",
        "                    frame_path = os.path.join(class_name, video_name)\n",
        "                    end_frame = len(os.listdir(frames_dir))\n",
        "\n",
        "                    video_ext = frames_dir.split(\".\")[-1]\n",
        "                    annotation_path = frames_dir\\\n",
        "                        .replace(\"\\\\\", \"/\") \\\n",
        "                        .replace(\"FRAMES/\", \"GT/\") \\\n",
        "                        .replace(video_ext, \"rtf\")\n",
        "\n",
        "                    with open(annotation_path, 'r') as file:\n",
        "                        text = rtf_to_text(file.read())\n",
        "                    if len(text):\n",
        "                        label = 1\n",
        "                        start_frame = int(text.split(\",\")[0])\n",
        "                        if start_frame == 0:\n",
        "                          start_frame = 1\n",
        "                    else:\n",
        "                        label = 0\n",
        "                        start_frame = 1\n",
        "\n",
        "                    self.video_list.append(VideoRecord(\n",
        "                        [frame_path, start_frame, end_frame, label],\n",
        "                        self.root_path))\n",
        "\n",
        "    def _sanity_check_samples(self):\n",
        "      # Controllo delle annotazioni ricavate per ogni video\n",
        "        for record in self.video_list:\n",
        "            if record.num_frames <= 0 or record.start_frame == record.end_frame:\n",
        "                print(f\"\\nDataset Warning: video {record.path} seems to have zero RGB frames on disk!\\n\")\n",
        "\n",
        "            elif record.num_frames < (self.num_segments * self.frames_per_segment):\n",
        "                print(f\"\\nDataset Warning: video {record.path} has {record.num_frames} frames \"\n",
        "                      f\"but the dataloader is set up to load \"\n",
        "                      f\"(num_segments={self.num_segments})*(frames_per_segment={self.frames_per_segment})\"\n",
        "                      f\"={self.num_segments * self.frames_per_segment} frames. Dataloader will throw an \"\n",
        "                      f\"error when trying to load this video.\\n\")\n",
        "\n",
        "    def _get_start_indices(self, record: VideoRecord) -> 'np.ndarray[int]':\n",
        "        \"\"\"\n",
        "        For each segment, choose a start index from where frames\n",
        "        are to be loaded from.\n",
        "\n",
        "        Args:\n",
        "            record: VideoRecord denoting a video sample.\n",
        "        Returns:\n",
        "            List of indices of where the frames of each\n",
        "            segment are to be loaded from.\n",
        "        \"\"\"\n",
        "        # choose start indices that are perfectly evenly spread across the video frames.\n",
        "        if self.test_mode:\n",
        "            distance_between_indices = (record.num_frames - self.frames_per_segment + 1) / float(self.num_segments)\n",
        "\n",
        "            start_indices = np.array([int(distance_between_indices / 2.0 + distance_between_indices * x)\n",
        "                                      for x in range(self.num_segments)])\n",
        "        # randomly sample start indices that are approximately evenly spread across the video frames.\n",
        "        else:\n",
        "            max_valid_start_index = (record.num_frames - self.frames_per_segment + 1) // self.num_segments\n",
        "\n",
        "            start_indices = np.multiply(list(range(self.num_segments)), max_valid_start_index) + \\\n",
        "                      np.random.randint(max_valid_start_index, size=self.num_segments)\n",
        "\n",
        "        return start_indices\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Union[\n",
        "        Tuple[List[Image.Image], Union[int, List[int]]],\n",
        "        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n",
        "        Tuple[Any, Union[int, List[int]]],\n",
        "        ]:\n",
        "        \"\"\"\n",
        "        For video with id idx, loads self.NUM_SEGMENTS * self.FRAMES_PER_SEGMENT\n",
        "        frames from evenly chosen locations across the video.\n",
        "\n",
        "        Args:\n",
        "            idx: Video sample index.\n",
        "        Returns:\n",
        "            A tuple of (video, label). Label is either a single\n",
        "            integer or a list of integers in the case of multiple labels.\n",
        "            Video is either 1) a list of PIL images if no transform is used\n",
        "            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n",
        "            if the transform \"ImglistToTensor\" is used\n",
        "            3) or anything else if a custom transform is used.\n",
        "        \"\"\"\n",
        "        record: VideoRecord = self.video_list[idx]\n",
        "\n",
        "        frame_start_indices: 'np.ndarray[int]' = self._get_start_indices(record)\n",
        "\n",
        "        return self._get(record, frame_start_indices)\n",
        "\n",
        "    def _get(self, record: VideoRecord, frame_start_indices: 'np.ndarray[int]') -> Union[\n",
        "        Tuple[List[Image.Image], Union[int, List[int]]],\n",
        "        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n",
        "        Tuple[Any, Union[int, List[int]]],\n",
        "        ]:\n",
        "        \"\"\"\n",
        "        Loads the frames of a video at the corresponding\n",
        "        indices.\n",
        "\n",
        "        Args:\n",
        "            record: VideoRecord denoting a video sample.\n",
        "            frame_start_indices: Indices from which to load consecutive frames from.\n",
        "        Returns:\n",
        "            A tuple of (video, label). Label is either a single\n",
        "            integer or a list of integers in the case of multiple labels.\n",
        "            Video is either 1) a list of PIL images if no transform is used\n",
        "            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n",
        "            if the transform \"ImglistToTensor\" is used\n",
        "            3) or anything else if a custom transform is used.\n",
        "        \"\"\"\n",
        "\n",
        "        frame_start_indices = frame_start_indices + record.start_frame\n",
        "        images = list()\n",
        "\n",
        "        # from each start_index, load self.frames_per_segment\n",
        "        # consecutive frames\n",
        "        for start_index in frame_start_indices:\n",
        "            frame_index = int(start_index)\n",
        "\n",
        "            # load self.frames_per_segment consecutive frames\n",
        "            for _ in range(self.frames_per_segment):\n",
        "                image = self._load_image(record.path, frame_index)\n",
        "                images.append(image)\n",
        "\n",
        "                if frame_index < record.end_frame:\n",
        "                    frame_index += 1\n",
        "\n",
        "        if self.transform is not None:\n",
        "            transform_input = {\"image\": images[0]}\n",
        "            for i, image in enumerate(images[1:]):\n",
        "                transform_input[\"image%d\" % i] = image\n",
        "            images = self.transform(**transform_input)\n",
        "\n",
        "        if self.totensor:\n",
        "            images = self.totensor_transform(images)\n",
        "        return images, record.label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_list)\n",
        "\n",
        "\n",
        "class ImglistOrdictToTensor(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Converts a list or a dict of numpy images to a torch.FloatTensor\n",
        "    of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH).\n",
        "    Can be used as first transform for ``VideoFrameDataset``.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(img_list_or_dict):\n",
        "        \"\"\"\n",
        "        Converts each numpy image in a list or a dict to\n",
        "        a torch Tensor and stacks them into a single tensor.\n",
        "\n",
        "        Args:\n",
        "            img_list_or_dict: list or dict of numpy images.\n",
        "        Returns:\n",
        "            tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\n",
        "        \"\"\"\n",
        "        if isinstance(img_list_or_dict, list):\n",
        "            return torch.stack([transforms.functional.to_tensor(img)\n",
        "                                for img in img_list_or_dict])\n",
        "        else:\n",
        "            return torch.stack([transforms.functional.to_tensor(img_list_or_dict[k])\n",
        "                                for k in img_list_or_dict.keys()])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "76_bB6C6TxyY"
      },
      "source": [
        "Non viene fatto alcun controllo durante la selezione dei frame sul fatto che in fase di training un frame potrebbe essere scelto più volte. Non è un grande problema in quanto la probabilità che accada è molto bassa, e se succede la rete è molto diversa tra le due circostanze in cui uno stesso frame viene valutato. Inoltre, le augmentation potrebbero agire diversamente su quel frame.\n",
        "\n",
        "Questo controllo potrebbe essere fatto ma complica di molto l'implementazione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GbpH5oY22agC",
        "outputId": "0db995a3-e954-476b-f8e5-102994ffe65f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def demo_visualization():\n",
        "    from torchvision.utils import make_grid\n",
        "    import matplotlib.pyplot as plt\n",
        "    import  albumentations\n",
        "    # Augmentation\n",
        "    # OneOf prende in ingresso una lista di augmentation ed applica con una\n",
        "    # certa probabilità solo una augmentation di questa lista (presa casualmente)\n",
        "    # augmentation = albumentations.OneOf(\n",
        "    #     [\n",
        "    #     # albumentations.HorizontalFlip(p=1.),\n",
        "    #     albumentations.ShiftScaleRotate(shift_limit=0., scale_limit=0.5, rotate_limit=0, p=1.),\n",
        "    #     ],\n",
        "    #     p=1)\n",
        "\n",
        "\n",
        "        \n",
        "    augmentation = albumentations.Compose([\n",
        "        albumentations.Downscale(p=0), # Riduce la risoluzione\n",
        "    ], p=1)\n",
        "    # Dataset with augmentation for visualization\n",
        "    dataset = VideoFrameDataset(root_path=\"../data/SPLITTED_FRAMES/TRAINING_SET/\",\n",
        "                                num_segments=1,\n",
        "                                frames_per_segment=1,\n",
        "                                transform=augmentation,\n",
        "                                )\n",
        "    counter = 0\n",
        "    for X, y in dataset:\n",
        "        if counter > 10:\n",
        "            break\n",
        "        plt.figure()\n",
        "        plt.imshow(make_grid(X).numpy().transpose((1, 2, 0)))\n",
        "        plt.title(y)\n",
        "        plt.show()\n",
        "        counter += 1\n",
        "\n",
        "\n",
        "\n",
        "demo_visualization()\n",
        "\n",
        "# Questa demo è utile per testare l'augumentation prima di far iniziare il\n",
        "# training per un certo numero di ore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPT3-ZlP2fCD",
        "outputId": "e2ce0913-edb7-400c-f461-76fee18c0965"
      },
      "outputs": [],
      "source": [
        "def demo_training():\n",
        "    # Preprocessing and augmentation\n",
        "    # applica sempre, ha senso per il preprocessing\n",
        "    preprocessing = albumentations.Sequential([\n",
        "        albumentations.Resize(height=224, width=224, always_apply=True),\n",
        "        albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225],\n",
        "                                 max_pixel_value=255.,\n",
        "                                 always_apply=True), \n",
        "    ])\n",
        "\n",
        "\n",
        "    # Rispetto al preprocessing di alexnet, dove fecevamo un resize a 257 e un\n",
        "    # crop centrale a 224, in questo caso facciamo un resize diretto perché i\n",
        "    # dati non sono tali per cui l'oggetto di interesse è posizionato ben \n",
        "    # centrale e quindi andremmo a perdere informazioni facendo un crop\n",
        "\n",
        "    augmentation = albumentations.OneOf([\n",
        "        albumentations.HorizontalFlip(p=1.),\n",
        "        ], p=.5)\n",
        "\n",
        "    # Dataset for training\n",
        "    dataset = VideoFrameDataset(root_path=\"FRAMES/TRAINING_SET/\",\n",
        "                                num_segments=1,\n",
        "                                frames_per_segment=1,\n",
        "                                transform=albumentations.Compose([\n",
        "                                    preprocessing,\n",
        "                                    augmentation],\n",
        "                                    p=1.,\n",
        "                                )\n",
        "                                )\n",
        "\n",
        "    print(dataset, \"\\nelements:\", len(dataset))\n",
        "    X, y = dataset[0]\n",
        "    print('first element data', X.shape, X.min(), X.max(), X.mean(), X.std(),\n",
        "          '\\nlabel', y)\n",
        "    return dataset\n",
        "\n",
        "dataset_train = demo_training()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-N4yxG07b-0",
        "outputId": "b891f8bf-c4c8-4f53-de3f-601fdd15cd97"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, shuffle=True,\n",
        "                              batch_size=2, num_workers=0, pin_memory=True)\n",
        "print(dataloader_train, \"\\nbatches:\", len(dataloader_train))\n",
        "\n",
        "for X, y in tqdm(dataloader_train):\n",
        "  print('batch data', X.shape, X.min(), X.max(), X.mean(), X.std(), '\\nlabel', y)\n",
        "\n",
        "# Per ricondurci ad un problema di classificazione di immagini, possiamo mettere\n",
        "# ad 1 il frame ricevuto e fare un'operazione di squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqpA-pfiZYQx"
      },
      "outputs": [],
      "source": [
        "# cp -r FRAMES gdrive/MyDrive/Magistrale/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.10 ('MachineLearning')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "3a2c952ceb315243f34ba302acbe0fb5218585a422897d4fb07b81cc17398136"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
